<!-- Generated by Harlequin WebMaker 2.2.3 (30-Apr-1996)
Macintosh Common Lisp Version 3.0kp2p2 [AppGen 3.0b1kp2p2] -->
<HTML> <HEAD>
<TITLE>Interpreting Floating-Point Values (IM:PN)</TITLE>
<LINK REL="stylesheet" TYPE="text/css" HREF="../../Resources/CSS/frameset_styles.css">
<script type="text/javascript" language="JavaScript" src="../../Resources/JavaScript/page.js"></script>
</head>

<BODY BGCOLOR="#FFFFFF" TEXT="#000000">

<div style="width:100%; position:fixed;"><div align="center" id="watermark" style="position: relative; margin-left:auto; margin-right:auto; z-index:20; width:500px;">
<div class="legacybox" style="position: relative;">
<h1>Legacy Document<span class=closebutton><a href="javascript:closeWatermark()"><img src="../../Resources/Images/closebutton.png" width="14" height="14" border="0" align="top" alt="close button"></a></span></h1><p><b>Important:</b>
The information in this document is obsolete and should not be used for new development.</p></div></div></div>

<A NAME="HEADING17"></A>


<!-- start of header -->

<!--#include virtual="/includes/framesetheader" -->

<!-- end of header -->


<!-- Main Body -->

<CENTER>
<P>
<A HREF="PPCNumerics-16.html"><IMG ALIGN="BOTTOM" SRC="prev.gif" BORDER="none" HSPACE="20" ALT="Previous"></A> <A HREF="PPCNumerics-2.html"><IMG ALIGN="BOTTOM" SRC="content.gif" BORDER="none" HSPACE="20" ALT="Book Contents"></A> <A HREF="PPCNumerics-192.html"><IMG ALIGN="BOTTOM" SRC="index.gif" BORDER="none" HSPACE="20" ALT="Book Index"></A> <A HREF="PPCNumerics-18.html"><IMG ALIGN="BOTTOM" SRC="next.gif" BORDER="none" HSPACE="20" ALT="Next"></A> </CENTER><P>
<FONT SIZE="-1"><DL><DT><a href="../../macos8/mac8.html" onmouseover="window.status='Macintosh Documentation'; return true"><B>Inside Macintosh:</B></A> <A HREF="PPCNumerics-2.html"><B>PowerPC Numerics </B></A> / <A HREF="PPCNumerics-9.html"><B>Part 1 - The PowerPC Numerics Environment</B></A><BR><DD><A HREF="PPCNumerics-15.html"><B>Chapter 2 - Floating-Point Data Formats </B></A></DL></FONT><P>
<HR>
<BLOCKQUOTE>
<A NAME="HEADING17-0"></A>
<H1><A NAME="MARKER-9-36"></A>Interpreting Floating-Point Values </H1>
 <A NAME="MARKER-9-1"></A>Regardless of which data format (single, double, or double-double) you use, the numerics environment uses the same basic method to interpret which floating-point value the data format represents. This section describes that method.<P>
 Every floating-point data format has a sign bit, an exponent field, and a fraction field. These three fields provide binary encodings of a sign (+ or -), an exponent, and a <B>significand,</B> respectively, of a floating-point value. The value is interpreted as<P>
 
<MATH>
\xB1significand&#215;2<SUP>exponent+-bias</SUP>
</MATH>

<P>
 where <P>
<DL>
<DT>\xB1
<DD> is the sign stored in the sign bit (1 is negative, 0 is positive).<A NAME="MARKER-9-15"></A>
<DT><I><A NAME="MARKER-9-39"></A>significand</I> 
<DD> has the form 
<MATH>
b<SUB>0</SUB><B>.</B>b<SUB>1</SUB>b<SUB>2</SUB>...b<SUB>3</SUB>b<SUB>precision-1</SUB>
</MATH>

where
<MATH>
b<SUB>1</SUB>b<SUB>2</SUB>...b<SUB>3</SUB>b<SUB>precision-1</SUB>

</MATH>

 are <BR>the bits in the fraction field and 

<MATH>
b<SUB>0</SUB>
</MATH>

 is an implicit bit whose value is interpreted as described in the sections <A HREF="#MARKER-9-45">"Normalized Numbers"</A> and <A HREF="#MARKER-9-51">"Denormalized Numbers."</A> The significand is sometimes called the <I>mantissa</I>.<I><A NAME="MARKER-2-40"></A></I>
<DT><I>exponent</I> 
<DD> is the value of the exponent field. <A NAME="MARKER-2-41"></A>
<DT><I><A NAME="MARKER-9-43"></A>bias</I> 
<DD> is the bias of the exponent. The <B>bias</B> is a predefined value (127 for single format, 1023 for double and double-double formats) that is added to the exponent when it is stored in the exponent field. When the floating-point number is evaluated, the bias is subtracted to return the correct exponent. The minimum biased exponent field (all 0's) and maximum biased exponent field (all 1's) are assigned special floating-point values (described in the next several sections). <A NAME="MARKER-9-24"></A>
</DL>
 <A NAME="MARKER-2-19"></A>In a numeric data format, each valid representation belongs to exactly one of these classes, which are described in the sections that follow:<P>
<UL>
<LI>normalized numbers
<LI>denormalized numbers
<LI>Infinities
<LI>NaNs (signaling or quiet)
<LI>zeros<P>
</UL>
<A NAME="HEADING17-15"></A>
<H2><A NAME="MARKER-9-45"></A>Normalized Numbers</H2>
 <A NAME="MARKER-2-9"></A>The numeric data formats represent most floating-point numbers as <B>normalized numbers,</B> meaning that the implicit leading bit (
<MATH>
b<SUB>0</SUB>
</MATH>

 on <A HREF="#MARKER-9-39">page 2-4</A>) of the significand is 1. Normalization maximizes the resolution of the data type and ensures that representations are unique. <A HREF="#MARKER-9-47">Figure 2-2</A> shows the magnitudes of normalized numbers in single precision on the number line. The spacing of the vertical marks indicates the relative density of numbers in each binade. (A <B>binade</B> is a collection of numbers between two successive powers of 2.) Notice that the numbers get more dense as they approach 0.<P>
<DL>
<DT><B>Note</B>
<DD>The figure shows only the relative density of the numbers; in reality, the density is immensely greater than it is possible to show in such a figure. For example, there are 
<MATH>
2<SUP>23</SUP>
</MATH>

 (8,388,608) single-precision numbers in the interval 
<MATH>
2<SUP>-126</SUP> <IMG BORDER="NONE" SRC="graphics/lessthan.gif"> x2<SUP>-125</SUP>
</MATH>

.<EM></EM>  <IMG ALIGN="BOTTOM" SRC="graphics/diamond.gif">
</DL>
<B>Figure 2-2  <A NAME="MARKER-9-47"></A>Normalized single-precision numbers on the number line<A NAME="MARKER-2-48"></A></B><P>
<IMG ALIGN="BOTTOM" SRC="graphics/PPCN-L-10.jpg"><P>
 Using only normalized representations creates a gap around the value 0, as shown in <A HREF="#MARKER-9-47">Figure 2-2</A>. If a computer supports only the normalized numbers, it must round all tiny values to 0. For example, suppose such a computer must perform the operation 
<MATH>
x+-y
</MATH>

, where <I>x</I> and <I>y</I> are very close to, but not equal to, each other. If the difference between <I>x</I> and <I>y</I> is smaller than the smallest normalized number, the computer must deliver 0 as the result. Thus, for such <B>flush-to-zero systems,</B> the following statement is <I>not</I> true for all real numbers:<B><A NAME="MARKER-2-53"></A></B><P>
 
<MATH>
x+-y = 0
</MATH>

 if and only if 
<MATH>
x = y
</MATH>

 <A NAME="MARKER-2-50"></A><P>
<A NAME="HEADING17-22"></A>
<H2><A NAME="MARKER-9-51"></A>Denormalized Numbers</H2>
 <A NAME="MARKER-2-52"></A>Instead of using only normalized numbers and allowing this small gap around 0, PowerPC processor-based Macintosh computers use <B>denormalized numbers,</B> in which the leading implicit bit (<EM>
<MATH>
b<SUB>0</SUB>
</MATH>

</EM> on <A HREF="#MARKER-9-39">page 2-4</A>) of the significand is 0 and the minimum exponent is used.<P>
<DL>
<DT><B>Note</B>
<DD>Some references use the term <B>subnormal</B> <B>numbers</B> instead of <I>denormalized numbers.</I><EM></EM>  <IMG ALIGN="BOTTOM" SRC="graphics/diamond.gif">
</DL>
 <A NAME="MARKER-2-53"></A><A NAME="MARKER-9-5"></A><A HREF="#MARKER-9-57">Figure 2-3</A> illustrates the relative magnitudes of normalized and denormalized numbers in single precision. Notice that the denormalized numbers have the same density as the numbers in the smallest normalized binade. This means that the roundoff error is the same regardless of whether an operation produces a denormalized number or a very small normalized number. As stated previously, without denormalized numbers, operations would have to round tiny values to 0, which is a much greater roundoff error.<A NAME="MARKER-2-55"></A><A NAME="MARKER-2-59"></A><P>
<B>Figure 2-3  <A NAME="MARKER-9-57"></A>Denormalized single-precision numbers on the number line<A NAME="MARKER-2-40"></A></B><P>
<IMG ALIGN="BOTTOM" SRC="graphics/PPCN-L-32.jpg"><P>
 To put it another way, the use of denormalized numbers makes the following statement true for all real numbers: <P>
 
<MATH>
x+-y = 0
</MATH>

 if and only if 
<MATH>
x = y
</MATH>

<P>
 Another advantage of denormalized numbers is that error analysis involving small values is much easier without the gap around zero shown in <A HREF="#MARKER-9-47">Figure 2-2</A> (Demmel 1984).<A NAME="MARKER-2-59"></A><P>
 The computer determines that a floating-point number is denormalized (and therefore <BR>that its implicit leading bit is interpreted as 0) when the biased exponent field is filled <BR>with 0's and the fraction field is nonzero.<P>
 <A NAME="MARKER-9-11"></A><A HREF="#MARKER-9-7">Table 2-2</A> shows how a single-precision value 
<MATH>
A<SUB>0</SUB>
</MATH>

becomes progressively denormalized as it is repeatedly divided by 2, with rounding to nearest. This process is called <B>gradual underflow.</B> In the table, values
<MATH>
A<SUB>2</SUB>...A<SUB>25</SUB>
</MATH>


 are denormalized; 
<MATH>
A<SUB>25</SUB>
</MATH>

 is the smallest positive denormalized number in single format. Notice that as soon as the values are too small to be normalized, the biased exponent value becomes 0. 
<TABLE BORDER="0" CELLPADDING="3">
<CAPTION><A NAME="MARKER-9-7"></A><B>Table 2-2 Example of gradual underflow</B></CAPTION>
<TH>Variable or operation<TH>Value<TH>Biased exponent<TH>Comment<TR>
<TD><EM>
<MATH>
A<SUB>0</SUB>
</MATH>

</EM><TD>1.100 1100 1100 1100 1100 1101 <EM></EM> 
<MATH>
2<SUP>-125</SUP>
</MATH>

<TD>2<TD><EM></EM><TR>
<TD>
<MATH>
A<SUB>1</SUB> = A<SUB>0</SUB>/2
</MATH>

<TD>1.100 1100 1100 1100 1100 1101 <EM></EM> 
<MATH>
2<SUP>-126</SUP>
</MATH>

<TD>1<TD><EM></EM><TR>
<TD>
<MATH>
A<SUB>2</SUB> = A<SUB>1</SUB>/2
</MATH>

<TD>0.110 0110 0110 0110 0110 0110 <EM></EM> 
<MATH>
2<SUP>-126</SUP>
</MATH>

<EM></EM><TD>0<TD>Inexact<A HREF="#FOOTNOTE-4">[4]</A><TR>
<TD>
<MATH>
A<SUB>3</SUB> = A<SUB>2</SUB>/2
</MATH>

<TD>0.011 0011 0011 0011 0011 0011 <EM></EM> 
<MATH>
2<SUP>-126</SUP>
</MATH>

<EM></EM><TD>0<TD>Exact result<TR>
<TD>
<MATH>
A<SUB>4</SUB> = A<SUB>3</SUB>/2
</MATH>

<TD>0.001 1001 1001 1001 1001 1010 <EM></EM> 
<MATH>
2<SUP>-126</SUP>
</MATH>

<EM></EM><TD>0<TD>Inexact<A HREF="#FOOTNOTE-4">[4]</A><TR>
<TD>&nbsp;<TD>.<TD>&nbsp;<TD>&nbsp;<TR>
<TD>&nbsp;<TD>.<TD>&nbsp;<TD>&nbsp;<TR>
<TD>&nbsp;<TD>.<TD>&nbsp;<TD>&nbsp;<TR>
<TD>
<MATH>
A<SUB>23</SUB> = A<SUB>22</SUB>/2</MATH>

<TD>0.000 0000 0000 0000 0000 0011 <EM></EM> 
<MATH>
2<SUP>-126</SUP>
</MATH>

<EM></EM><TD>0<TD>Exact result<TR>
<TD>
<MATH>
A<SUB>24</SUB> = A<SUB>23</SUB>/2
</MATH>

<TD>0.000 0000 0000 0000 0000 0010 <EM></EM> 
<MATH>
2<SUP>-126</SUP>
</MATH>

<EM></EM><TD>0<TD>Inexact<A HREF="#FOOTNOTE-4">[4]</A><TR>
<TD>
<MATH>
A<SUB>25</SUB> = A<SUB>24</SUB>/2</MATH>

<TD>0.000 0000 0000 0000 0000 0001 <EM></EM> 
<MATH>
2<SUP>-126</SUP>
</MATH>

<EM></EM><TD>0<TD>Exact result<TR>
<TD>
<MATH>
A<SUB>26</SUB> = A<SUB>25</SUB>/2</MATH>

<TD>0.0<TD>0<TD>Inexact<A HREF="#FOOTNOTE-4">[4]</A></TABLE>
<P>
<A NAME="HEADING17-34"></A>
<H2><A NAME="MARKER-9-61"></A>Infinities</H2>
 <A NAME="MARKER-2-62"></A>An <B>Infinity</B> is a special bit pattern that can arise in one of two ways:<P>
<UL>
<LI>When an operation (such as 
<MATH>
1/0
</MATH>

) should produce a mathematical infinity, the result is an Infinity.
<LI>When an operation attempts to produce a number with a magnitude too great for the number's intended floating-point data type, the result might be a value with the largest possible magnitude or it might be an Infinity (depending on the current rounding direction).<P>
</UL>
 These bit patterns (as well as NaNs, introduced next) are recognized in subsequent operations and produce predictable results. The Infinities, one positive and one negative, generally behave as suggested by the theory of limits. For example:<A NAME="MARKER-2-57"></A><A NAME="MARKER-2-69"></A><P>
<UL>
<LI>Adding 1 to + <IMG BORDER="NONE" SRC="graphics/infinity.gif">  yields + <IMG BORDER="NONE" SRC="graphics/infinity.gif"> .
<LI>Dividing 
<MATH>
-1
</MATH>

 by +0 yields 
<MATH>
- <IMG BORDER="NONE" SRC="graphics/infinity.gif"> 
</MATH>

.
<LI>Dividing 1 by 
<MATH>
- <IMG BORDER="NONE" SRC="graphics/infinity.gif"> 
</MATH>

 yields 
<MATH>
-0
</MATH>

.<P>
</UL>
 The computer determines that a floating-point number is an Infinity if its exponent field is filled with 1's and its fraction field is filled with 0's. So, for example, in single format, if the sign bit is 1, the exponent field is 255 (which is the maximum biased exponent for the single format), and the fraction field is 0, the floating-point number represented is 
<MATH>
- <IMG BORDER="NONE" SRC="graphics/infinity.gif"> 
</MATH>

 (see <A HREF="#MARKER-9-65">Figure 2-4</A>).<P>
<B>Figure 2-4  <A NAME="MARKER-9-65"></A>Infinities represented in single precision<A NAME="MARKER-2-70"></A></B><P>
<IMG ALIGN="BOTTOM" SRC="graphics/PPCN-L-28.jpg"><P>
<A NAME="HEADING17-45"></A>
<H2><A NAME="MARKER-9-67"></A>NaNs</H2>
 <A NAME="MARKER-2-68"></A>When a numeric operation cannot produce a meaningful result, the operation delivers a special bit pattern called a <B>NaN (Not-a-Number).</B> For example, zero divided by zero, + <IMG BORDER="NONE" SRC="graphics/infinity.gif">  added to 
<MATH>
- <IMG BORDER="NONE" SRC="graphics/infinity.gif"> 
</MATH>

, and 
<MATH>
SQRT-1
</MATH>

 yield NaNs. A NaN can occur in any of the numeric data formats (single, double, and double-double), but generally, system-specific <B>integer types</B> (non-numeric types exclusively for integer values) have no representation for NaNs. <A NAME="MARKER-2-69"></A><P>
 NaNs propagate through arithmetic operations. Thus, the result of 3.0 added to a NaN is the same NaN. If two operands of an operation are NaNs, the result is one of the NaNs. NaNs are of two kinds: <B>quiet NaNs,</B> the usual kind produced by floating-point operations, and <B>signaling NaNs.</B><P>
 <A NAME="MARKER-2-70"></A>When a signaling NaN is encountered as an operand of an arithmetic operation, the invalid-operation exception is signaled and a quiet NaN is the delivered result. Signaling NaNs are not created by any numeric operations, but you might find it useful to create signaling NaNs manually. For example, you might fill uninitialized memory with signaling NaNs so that if one is ever encountered in a program, you will know that uninitialized memory is accessed.<A NAME="MARKER-2-71"></A><P>
 A NaN may have an associated code that indicates its origin. These codes are listed in <A HREF="#MARKER-9-10">Table 2-3</A>. The NaN code is the 8th through 15th most significant bits of the fraction field.
<TABLE BORDER="0" CELLPADDING="3">
<CAPTION><A NAME="MARKER-9-10"></A><B>Table 2-3 NaN codes</B></CAPTION>
<TH>Decimal<TH>Hexadecimal<TH>Meaning<TR>
<TD>1<TD>0x01<TD>Invalid square root, such as 
<MATH>
SQRT-1
</MATH>

 <TR>
<TD>2<TD>0x02<TD>Invalid addition, such as 
<MATH>
(+ <IMG BORDER="NONE" SRC="graphics/infinity.gif"> )+(- <IMG BORDER="NONE" SRC="graphics/infinity.gif"> )
</MATH>

<TR>
<TD>4<TD>0x04<TD>Invalid division, such as 
<MATH>
0/0
</MATH>

<TR>
<TD>8<TD>0x08<TD>Invalid multiplication, such as 
<MATH>
0&#215; <IMG BORDER="NONE" SRC="graphics/infinity.gif"> 
</MATH>

<TR>
<TD>9<TD>0x09<TD>Invalid remainder or modulo, such as <I>x</I> rem 0<TR>
<TD>17<TD>0x11<TD>Attempt to convert invalid ASCII string<TR>
<TD>21<TD>0x15<TD>Attempt to create a NaN with a zero code<TR>
<TD>33<TD>0x21<TD>Invalid argument to trigonometric function (such as cos, sin, tan)<TR>
<TD>34<TD>0x22<TD>Invalid argument to inverse trigonometric function (such as acos, asin, atan)<TR>
<TD>36<TD>0x24<TD>Invalid argument to logarithmic function (such as log, 
<MATH>
log10
</MATH>

)<TR>
<TD>37<TD>0x25<TD>Invalid argument to exponential function (such as exp, expm1)<TR>
<TD>38<TD>0x26<TD>Invalid argument to financial function (compound or annuity)<TR>
<TD>40<TD>0x28<TD>Invalid argument to inverse hyperbolic function (such as acosh, asinh)<TR>
<TD>42<TD>0x2A<TD>Invalid argument to gamma function (gamma or lgamma)</TABLE>
<P>
<DL>
<DT><B>Note</B>
<DD>The PowerPC processor always returns 0 for the NaN code.<EM></EM>  <IMG ALIGN="BOTTOM" SRC="graphics/diamond.gif">
</DL>
 The computer determines that a floating-point number is a NaN if its exponent field is filled with 1's and its fraction field is nonzero. The most significant bit of the fraction field distinguishes quiet and signaling NaNs. It is set for quiet NaNs and clear for signaling NaNs. For example, in single format, if the sign field has the value 1, the exponent field has the value 255, and the fraction field has the value 65,280, then the number is a signaling NaN. If the sign is 1, the exponent is 255, and the fraction field has the value 4,259,584 (which means the fraction field has a leading 1 bit), the value is a quiet NaN. <A HREF="#MARKER-9-72">Figure 2-5</A> illustrates these examples.<P>
<B>Figure 2-5  <A NAME="MARKER-9-72"></A>NaNs represented in single precision<A NAME="MARKER-2-73"></A></B><P>
<IMG ALIGN="BOTTOM" SRC="graphics/PPCN-L-37.jpg"><P>
<A NAME="HEADING17-54"></A>
<H2>Zeros</H2>
 <A NAME="MARKER-2-74"></A>Each floating-point format has two representations for zero: +0 and 
<MATH>
-0
</MATH>

. Although the two zeros compare as equal 
<MATH>
(+0) = -0
</MATH>

, their behaviors in IEEE arithmetic are slightly different.<P>
 Ordinarily, the sign of zero does not matter except (possibly) for a function discontinuous at zero. Though the two forms are numerically equal, a program can distinguish +0 from 
<MATH>
-0
</MATH>

 by operations such as division by zero or by performing the numeric copysign function.<P>
 The sign of zero obeys the usual sign laws for multiplication and division. For example, 
<MATH>
(+0)&#215;(-1) = -0
</MATH>

 and 
<MATH>
1/(-0) = - <IMG BORDER="NONE" SRC="graphics/infinity.gif"> 
</MATH>

. Because extreme negative underflows yield 
<MATH>
-0
</MATH>

, expressions like 
<MATH>
1/x<SUP>3</SUP>
</MATH>

 produce the correct sign for  <IMG BORDER="NONE" SRC="graphics/infinity.gif">  when <I>x</I> is tiny and negative. Addition and subtraction produce 
<MATH>
-0
</MATH>

 only in these cases:<P>
<UL>
<LI><A NAME="MARKER-2-75"></A>
<MATH>
(-0)+-(+0)yields
</MATH>


<MATH>
-0
</MATH>


<LI>
<MATH>
(-0)+(-0)yields
</MATH>


<MATH>
-0
</MATH>

<P>
</UL>
 When rounding downward, with <I>x</I> finite,<P>
<UL>
<LI>
<MATH>
x+-xyields
</MATH>


<MATH>
-0
</MATH>


<LI>
<MATH>
x+(-x)yields
</MATH>


<MATH>
-0
</MATH>

<P>
</UL>
 The square root of 
<MATH>
-0
</MATH>

 is 
<MATH>
-0
</MATH>

.<P>
 The sign of zero is important in complex arithmetic (Kahan 1987).<P>
 The computer determines that a floating-point number is 0 if its exponent field and its fraction field are filled with 0's. For example, in single format, if the sign bit is 0, the exponent field is 0, and the fraction field is 0, the number is +0 (see <A HREF="#MARKER-9-76">Figure 2-6</A>).<P>
<B>Figure 2-6  <A NAME="MARKER-9-76"></A>Zeros represented in single precision<A NAME="MARKER-9-8"></A><A NAME="MARKER-2-42"></A><A NAME="MARKER-2-79"></A></B><P>
<IMG ALIGN="BOTTOM" SRC="graphics/PPCN-L-30.jpg"><P>
<HR>

<A NAME="FOOTNOTE-4">[4] Whenever division returns an inexact tiny value, the exception bit for underflow is set to indicate that a low-order bit has been lost.<A NAME="MARKER-9-35"></A>

</BLOCKQUOTE>
<HR>
<CENTER>
<A HREF="PPCNumerics-16.html"><IMG ALIGN="BOTTOM" SRC="prev.gif" BORDER="none" HSPACE="20" ALT="Previous"></A> <A HREF="PPCNumerics-2.html"><IMG ALIGN="BOTTOM" SRC="content.gif" BORDER="none" HSPACE="20" ALT="Book Contents"></A> <A HREF="PPCNumerics-192.html"><IMG ALIGN="BOTTOM" SRC="index.gif" BORDER="none" HSPACE="20" ALT="Book Index"></A> <A HREF="PPCNumerics-18.html"><IMG ALIGN="BOTTOM" SRC="next.gif" BORDER="none" HSPACE="20" ALT="Next"></A> </CENTER><P>
<CENTER><FONT SIZE="-1"><A HREF="PPCNumerics-3.html">&copy; Apple Computer, Inc.</A><BR>13 JUL 1996</CENTER></FONT><P>

<!-- start of footer  -->

<!--#include virtual="/includes/framesetfooter" -->

<!-- end of footer -->


</BODY>
</HTML>  
