<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
	<title>Audio Unit Programming Guide: The Audio Unit</title>
	<meta id="Generator" name="Generator" content="Gutenberg"/>
	<meta id="GeneratorVersion" name="GeneratorVersion" content="v132"/>
	<meta http-equiv="content-type" content="text/html;charset=utf-8"/>
	<meta id="Copyright" name="Copyright" content="Copyright 2009 Apple Inc. All Rights Reserved."/>
	<meta id="IndexTitle" name="IndexTitle" content="The Audio Unit"/>
	<meta id="xcode-display" name="xcode-display" content="render"/>
	<meta id="toc-file" name="toc-file" content="../toc.html"/>
	<meta id="RESOURCES" content="../../../../Resources" />
	
	<link rel="stylesheet" type="text/css" href="../../../../Resources/CSS/frameset_styles.css"/>
	<script language="JavaScript" type="text/javascript" src="../../../../Resources/JavaScript/lib/prototype.js"></script>
	<script language="JavaScript" type="text/javascript" src="../../../../Resources/JavaScript/lib/scriptaculous.js"></script>
	<script language="JavaScript" type="text/javascript" src="../../../../Resources/JavaScript/page.js"></script>
	<script language="JavaScript" type="text/javascript" src="../../../../Resources/JavaScript/pedia.js"></script>
	<!--[if lte IE 6]>
		<style type="text/css">
			/*<![CDATA[*/ 
			html {overflow-x:auto; overflow-y:hidden;  }
			/*]]>*/
		</style>
	<![endif]-->
</head>    
<body bgcolor="#ffffff" onload="initialize_page();"><a name="//apple_ref/doc/uid/TP40003278-CH12" title="The Audio Unit"></a>
    <noscript>
    <div id="tocMenu">
        <iframe id="toc_content" name="toc_content" SRC="../toc.html" width="210" height="100%" align="left" frameborder="0">This document set is best viewed in a browser that supports iFrames.</iframe>
    </div>
    </noscript>
    <div id="bodyText">
        <a name="top"></a>
        <div class="hideOnPrint hideInXcode">
        <!-- start of header -->
        <!--#include virtual="/includes/framesetheader" -->
        <!-- end of header -->
        </div>
        
        <!-- start of path -->
<div class="breadcrumb hideOnPrint hideInXcode"><a href="http://developer.apple.com/" target="_top">ADC Home</a> &gt; <a href="../../../../../referencelibrary/index.html#//apple_ref/doc/uid/TP30000943" target="_top">Reference Library</a> &gt; <a href="../../../../index.html#//apple_ref/doc/uid/TP30000440" target="_top">Guides</a> &gt; <a href="../../../index.html#//apple_ref/doc/uid/TP30000440-TP30000428" target="_top">Audio</a> &gt; <a href="../../../CoreAudio-date.html#//apple_ref/doc/uid/TP30000440-TP30000428-TP30000500" target="_top">Core Audio</a> &gt; <a href="../Introduction/Introduction.html#//apple_ref/doc/uid/TP40003278-CH1-SW2">Audio Unit Programming Guide</a> &gt; </div><br class="hideInXcode"/><!-- end of path -->
        
        <div class="mini_nav_text" align="left">
        <span class="navButtons">
        <a href="../AudioUnitDevelopmentFundamentals/AudioUnitDevelopmentFundamentals.html">&lt; Previous Page</a><span style="margin-left: 8px"><a href="../TheAudioUnitView/TheAudioUnitView.html">Next Page &gt;</a></span>
        </span>
        <span id="showHideTOCUpperSpan">
        <a href="#" onclick="showHideTOC();"><img src="../../../../Resources/Images/show_toc_icon.gif" width="15" height="14" border="0" style="margin-bottom: -2px;" alt="" hideText="Hide TOC" showText="Show TOC" /></a> <a href="#" onclick="showHideTOC();">Hide TOC</a>
        </span>
        </div>

        <hr />
        
        
        <a name="//apple_ref/doc/uid/TP40003278-CH12-SW1" title="The Audio Unit"></a><h1>The Audio Unit</h1><p>When you develop an audio unit, you begin with the part that performs the audio work. This part exists within the <code>MacOS</code> folder inside the audio unit bundle as shown in <span class="content_text"><a href="../AudioUnitDevelopmentFundamentals/AudioUnitDevelopmentFundamentals.html#//apple_ref/doc/uid/TP40003278-CH7-SW7">Figure 1-2</a></span>. You can optionally add a custom user interface, or view, as described in the next chapter, <span class="content_text"><a href="../TheAudioUnitView/TheAudioUnitView.html#//apple_ref/doc/uid/TP40003278-CH13-SW1">“The Audio Unit View.”</a></span></p><p>In this chapter you learn about the architecture and programmatic elements of an audio unit. You also learn about the steps you take when you create an audio unit.</p>
<!-- This template is being used for both PDF and HTML. -->

    
    <h4>In this section:</h4>
    
    
    <p class="blockquote">
    
        
			
			
				<a href="TheAudioUnit.html#//apple_ref/doc/uid/TP40003278-CH12-SW16">Audio Unit Architecture</a>
				
			<br/>
			
        
			
			
				<a href="TheAudioUnit.html#//apple_ref/doc/uid/TP40003278-CH12-DontLinkElementID_36">Creating an Audio Unit by Subclassing</a>
				
			<br/>
			
        
			
			
				<a href="TheAudioUnit.html#//apple_ref/doc/uid/TP40003278-CH12-SW10">Control Code: Parameters, Factory Presets, and Properties</a>
				
			<br/>
			
        
			
			
				<a href="TheAudioUnit.html#//apple_ref/doc/uid/TP40003278-CH12-SW14">Synthesis, Processing, and Data Format Conversion Code</a>
				
			<br/>
			
        
			
			
				<a href="TheAudioUnit.html#//apple_ref/doc/uid/TP40003278-CH12-SW2">Audio Unit Life Cycle</a>
				
			<br/>
			
        

    </p><br/>

<a name="//apple_ref/doc/uid/TP40003278-CH12-SW16" title="Audio Unit Architecture"></a><h2>Audio Unit Architecture</h2><p>The internal architecture of an audio unit consists of scopes, elements, connections, and channels, all of which serve the audio processing code. <span class="content_text">Figure 2-1</span> illustrates these parts as they exist in a typical effect unit. This section describes each of these parts in turn. For discussion on the section marked <em>DSP</em> in the figure, representing the audio processing code in an effect unit, see <span class="content_text"><a href="TheAudioUnit.html#//apple_ref/doc/uid/TP40003278-CH12-SW14">“Synthesis, Processing, and Data Format Conversion Code.”</a></span></p><br/><div><a name="//apple_ref/doc/uid/TP40003278-CH12-SW11" title="Figure 2-1Audio unit architecture for an effect unit"></a><p><strong>Figure 2-1&nbsp;&nbsp;</strong>Audio unit architecture for an effect unit</p><img src = "../Art/au_architecture.jpg" alt = "Audio unit architecture for an effect unit" ></div><br/><a name="//apple_ref/doc/uid/TP40003278-CH12-SW21" title="Audio Unit Scopes"></a><h3>Audio Unit Scopes</h3><p>An audio unit <strong>scope</strong> is a programmatic context. Unlike the general computer science notion of scopes, however, audio unit scopes cannot be nested. Each scope is a discrete context.</p><p>You use scopes when writing code that sets or retrieves values of parameters and properties. For example, <span class="content_text">Listing 2-1</span> shows an implementation of a standard <code><a href="../../../Reference/CoreAudio/audiocodec/audiocodec.html#//apple_ref/doc/c_ref/GetProperty" target="_top">GetProperty</a></code> method, as used in the effect unit you build in <span class="content_text"><a href="../Tutorial-BuildingASimpleEffectUnitWithAGenericView/Tutorial-BuildingASimpleEffectUnitWithAGenericView.html#//apple_ref/doc/uid/TP40003278-CH5-SW4">“Tutorial: Building a Simple Effect Unit with a Generic View”</a></span>:</p><a name="//apple_ref/doc/uid/TP40003278-CH12-SW19" title="Listing 2-1Using &acirc;&#128;&#156;scope&acirc;&#128;&#157; in the GetProperty method"></a><p class="codesample"><strong>Listing 2-1&nbsp;&nbsp;</strong>Using “scope” in the GetProperty method</p><div class="codesample"><table><tr><td scope="row"><pre>ComponentResult TremoloUnit::GetProperty (<span></span></pre></td></tr><tr><td scope="row"><pre>    AudioUnitPropertyID    inID,<span></span></pre></td></tr><tr><td scope="row"><pre class="bold">    AudioUnitScope         inScope,   // the host specifies the scope</pre><pre><span></span></pre></td></tr><tr><td scope="row"><pre>    AudioUnitElement       inElement,<span></span></pre></td></tr><tr><td scope="row"><pre>    void                   *outData<span></span></pre></td></tr><tr><td scope="row"><pre>) {<span></span></pre></td></tr><tr><td scope="row"><pre>    return AUEffectBase::GetProperty (inID, inScope, inElement, outData);<span></span></pre></td></tr><tr><td scope="row"><pre>}<span></span></pre></td></tr></table></div>	<p>When a host application calls this method to retrieve the value of a property, the host specifies the scope in which the property is defined. The implementation of the GetProperty method, in turn, can respond to various scopes with code such as this:</p><div class="codesample"><table><tr><td scope="row"><pre>if (inScope == kAudioUnitScope_Global) {<span></span></pre></td></tr><tr><td scope="row"><pre>    // respond to requests targeting the global scope<span></span></pre></td></tr><tr><td scope="row"><pre>} else if (inScope == kAudioUnitScope_Input) {<span></span></pre></td></tr><tr><td scope="row"><pre>    // respond to requests targeting the input scope<span></span></pre></td></tr><tr><td scope="row"><pre>} else {<span></span></pre></td></tr><tr><td scope="row"><pre>    // respond to other requests<span></span></pre></td></tr><tr><td scope="row"><pre>}<span></span></pre></td></tr></table></div>	<p>There are five scopes defined by Apple in the <code>AudioUnitProperties.h</code> header file in the Audio Unit framework, shown in <span class="content_text">Listing 2-2</span>:</p><a name="//apple_ref/doc/uid/TP40003278-CH12-SW29" title="Listing 2-2Audio unit scopes"></a><p class="codesample"><strong>Listing 2-2&nbsp;&nbsp;</strong>Audio unit scopes</p><div class="codesample"><table><tr><td scope="row"><pre>enum {<span></span></pre></td></tr><tr><td scope="row"><pre>    kAudioUnitScope_Global   = 0,<span></span></pre></td></tr><tr><td scope="row"><pre>    kAudioUnitScope_Input    = 1,<span></span></pre></td></tr><tr><td scope="row"><pre>    kAudioUnitScope_Output   = 2,<span></span></pre></td></tr><tr><td scope="row"><pre>    kAudioUnitScope_Group    = 3,<span></span></pre></td></tr><tr><td scope="row"><pre>    kAudioUnitScope_Part     = 4<span></span></pre></td></tr><tr><td scope="row"><pre>};<span></span></pre></td></tr></table></div>	<p> The three most important scopes are:</p><ul class="spaceabove"><li class="li"><p>Input scope: The context for audio data coming into an audio unit. Code in an audio unit, a host application, or an audio unit view can address an audio unit’s input scope for such things as the following:</p><ul class="nested"><li class="nested li"><p>An audio unit defining additional input elements</p></li><li class="nested li"><p>An audio unit or a host setting an input audio data stream format</p></li><li class="nested li"><p>An audio unit view setting the various input levels on a mixer audio unit</p></li><li class="nested li"><p>A host application connecting audio units into an audio processing graph</p></li></ul><p>Host applications also use the input scope when registering a render callback, as described in <span class="content_text"><a href="TheAudioUnit.html#//apple_ref/doc/uid/TP40003278-CH12-SW12">“Render Callback Connections.”</a></span></p></li><li class="li"><p>Output scope: The context for audio data leaving an audio unit. The output scope is used for most of the same things as input scope: connections, defining additional output elements, setting an output audio data stream format, and setting output levels in the case of a mixer unit with multiple outputs.</p><p>A host application, or a downstream audio unit in an audio processing graph, also addresses the output scope when invoking rendering.</p></li><li class="li"><p>Global scope: The context for audio unit characteristics that apply to the audio unit as a whole. Code within an audio unit addresses its own global scope for setting or getting the values of such properties as:</p><ul class="nested"><li class="nested li"><p>latency,</p></li><li class="nested li"><p>tail time, and</p></li><li class="nested li"><p>supported number(s) of channels.</p></li></ul></li></ul><p>Host applications can also query the global scope of an audio unit to get these values.</p><p>There are two additional audio unit scopes, intended for instrument units, defined in <code>AudioUnitProperties.h</code>:</p><ul class="spaceabove"><li class="li"><p>Group scope: A context specific to the rendering of musical notes in instrument units</p></li><li class="li"><p>Part scope: A context specific to managing the various voices of multitimbral instrument units</p></li></ul><p>This version of <em>Audio Unit Programming Guide</em> does not discuss group scope or part scope.</p><a name="//apple_ref/doc/uid/TP40003278-CH12-DontLinkElementID_33" title="Audio Unit Elements"></a><h3>Audio Unit Elements</h3><p>An audio unit <strong>element</strong> is a programmatic context that is nested within a scope. Most commonly, elements come into play in the input and output scopes. Here, they serve as programmatic analogs of the signal buses used in hardware audio devices. Because of this analogy, audio unit developers often refer to elements in the input or output scopes as <em>buses</em>; this document follows suit.</p><p>As you may have noticed in <span class="content_text"><a href="TheAudioUnit.html#//apple_ref/doc/uid/TP40003278-CH12-SW19">Listing 2-1</a></span>, hosts specify the element as well as the scope they are targeting when getting or setting properties or parameters. Here is that method again, with the <em>inElement</em> parameter highlighted:</p><a name="//apple_ref/doc/uid/TP40003278-CH12-DontLinkElementID_77" title="Listing 2-3Using &acirc;&#128;&#156;element&acirc;&#128;&#157; in the GetProperty method"></a><p class="codesample"><strong>Listing 2-3&nbsp;&nbsp;</strong>Using “element” in the GetProperty method</p><div class="codesample"><table><tr><td scope="row"><pre>ComponentResult TremoloUnit::GetProperty (<span></span></pre></td></tr><tr><td scope="row"><pre>    AudioUnitPropertyID    inID,<span></span></pre></td></tr><tr><td scope="row"><pre>    AudioUnitScope         inScope,<span></span></pre></td></tr><tr><td scope="row"><pre class="bold">    AudioUnitElement       inElement,  // the host specifies the element here</pre><pre><span></span></pre></td></tr><tr><td scope="row"><pre>    void                   *outData<span></span></pre></td></tr><tr><td scope="row"><pre>) {<span></span></pre></td></tr><tr><td scope="row"><pre>    return AUEffectBase::GetProperty (inID, inScope, inElement, outData);<span></span></pre></td></tr><tr><td scope="row"><pre>}<span></span></pre></td></tr></table></div>	<p>Elements are identified by integer numbers and are zero indexed. In the input and output scopes, element numbering must be contiguous. In the typical case, the input and output scopes each have one element, namely element (or bus) <code>0</code>.</p><p>The global scope in an audio unit is unusual in that it always has exactly one element. Therefore, the global scope’s single element is always element <code>0</code>.</p><p>A bus (that is, an input or output element) always has exactly one stream format. The stream format specifies a variety of characteristics for the bus, including sample rate and number of channels. Stream format is described by the <strong>audio stream description structure</strong> (<code>AudioStreamBasicDescription</code>), declared in the <code>CoreAudioTypes.h</code> header file and shown in <span class="content_text">Listing 2-4</span>:</p><a name="//apple_ref/doc/uid/TP40003278-CH12-SW28" title="Listing 2-4The audio stream description structure"></a><p class="codesample"><strong>Listing 2-4&nbsp;&nbsp;</strong>The audio stream description structure</p><div class="codesample"><table><tr><td scope="row"><pre>struct AudioStreamBasicDescription {<span></span></pre></td></tr><tr><td scope="row"><pre>    Float64 mSampleRate;        // sample frames per second<span></span></pre></td></tr><tr><td scope="row"><pre>    UInt32  mFormatID;          // a four-char code indicating stream type<span></span></pre></td></tr><tr><td scope="row"><pre>    UInt32  mFormatFlags;       // flags specific to the stream type<span></span></pre></td></tr><tr><td scope="row"><pre>    UInt32  mBytesPerPacket;    // bytes per packet of audio data<span></span></pre></td></tr><tr><td scope="row"><pre>    UInt32  mFramesPerPacket;   // frames per packet of audio data<span></span></pre></td></tr><tr><td scope="row"><pre>    UInt32  mBytesPerFrame;     // bytes per frame of audio data<span></span></pre></td></tr><tr><td scope="row"><pre>    UInt32  mChannelsPerFrame;  // number of channels per frame<span></span></pre></td></tr><tr><td scope="row"><pre>    UInt32  mBitsPerChannel;    // bit depth<span></span></pre></td></tr><tr><td scope="row"><pre>    UInt32  mReserved;          // padding<span></span></pre></td></tr><tr><td scope="row"><pre>};<span></span></pre></td></tr><tr><td scope="row"><pre>typedef struct AudioStreamBasicDescription AudioStreamBasicDescription;<span></span></pre></td></tr></table></div>	<p>An audio unit can let a host application get and set the stream formats of its buses using the <code>kAudioUnitProperty_StreamFormat</code> property, declared in the <code>AudioUnitProperties.h</code> header file. This property’s value is an audio stream description structure.</p><p>Typically, you will need just a single input bus and a single output bus in an audio unit. When you create an effect unit by subclassing the <code>AUEffectBase</code> class, you get one input and one output bus by default. Your audio unit can specify additional buses by overriding the main class’s constructer. You would then indicate additional buses using the <code>kAudioUnitProperty_BusCount</code> property, or its synonym <code>kAudioUnitProperty_ElementCount</code>, both declared in the <code>AudioUnitProperties.h</code> header file.</p><p>You might find additional buses helpful if you are building an interleaver or deinterleaver audio unit, or an audio unit that contains a primary audio data path as well as a sidechain path for modulation data.</p><p></p><p>A bus can have exactly one connection, as described next.</p><p></p><a name="//apple_ref/doc/uid/TP40003278-CH12-DontLinkElementID_34" title="Audio Unit Connections"></a><h3>Audio Unit Connections</h3><p></p><p>A <strong>connection</strong> is a hand-off point for audio data entering or leaving an audio unit. Fresh audio data samples move through a connection and into an audio unit when the audio unit calls a render callback. Processed audio data samples leave an audio unit when the audio unit’s render method gets called. The Core Audio SDK’s class hierarchy implements audio data hand-off, working with an audio unit’s rendering code.</p><p>Hosts establish connections at the granularity of a bus, and not of individual channels. You can see this in <span class="content_text"><a href="TheAudioUnit.html#//apple_ref/doc/uid/TP40003278-CH12-SW11">Figure 2-1</a></span>. The number of channels in a connection is defined by the stream format, which is set for the bus that contains the connection.</p><p></p><a name="//apple_ref/doc/uid/TP40003278-CH12-SW17" title="Audio Processing Graph Connections"></a><h4>Audio Processing Graph Connections</h4><p>To connect one audio unit to another, a host application sets a property in the destination audio unit. Specifically, it sets the <code>kAudioUnitProperty_MakeConnection</code> property in the input scope of the destination audio unit. When you build your audio units using the Core Audio SDK, this property is implemented for you.</p><p>In setting a value for this property, the host specifies the source and destination bus numbers using an <strong>audio unit connection structure</strong> (<code>AudioUnitConnection</code>), shown in <span class="content_text">Listing 2-5</span>:</p><a name="//apple_ref/doc/uid/TP40003278-CH12-SW27" title="Listing 2-5The audio unit connection structure"></a><p class="codesample"><strong>Listing 2-5&nbsp;&nbsp;</strong>The audio unit connection structure</p><div class="codesample"><table><tr><td scope="row"><pre>typedef struct AudioUnitConnection {<span></span></pre></td></tr><tr><td scope="row"><pre>    AudioUnit sourceAudioUnit;    // the audio unit that supplies audio<span></span></pre></td></tr><tr><td scope="row"><pre>                                  //    data to the audio unit whose<span></span></pre></td></tr><tr><td scope="row"><pre>                                  //    connection property is being set<span></span></pre></td></tr><tr><td scope="row"><pre>    UInt32    sourceOutputNumber; // the output bus of the source unit<span></span></pre></td></tr><tr><td scope="row"><pre>    UInt32    destInputNumber;    // the input bus of the destination unit<span></span></pre></td></tr><tr><td scope="row"><pre>} AudioUnitConnection;<span></span></pre></td></tr></table></div>	<p>The <code>kAudioUnitProperty_MakeConnection</code> property and the audio unit connection structure are declared in the <code>AudioUnitProperties.h</code> file in the Audio Unit framework.</p><p>As an audio unit developer, you must make sure that your audio unit can be connected for it to be valid. You do this by supporting appropriate stream formats. When you create an audio unit by subclassing the classes in the SDK, your audio unit will be connectible. The default, required stream format for audio units is described in <span class="content_text"><a href="TheAudioUnit.html#//apple_ref/doc/uid/TP40003278-CH12-SW20">“Commonly Used Properties.”</a></span></p><p><span class="content_text"><a href="../AudioUnitDevelopmentFundamentals/AudioUnitDevelopmentFundamentals.html#//apple_ref/doc/uid/TP40003278-CH7-SW14">Figure 1-7</a></span> illustrates that the entity upstream from an audio unit can be either another audio unit or a host application. Whichever it is, the upstream entity is typically responsible for setting an audio unit’s input stream format before a connection is established. If an audio unit cannot support the stream format being requested, it returns an error and the connection fails.</p><a name="//apple_ref/doc/uid/TP40003278-CH12-SW12" title="Render Callback Connections"></a><h4>Render Callback Connections</h4><p>A host application can send audio data to an audio unit directly and can retrieve processed data from the audio unit directly. You don’t need to make any changes to your audio unit to support this sort of connection.</p><p>To prepare to send data to an audio unit, a host defines a <strong>render callback</strong> (shown in <span class="content_text"><a href="../AudioUnitDevelopmentFundamentals/AudioUnitDevelopmentFundamentals.html#//apple_ref/doc/uid/TP40003278-CH7-SW14">Figure 1-7</a></span>) and registers it with the audio unit. The signature for the callback is declared in the <code>AUComponent.h</code> header file in the Audio Unit framework, as shown in <span class="content_text">Listing 2-6</span>:</p><a name="//apple_ref/doc/uid/TP40003278-CH12-SW26" title="Listing 2-6The render callback"></a><p class="codesample"><strong>Listing 2-6&nbsp;&nbsp;</strong>The render callback</p><div class="codesample"><table><tr><td scope="row"><pre>typedef OSStatus (*AURenderCallback)(<span></span></pre></td></tr><tr><td scope="row"><pre>    void                          *inRefCon,<span></span></pre></td></tr><tr><td scope="row"><pre>    AudioUnitRenderActionFlags    *ioActionFlags,<span></span></pre></td></tr><tr><td scope="row"><pre>    const AudioTimeStamp          *inTimeStamp,<span></span></pre></td></tr><tr><td scope="row"><pre>    UInt32                        inBusNumber,<span></span></pre></td></tr><tr><td scope="row"><pre>    UInt32                        inNumberFrames,<span></span></pre></td></tr><tr><td scope="row"><pre>    AudioBufferList               *ioData<span></span></pre></td></tr><tr><td scope="row"><pre>);<span></span></pre></td></tr></table></div>	<p>The host must explicitly set the stream format for the audio unit’s input as a prerequisite to making the connection. The audio unit calls the callback in the host when it’s ready for more audio data.</p><p>In contrast, for an audio processing graph connection, the upstream audio unit supplies the render callback. In a graph, the upstream audio unit also sets the downstream audio unit’s input stream format.</p><p>A host can retrieve processed audio data from an audio unit directly by calling the <code><a href="../../../Reference/CoreAudio/audio_units/audio_units.html#//apple_ref/doc/c_ref/AudioUnitRender" target="_top">AudioUnitRender</a></code> function on the audio unit, as shown in <span class="content_text">Listing 2-7</span>:</p><a name="//apple_ref/doc/uid/TP40003278-CH12-SW25" title="Listing 2-7The AudioUnitRender function"></a><p class="codesample"><strong>Listing 2-7&nbsp;&nbsp;</strong>The AudioUnitRender function</p><div class="codesample"><table><tr><td scope="row"><pre>extern ComponentResult AudioUnitRender (<span></span></pre></td></tr><tr><td scope="row"><pre>    AudioUnit                     ci,<span></span></pre></td></tr><tr><td scope="row"><pre>    AudioUnitRenderActionFlags    *ioActionFlags,<span></span></pre></td></tr><tr><td scope="row"><pre>    const AudioTimeStamp          *inTimeStamp,<span></span></pre></td></tr><tr><td scope="row"><pre>    UInt32                        inOutputBusNumber,<span></span></pre></td></tr><tr><td scope="row"><pre>    UInt32                        inNumberFrames,<span></span></pre></td></tr><tr><td scope="row"><pre>    AudioBufferList               *ioData<span></span></pre></td></tr><tr><td scope="row"><pre>);<span></span></pre></td></tr></table></div>	<p>The Core Audio SDK passes this function call into your audio unit as a call to the audio unit’s <code><!--a-->Render<!--/a--></code> method.</p><p>You can see the similarity between the render callback and <code><a href="../../../Reference/CoreAudio/audio_units/audio_units.html#//apple_ref/doc/c_ref/AudioUnitRender" target="_top">AudioUnitRender</a></code> signatures, which reflects their coordinated use in audio processing graph connections. Like the render callback, the <code><a href="../../../Reference/CoreAudio/audio_units/audio_units.html#//apple_ref/doc/c_ref/AudioUnitRender" target="_top">AudioUnitRender</a></code> function is declared in the <code>AUComponent.h</code> header file in the Audio Unit framework.</p><a name="//apple_ref/doc/uid/TP40003278-CH12-DontLinkElementID_35" title="Audio Unit Channels"></a><h3>Audio Unit Channels</h3><p>An audio unit <strong>channel</strong> is, conceptually, a monaural, noninterleaved path for audio data samples that goes to or from an audio unit’s processing code. The Core Audio SDK represents channels as buffers. Each buffer is described by an audio buffer structure (<code>AudioBuffer</code>), as declared in the <code>CoreAudioTypes.h</code> header file in the Core Audio framework, as shown in <span class="content_text">Listing 2-8</span>:</p><a name="//apple_ref/doc/uid/TP40003278-CH12-SW24" title="Listing 2-8The audio buffer structure"></a><p class="codesample"><strong>Listing 2-8&nbsp;&nbsp;</strong>The audio buffer structure</p><div class="codesample"><table><tr><td scope="row"><pre>struct AudioBuffer {<span></span></pre></td></tr><tr><td scope="row"><pre>    UInt32  mNumberChannels; // number of interleaved channels in the buffer<span></span></pre></td></tr><tr><td scope="row"><pre>    UInt32  mDataByteSize;   // size, in bytes, of the buffer<span></span></pre></td></tr><tr><td scope="row"><pre>    void    *mData;          // pointer to the buffer<span></span></pre></td></tr><tr><td scope="row"><pre>};<span></span></pre></td></tr><tr><td scope="row"><pre>typedef struct AudioBuffer AudioBuffer;<span></span></pre></td></tr></table></div>	<p>An audio buffer can hold a single channel, or multiple interleaved channels. However, most types of audio units, including effect units, use only noninterleaved data. These audio units expect the <code>mNumberChannels</code> field in the audio buffer structure to equal <code>1</code>.</p><p>Output units and format converter units can accept interleaved channels, represented by an audio buffer with the <code>mNumberChannels</code> field set to <code>2</code> or greater.</p><p>An audio unit manages the set of channels in a bus as an audio buffer list structure (<code>AudioBufferList</code>), also defined in <code>CoreAudioTypes.h</code>, as shown in <span class="content_text">Listing 2-9</span>:</p><a name="//apple_ref/doc/uid/TP40003278-CH12-SW23" title="Listing 2-9The audio buffer list structure"></a><p class="codesample"><strong>Listing 2-9&nbsp;&nbsp;</strong>The audio buffer list structure</p><div class="codesample"><table><tr><td scope="row"><pre>struct AudioBufferList {<span></span></pre></td></tr><tr><td scope="row"><pre>    UInt32      mNumberBuffers;  // the number of buffers in the list<span></span></pre></td></tr><tr><td scope="row"><pre>    AudioBuffer mBuffers[kVariableLengthArray]; // the list of buffers<span></span></pre></td></tr><tr><td scope="row"><pre>};<span></span></pre></td></tr><tr><td scope="row"><pre>typedef struct AudioBufferList  AudioBufferList;<span></span></pre></td></tr></table></div>	<p>In the common case of building an <em>n</em>-to-<em>n</em> channel effect unit, such as the one you build in <span class="content_text"><a href="../Tutorial-BuildingASimpleEffectUnitWithAGenericView/Tutorial-BuildingASimpleEffectUnitWithAGenericView.html#//apple_ref/doc/uid/TP40003278-CH5-SW4">“Tutorial: Building a Simple Effect Unit with a Generic View,”</a></span> the audio unit template and superclasses take care of managing channels for you. You create this type of effect unit by subclassing the <code>AUEffectBase</code> class in the SDK.</p><p>In contrast, when you build an <em>m</em>-to-<em>n</em> channel effect unit (for example, stereo-to-mono effect unit), you must write code to manage channels. In this case, you create your effect unit by subclassing the <code>AUBase</code> class. (As with the rest of this document, this consideration applies to version 1.4.3 of the Core Audio SDK, current at the time of publication.)</p><a name="//apple_ref/doc/uid/TP40003278-CH12-DontLinkElementID_36" title="Creating an Audio Unit by Subclassing"></a><h2>Creating an Audio Unit by Subclassing</h2><p>The simplest, and recommended, way to create an audio unit is by subclassing the Core Audio SDK’s C++ superclasses. With minimal effort, this gives you all of the programmatic scaffolding and hooks your audio unit needs to interact with other parts of Core Audio, the Component Manager, and audio unit host applications.</p><p>For example, when you build an <em>n</em>-channel to <em>n</em>-channel effect unit using the SDK, you define your audio unit’s main class as a subclass of the <code>AUEffectBase</code> superclass. When you build a basic instrument unit (also known as a software-based music synthesizer), you define your audio unit’s main class as a subclass of the SDK <code>AUInstrumentBase</code> superclass. <span class="content_text"><a href="../AudioUnitClassHierarchy/AudioUnitClassHierarchy.html#//apple_ref/doc/uid/TP40003278-CH9-SW2">“Appendix: Audio Unit Class Hierarchy”</a></span> describes these classes along with the others in the SDK.</p><p>In practice, subclassing usually amounts to one of two things:</p><ul class="ul"><li class="li"><p>Creating an audio unit Xcode project with a supplied template. In this case, creating the project gives you source files that define custom subclasses of the appropriate superclasses. You modify and extend these files to define the custom features and behavior of your audio unit.</p></li><li class="li"><p>Making a copy of an audio unit project from the SDK, which already contains custom subclasses. In this case, you may need to strip out code that isn’t relevant to your audio unit, as well as change symbols in the project to properly identify and refer to your audio unit. You then work in the same way you would had you started with an Xcode template.</p></li></ul><a name="//apple_ref/doc/uid/TP40003278-CH12-SW10" title="Control Code: Parameters, Factory Presets, and Properties"></a><h2>Control Code: Parameters, Factory Presets, and Properties</h2><p>Most audio units are user adjustable in real time. For example, a reverb unit might have user settings for initial delay, reverberation density, decay time, and dry/wet mix. Such adjustable settings are called <strong>parameters</strong>. Parameters have floating point or integer values. Floating point parameters typically use a slider interface in the audio unit’s view. You can associate names with integer values to provide a menu interface for a parameter, such as to let the user pick tremolo type in a tremolo effect unit. Built-in (developer defined) combinations of parameter settings in an audio unit are called <strong>factory presets</strong>.</p><p>All audio units also have characteristics, typically non-time varying and not directly settable by a user, called <strong>properties</strong>. A property is a key/value pair that refines the plug-in API of your audio unit by declaring attributes or behavior. For example, you use the property mechanism to declare such audio unit characteristics as sample latency and audio data stream format. Each property has an associated data type to hold its value. For more on properties, as well as definitions of latency and stream format, see <span class="content_text"><a href="TheAudioUnit.html#//apple_ref/doc/uid/TP40003278-CH12-SW20">“Commonly Used Properties.”</a></span></p><p>Host applications can query an audio unit about its parameters and about its standard properties, but not about its custom properties. Custom properties are for communication between an audio unit and a custom view designed in concert with the audio unit.</p><p>To get parameter information from an audio unit, a host application first gets the value of the audio unit’s <code>kAudioUnitProperty_ParameterList</code> property, a property provided for you by superclasses in the SDK. This property’s value is a list of the defined parameter IDs for the audio unit. The host can then query the <code>kAudioUnitProperty_ParameterInfo</code> property for each parameter ID.</p><p>Hosts and views can also receive parameter and property change information using notifications, as described in <span class="content_text"><a href="../TheAudioUnitView/TheAudioUnitView.html#//apple_ref/doc/uid/TP40003278-CH13-SW5">“Parameter and Property Events.”</a></span></p><a name="//apple_ref/doc/uid/TP40003278-CH12-SW15" title="Defining and Using Parameters"></a><h3>Defining and Using Parameters</h3><p>Specifying parameters means specifying which settings you’d like to offer for control by the user, along with appropriate units and ranges. For example, an audio unit that provides a tremolo effect might offer a parameter for tremolo rate. You’d probably specify a unit of hertz and might specify a range from 1 to 10.</p><p>To define the parameters for an audio unit, you override the <code>GetParameterInfo</code> method from the <code>AUBase</code> class. You write this method to tell the view how to represent a control for each parameter, and to specify each parameter’s default value.</p><p>The <code>GetParameterInfo</code> method may be called:</p><ul class="spaceabove"><li class="li"><p>By the audio unit’s view (custom if you provide one, generic otherwise) when the view is drawn on screen</p></li><li class="li"><p>By a host application that is providing a generic view for the audio unit</p></li><li class="li"><p>By a host application that is representing the audio unit’s parameters on a hardware control surface</p></li></ul><p>To make use of a parameter’s current setting (as adjusted by a user) when rendering audio, you call the <code><!--a-->GetParameter<!--/a--></code> method. This method is inherited from the <code>AUEffectBase</code> class.</p><p>The <code><!--a-->GetParameter<!--/a--></code> method takes a parameter ID as its one argument and returns the parameter’s current value. You typically make this call within the audio unit’s <code><!--a-->Process<!--/a--></code> method to update the parameter value once for each render cycle. Your rendering code can then use the parameter’s current value.</p><p>In addition to the <code>GetParameterInfo</code> method (for telling a view or host about a parameter’s current value) and the <code><!--a-->GetParameter<!--/a--></code> method (for making use of a parameter value during rendering), an audio unit needs a way to set its parameter values. For this it typically uses the <code>SetParameter</code> method, from the <code>AUEffectBase</code> class.</p><p>There are two main times an audio unit calls the <code>SetParameter</code> method:</p><ul class="spaceabove"><li class="li"><p>During instantiation—in its constructor method—to set its default parameter values</p></li><li class="li"><p>When running—when a host or a view invokes a parameter value change—to update its parameter values</p></li></ul><p>The <code><!--a-->SetParameter<!--/a--></code> method takes two method parameters—the ID of the parameter to be changed and its new value, as shown in <span class="content_text">Listing 2-10</span>:</p><a name="//apple_ref/doc/uid/TP40003278-CH12-SW22" title="Listing 2-10The SetParameter method"></a><p class="codesample"><strong>Listing 2-10&nbsp;&nbsp;</strong>The SetParameter method</p><div class="codesample"><table><tr><td scope="row"><pre>void SetParameter(<span></span></pre></td></tr><tr><td scope="row"><pre>    UInt32 paramID,<span></span></pre></td></tr><tr><td scope="row"><pre>    Float32 value<span></span></pre></td></tr><tr><td scope="row"><pre>);<span></span></pre></td></tr></table></div>	<p>The audio unit you build in <span class="content_text"><a href="../Tutorial-BuildingASimpleEffectUnitWithAGenericView/Tutorial-BuildingASimpleEffectUnitWithAGenericView.html#//apple_ref/doc/uid/TP40003278-CH5-SW4">“Tutorial: Building a Simple Effect Unit with a Generic View”</a></span> makes use of all three of these methods: <code>GetParameterInfo</code>, <code><!--a-->GetParameter<!--/a--></code>, and <code>SetParameter</code>.</p><p>An audio unit sometimes needs to invoke a value change for one of its parameters. It might do this in response to a change (invoked by a view or host) in another parameter. When an audio unit on its own initiative changes a parameter value, it should post an event notification.</p><p>For example, in a bandpass filter audio unit, a user might lower the upper corner frequency to a value below the current setting of the frequency band’s lower limit. The audio unit could respond by lowering the lower corner frequency appropriately. In such a case, the audio unit is responsible for posting an event notification about the self-invoked change. The notification informs the view and the host of the lower corner frequency parameter’s new value. To post the notification, the audio unit follows a call to the <code>SetParameter</code> method with a call to the <code>AUParameterListenerNotify</code> method.</p><p></p><a name="//apple_ref/doc/uid/TP40003278-CH12-DontLinkElementID_37" title="Factory Presets and Parameter Persistence"></a><h3>Factory Presets and Parameter Persistence</h3><p>You specify factory presets to provide convenience and added value for users. The more complex the audio unit, and the greater its number of parameters, the more a user will appreciate factory presets.</p><p>For example, the Mac OS X Matrix Reverb unit contains more than a dozen parameters. A user could find setting them in useful combinations daunting. The developers of the Matrix Reverb took this into account and provided a wide range of factory presets with highly descriptive names such as Small Room, Large Hall, and Cathedral.</p><p>The <code>GetPresets</code> method does for factory presets what the <code>GetParameterInfo</code> method does for parameters. You define factory presets by overriding <code>GetPresets</code>, and an audio unit’s view calls this method to populate the view’s factory presets menu.</p><p>When a user chooses a factory preset, the view calls the audio unit’s <code>NewFactoryPresetSet</code> method. You define this method in parallel with the <code>GetPresets</code> method. For each preset you offer in the factory presets menu, you include code in the <code>NewFactoryPresetSet</code> method to set that preset when the user requests it. For each factory preset, this code consists of a series of <code>SetParameter</code> method calls. See <span class="content_text"><a href="../Tutorial-BuildingASimpleEffectUnitWithAGenericView/Tutorial-BuildingASimpleEffectUnitWithAGenericView.html#//apple_ref/doc/uid/TP40003278-CH5-SW4">“Tutorial: Building a Simple Effect Unit with a Generic View”</a></span> for step-by-step guidance on implementing factory presets.</p><p>Parameter persistence is a feature, provided by a host application, that lets a user save parameter settings from one session to the next. When you develop audio units using the Core Audio SDK, your audio units will automatically support parameter persistence.</p><p>Host application developers provide parameter persistence by taking advantage of the SDK’s <code>kAudioUnitProperty_ClassInfo</code> property. This property uses a <code>CFPropertyListRef</code> dictionary to represent the current settings of an audio unit.</p><a name="//apple_ref/doc/uid/TP40003278-CH12-SW13" title="Defining and Using Properties"></a><h3>Defining and Using Properties</h3><p>There are more than 100 Apple-defined properties available for audio units. You find their declarations in the <code>AudioUnitProperties.h</code> header file in the Audio Unit framework. Each type of audio unit has a set of required properties as described in the <em>Audio Unit Specification</em>.</p><p>You can get started as an audio unit developer without touching or even being aware of most of these properties. In most cases, superclasses from the Core Audio SDK take care of implementing the required properties for you. And, in many cases, the SDK sets useful values for them.</p><p>Yet the more you learn about the rich palette of available audio unit properties, the better you can make your audio units.</p><p>Each Apple-defined property has a corresponding data type to represent the property’s value. Depending on the property, the data type is a structure, a dictionary, an array, or a floating point number.</p><p> For example, the <code>kAudioUnitProperty_StreamFormat</code> property, which describes an audio unit’s audio data stream format, stores its value in the <code>AudioStreamBasicDescription</code> structure. This structure is declared in the <code>CoreAudioTypes.h</code> header file in the <code>CoreAudio</code> framework.</p><p>The <code>AUBase</code> superclass provides general getting and setting methods that you can override to implement properties, as described in <span class="content_text"><a href="TheAudioUnit.html#//apple_ref/doc/uid/TP40003278-CH12-SW9">“Defining Custom Properties.”</a></span> These methods, <code><a href="../../../Reference/CoreAudio/audiocodec/audiocodec.html#//apple_ref/doc/c_ref/GetPropertyInfo" target="_top">GetPropertyInfo</a></code>, <code><a href="../../../Reference/CoreAudio/audiocodec/audiocodec.html#//apple_ref/doc/c_ref/GetProperty" target="_top">GetProperty</a></code>, and <code><a href="../../../Reference/CoreAudio/audiocodec/audiocodec.html#//apple_ref/doc/c_ref/SetProperty" target="_top">SetProperty</a></code>, work with associated “dispatch” methods in the SDK that you don’t call directly. The dispatch methods, such as <code>DispatchGetPropertyInfo</code>, provide most of the audio unit property magic for the SDK. You can examine them in the <code>AUBase.cpp</code> file in the SDK to see what they do.</p><p>The <code>AUEffectBase</code> class, <code>MusicDeviceBase</code> class, and other subclasses override the property accessor methods with code for properties specific to one type of audio unit. For example, the <code>AUEffectBase</code> class handles property calls that are specific to effect units, such as the <code>kAudioUnitProperty_BypassEffect</code> property.</p><a name="//apple_ref/doc/uid/TP40003278-CH12-SW20" title="Commonly Used Properties"></a><h4>Commonly Used Properties</h4><p>For some commonly used properties, the Core Audio SDK provides specific accessor methods. For example, The <code>CAStreamBasicDescription</code> class in the SDK provides methods for managing the <code>AudioStreamBasicDescription</code> structure for the <code>kAudioUnitProperty_StreamFormat</code> property.</p><p>Here are a few of the properties you may need to implement for your audio unit. You implement them when you want to customize your audio unit to vary from the default behavior for the audio unit’s type:</p><ul class="spaceabove"><li class="li"><p><code>kAudioUnitProperty_StreamFormat</code></p><p>Declares the audio data stream format for an audio unit’s input or output channels. A host application can set the format for the input and output channels separately. If you don’t implement this property to describe additional stream formats, a superclass from the SDK declares that your audio unit supports the default stream format: non-interleaved, 32-bit floating point, native-endian, linear PCM.</p></li><li class="li"><p><code>kAudioUnitProperty_BusCount</code></p><p>Declares the number of buses (also called elements) in the input or output scope of an audio unit. If you don’t implement this property, a superclass from the SDK declares that your audio unit uses a single input and output bus, each with an ID of <code>0</code>.</p></li><li class="li"><p><code>kAudioUnitProperty_Latency</code></p><p>Declares the minimum possible time for a sample to proceed from input to output of an audio unit, in seconds. For example, an FFT-based filter must acquire a certain number of samples to fill an FFT window before it can calculate an output sample. An audio unit with a latency as short as two or three samples should implement this property to report its latency.</p><p>If the sample latency for your audio unit varies, use this property to report the maximum latency. Alternatively, you can update the <code>kAudioUnitProperty_Latency</code> property value when latency changes, and issue a property change notification using the Audio Unit Event API.</p><p>If your audio unit’s latency is <code>0</code> seconds, you don’t need to implement this property. Otherwise you should, to let host applications compensate appropriately.</p></li><li class="li"><p><code>kAudioUnitProperty_TailTime</code></p><p>Declares the time, beyond an audio unit’s latency, for a nominal-level signal to decay to silence at an audio unit’s output after it has gone instantaneously to silence at the input. Tail time is significant for audio units performing an effect such as delay or reverberation. Apple recommends that all audio units implement the <code>kAudioUnitProperty_TailTime</code> property, even if its value is <code>0</code>.</p><p>If the tail time for your audio unit varies—such as for a variable delay—use this property to report the maximum tail time. Alternatively, you can update the <code>kAudioUnitProperty_TailTime</code> property value when tail time changes, and issue a property change notification using the Audio Unit Event API.</p></li><li class="li"><p><code>kAudioUnitProperty_SupportedNumChannels</code></p><p>Declares the supported numbers of input and output channels for an audio unit. The value for this property is stored in a channel information structure (<code>AUChannelInfo</code>), which is declared in the <code>AudioUnitProperties</code>.h header file:</p><div class="codesample"><table><tr><td scope="row"><pre>typedef struct AUChannelInfo {<span></span></pre></td></tr><tr><td scope="row"><pre>    SInt16    inChannels;<span></span></pre></td></tr><tr><td scope="row"><pre>    SInt16    outChannels;<span></span></pre></td></tr><tr><td scope="row"><pre>} AUChannelInfo;<span></span></pre></td></tr></table></div>	<a name="//apple_ref/doc/uid/TP40003278-CH12-SW30" title="Table 2-1Using a channel information structure"></a><div class="tableholder"><table class="graybox" border = "0" cellspacing="0" cellpadding="5"><caption class="tablecaption"><strong>Table 2-1&nbsp;&nbsp;</strong>Using a channel information structure</caption><tr><th scope="col" align="left" style="font-weight: bold" bgcolor="#CCCCCC"><p>Field values</p></th><th scope="col" align="left" style="font-weight: bold" bgcolor="#CCCCCC"><p>Example</p></th><th scope="col" align="left" style="font-weight: bold" bgcolor="#CCCCCC"><p>Meaning, using example</p></th></tr><tr><td  scope="row"><p>both fields are <code>–1</code></p></td><td ><p><code>inChannels = –1</code></p><p><code>outChannels = –1</code></p></td><td ><p>This is the default case. Any number of input and output channels, as long as the numbers match</p></td></tr><tr><td  scope="row"><p>one field is <code>–1</code>, the other field is positive</p></td><td ><p><code>inChannels = –1</code></p><p><code>outChannels = 2</code></p></td><td ><p>Any number of input channels, exactly two output channels</p></td></tr><tr><td  scope="row"><p>one field is <code>–1</code>, the other field is <code>–2</code></p></td><td ><p><code>inChannels = –1</code></p><p><code>outChannels = –2</code></p></td><td ><p>Any number of input channels, any number of output channels</p></td></tr><tr><td  scope="row"><p>both fields have non-negative values</p></td><td ><p><code>inChannels = 2</code></p><p><code>outChannels = 6</code></p></td><td ><p>Exactly two input channels, exactly six output channels</p></td></tr><tr><td  scope="row"></td><td ><p><code>inChannels = 0</code></p><p><code>outChannels = 2</code></p></td><td ><p>No input channels, exactly two output channels (such as for an instrument unit with stereo output)</p></td></tr><tr><td  scope="row"><p>both fields have negative values, neither of which is <code>–1</code> or <code>–2</code></p></td><td ><p><code>inChannels = –4</code></p><p><code>outChannels = –8</code></p></td><td ><p>Up to four input channels and up to eight output channels</p></td></tr></table></div><p>If you don’t implement this property, a superclass from the SDK declares that your audio unit can use any number of channels provided the number on input matches the number on output.</p></li><li class="li"><p><code>kAudioUnitProperty_CocoaUI</code></p><p>Declares where a host application can find the bundle and the main class for a Cocoa-based view for an audio unit. Implement this property if you supply a Cocoa custom view.</p></li></ul><p>The <code>kAudioUnitProperty_TailTime</code> property is the most common one you’ll need to implement for an effect unit. To do this:</p><ol class="ol"><li class="li"><p>Override the <code><!--a-->SupportsTail<!--/a--></code> method from the <code>AUBase</code> superclass by adding the following method statement to your audio unit custom class definition:</p><div class="codesample"><table><tr><td scope="row"><pre>virtual bool SupportsTail () {return true;}<span></span></pre></td></tr></table></div>	</li><li class="li"><p>If your audio unit has a tail time other than <code>0</code> seconds, override the <code><!--a-->GetTailTime<!--/a--></code> method from the <code>AUBase</code> superclass. For example, if your audio unit produces reverberation with a maximum decay time of 3000 mS, add the following override to your audio unit custom class definition:</p><div class="codesample"><table><tr><td scope="row"><pre>virtual Float64 GetTailTime() {return 3;}<span></span></pre></td></tr></table></div>	</li></ol><a name="//apple_ref/doc/uid/TP40003278-CH12-SW9" title="Defining Custom Properties"></a><h4>Defining Custom Properties</h4><p>You can define custom audio unit properties for passing information to and from a custom view. For example, the FilterDemo project in the Core Audio SDK uses a custom property to communicate the audio unit’s frequency response to its view. This allows the view to draw the frequency response as a curve.</p><p>To define a custom property when building your audio unit from the SDK, you override the <code>GetPropertyInfo</code> and <code>GetProperty</code> methods from the <code>AUBase</code> class. Your custom view calls these methods when it needs the current value of a property of your audio unit.</p><p>You add code to the <code>GetPropertyInfo</code> method to return the size of each custom property and a flag indicating whether it is writable. You can also use this method to check that each custom property is being called with an appropriate <strong>scope</strong> and <strong>element</strong>. <span class="content_text">Listing 2-11</span> shows this method’s signature:</p><a name="//apple_ref/doc/uid/TP40003278-CH12-SW8" title="Listing 2-11The GetPropertyInfo method from the SDK&acirc;&#128;&#153;s AUBase class"></a><p class="codesample"><strong>Listing 2-11&nbsp;&nbsp;</strong>The GetPropertyInfo method from the SDK’s AUBase class</p><div class="codesample"><table><tr><td scope="row"><pre>virtual ComponentResult GetPropertyInfo (<span></span></pre></td></tr><tr><td scope="row"><pre>    AudioUnitPropertyID  inID,<span></span></pre></td></tr><tr><td scope="row"><pre>    AudioUnitScope       inScope,<span></span></pre></td></tr><tr><td scope="row"><pre>    AudioUnitElement     inElement,<span></span></pre></td></tr><tr><td scope="row"><pre>    UInt32               &amp;outDataSize,<span></span></pre></td></tr><tr><td scope="row"><pre>    Boolean              &amp;outWritable);<span></span></pre></td></tr><tr><td scope="row"><pre> <span></span></pre></td></tr></table></div>	<p>You add code to the GetProperty method to tell the view the current value of each custom property:</p><a name="//apple_ref/doc/uid/TP40003278-CH12-DontLinkElementID_78" title="Listing 2-12The GetProperty method from the SDK&acirc;&#128;&#153;s AUBase class"></a><p class="codesample"><strong>Listing 2-12&nbsp;&nbsp;</strong>The GetProperty method from the SDK’s AUBase class</p><div class="codesample"><table><tr><td scope="row"><pre>virtual ComponentResult GetProperty (<span></span></pre></td></tr><tr><td scope="row"><pre>    AudioUnitPropertyID  inID,<span></span></pre></td></tr><tr><td scope="row"><pre>    AudioUnitScope       inScope,<span></span></pre></td></tr><tr><td scope="row"><pre>    AudioUnitElement     inElement,<span></span></pre></td></tr><tr><td scope="row"><pre>    void                 *outData);<span></span></pre></td></tr></table></div>	<p>You would typically structure the <code>GetPropertyInfo</code> and <code>GetProperty</code> methods as switch statements, with one case per custom property. Look at the <code>Filter::GetPropertyInfo</code> and <code>Filter::GetProperty</code> methods in the FilterDemo project to see an example of how to use these methods.</p><p>You override the <code><a href="../../../Reference/CoreAudio/audiocodec/audiocodec.html#//apple_ref/doc/c_ref/SetProperty" target="_top">SetProperty</a></code> method to perform whatever work is required to establish new settings for each custom property.</p><p>Each audio unit property must have a unique integer ID. Apple reserves property ID numbers between <code>0</code> and <code>63999</code>. If you use custom properties, specify ID numbers of <code>64000</code> or greater.</p><a name="//apple_ref/doc/uid/TP40003278-CH12-SW14" title="Synthesis, Processing, and Data Format Conversion Code"></a><h2>Synthesis, Processing, and Data Format Conversion Code</h2><p>Audio units synthesize, process, or transform audio data. You can do anything you want here, according to the desired function of your audio unit. The digital audio code that does this is right at the heart of why you create audio units. Yet such code is largely independent of the plug-in architecture that it lives within. You’d use the same or similar algorithms and data structures for audio units or other audio plug-in architectures. For this reason, this programming guide focuses on creating audio units as containers and interfaces for audio DSP code—not on how to write the DSP code.</p><p>At the same time, the way that digital audio code fits into, and interacts with, a plug-in does vary across architectures. This section describes how audio units built with the Core Audio SDK support digital audio code. The chapter <span class="content_text"><a href="../Tutorial-BuildingASimpleEffectUnitWithAGenericView/Tutorial-BuildingASimpleEffectUnitWithAGenericView.html#//apple_ref/doc/uid/TP40003278-CH5-SW4">“Tutorial: Building a Simple Effect Unit with a Generic View”</a></span> includes some non-trivial DSP code to help illustrate how it works for effect units.</p><a name="//apple_ref/doc/uid/TP40003278-CH12-DontLinkElementID_38" title="Signal Processing"></a><h3>Signal Processing</h3><p>To perform DSP, you use an effect unit (of type <code>'aufx'</code>), typically built as a subclass of the <code>AUEffectBase</code> class. <code>AUEffectBase</code> uses a helper class to handle the DSP, <code>AUKernelBase</code>, and instantiates one kernel object (<code>AUKernelBase</code>) for each audio channel.</p><p>Kernel objects are specific to <em>n</em>-to-<em>n</em> channel effect units subclassed from the <code>AUEffectBase</code> class. They are not part of other types of audio units.</p><p>The <code>AUEffectBase</code> class is strictly for building <em>n</em>-to-<em>n</em> channel effect units. If you are building an effect unit that does not employ a direct mapping of input to output channels, you subclass the <code>AUBase</code> superclass instead.</p><p>As described in <span class="content_text"><a href="../AudioUnitDevelopmentFundamentals/AudioUnitDevelopmentFundamentals.html#//apple_ref/doc/uid/TP40003278-CH7-SW11">“Processing: The Heart of the Matter,”</a></span> there are two primary methods for audio unit DSP code: <code>Process</code> and <code>Reset</code>. You override the <code>Process</code> method to define the DSP for your audio unit. You override the <code>Reset</code> method to define the cleanup to perform when a user takes an action to end signal processing, such as moving the playback point in a sound editor window. For example, you ensure with <code>Reset</code> that a reverberation decay doesn’t interfere with the start of play at a new point in a sound file.</p><p><span class="content_text"><a href="../Tutorial-BuildingASimpleEffectUnitWithAGenericView/Tutorial-BuildingASimpleEffectUnitWithAGenericView.html#//apple_ref/doc/uid/TP40003278-CH5-SW4">“Tutorial: Building a Simple Effect Unit with a Generic View”</a></span> provides a step-by-step example of implementing a <code><!--a-->Process<!--/a--></code> method.</p><p>While an audio unit is rendering, a user can make realtime adjustments using the audio unit’s view. Processing code typically takes into account the current values of parameters and properties that are relevant to the processing. For example, the processing code for a high-pass filter effect unit would perform its calculations based on the current corner frequency as set in the audio unit’s view. The processing code gets this value by reading the appropriate parameter, as described in <span class="content_text"><a href="TheAudioUnit.html#//apple_ref/doc/uid/TP40003278-CH12-SW15">“Defining and Using Parameters.”</a></span></p><p>Audio units built using the classes in the Core Audio SDK work only with constant bit rate (CBR) audio data. When a host application reads variable bit rate (VBR) data, it converts it to a CBR representation, in the form of linear PCM, before sending it to an audio unit.</p><a name="//apple_ref/doc/uid/TP40003278-CH12-DontLinkElementID_39" title="Music Synthesis"></a><h3>Music Synthesis</h3><p>An instrument unit (of type <code>'aumu'</code>), in contrast to effect unit, renders audio in terms of notes. It acts as a virtual music synthesizer. An instrument unit employs a bank of sounds and responds to MIDI control data, typically initiated by a keyboard.</p><p>You subclass the <code>AUMonotimbralInstrumentBase</code> class for most instrument units. This class supports monophonic and polyphonic instrument units that can play one voice (also known as a patch or an instrument sound) at a time. For example, if a user chooses a piano voice, the instrument unit acts like a virtual piano, with every key pressed on a musical keyboard invoking a piano note.</p><p>The Core Audio SDK class hierarchy also provides the <code>AUMultitimbralInstrumentBase</code> class. This class supports monophonic and polyphonic instrument units that can play more than one voice at a time. For example, you could create a multimbral instrument unit that would let a user play a virtual bass guitar with their left hand while playing virtual trumpet with their right hand, using a single keyboard.</p><a name="//apple_ref/doc/uid/TP40003278-CH12-DontLinkElementID_40" title="Music Effects"></a><h3>Music Effects</h3><p>A music effect unit (of type <code>'aumf'</code>) provides DSP, like an effect unit, but also responds to MIDI data, like an instrument unit. You build a music effect unit by subclassing the <code>AUMIDIEffectBase</code> superclass from the SDK. For example, you would do this to create an audio unit that provides a filtering effect that is tuned according to the note pressed on a keyboard.</p><a name="//apple_ref/doc/uid/TP40003278-CH12-DontLinkElementID_41" title="Data Format Conversion"></a><h3>Data Format Conversion</h3><p>Audio data transformations include such operations as sample rate conversion, sending a signal to multiple destinations, and altering time or pitch. To transform audio data in ways such as these, you build a format converter unit (of type <code>'aufc'</code>) as a subclass of the <code>AUBase</code> superclass in the SDK.</p><p>Audio units are not intended to work with variable bitrate (VBR) data, so audio units are not generally suited for converting to or from lossy compression formats such as MP3. For working with lossy compression formats, use Core Audio’s Audio Converter API, declared in the <code>AudioConverter.h</code> header file in the Audio Toolbox framework.</p><a name="//apple_ref/doc/uid/TP40003278-CH12-SW2" title="Audio Unit Life Cycle"></a><h2>Audio Unit Life Cycle</h2><p>An audio unit is more than its executable code, its view, and its plug-in API. It is a dynamic, responsive entity with a complex life cycle. Here you gain a grasp of this life cycle to help you make good design and coding decisions.</p><p>Consistent with the rest of this document, this section describes audio unit life cycle in terms of development based on the Core Audio SDK. For example, this section’s discussion of object instantiation and initialization refers to SDK subclasses. If you are developing with the Audio Unit framework directly, instead of with the SDK, the audio unit class hierarchy isn’t in the picture.</p><a name="//apple_ref/doc/uid/TP40003278-CH12-DontLinkElementID_42" title="Overview"></a><h3>Overview</h3><p>The life cycle of an audio unit, as for any plug-in, consists of responding to requests. Each method that you override or write from scratch in your audio unit is called by an outside process, among them:</p><ul class="spaceabove"><li class="li"><p>The Mac OS X Component Manager, acting on behalf of a host application</p></li><li class="li"><p>A host application itself</p></li><li class="li"><p>An audio processing graph and, in particular, the downstream audio unit</p></li><li class="li"><p>The audio unit’s view, as manipulated by a user</p></li></ul><p>You don’t need to anticipate which process or context is calling your code. To the contrary, you design your audio unit to be agnostic to the calling context.</p><p>Audio unit life cycle proceeds through a series of states, which include:</p><ul class="spaceabove"><li class="li"><p><strong>Uninstantiated.</strong> In this state, there is no object instance of the audio unit, but the audio unit’s class presence on the system is registered by the Component Manager. Host applications use the Component Manager registry to find and open audio units.</p></li><li class="li"><p><strong>Instantiated but not initialized.</strong> In this state, host applications can query an audio unit object for its properties and can configure some properties. Users can manipulate the parameters of an instantiated audio unit by way of a view.</p></li><li class="li"><p><strong>Initialized.</strong> Host applications can hook up initialized audio units into audio processing graphs. Hosts and graphs can ask initialized audio units to render audio. In addition, some properties can be changed in the initialized state.</p></li><li class="li"><p><strong>Uninitialized.</strong> The Audio Unit architecture allows an audio unit to be explicitly uninitialized by a host application. The uninitialization process is not necessarily symmetrical with initialization. For example, an instrument unit can be designed to still have access, in this state, to a MIDI sound bank that it allocated upon initialization.</p></li></ul><a name="//apple_ref/doc/uid/TP40003278-CH12-DontLinkElementID_43" title="Categories of Programmatic Events"></a><h3>Categories of Programmatic Events</h3><p>Audio units respond to two main categories of programmatic events, described in detail later in this chapter:</p><ul class="spaceabove"><li class="li"><p><em>Housekeeping events</em> that the host application initiates. These include finding, opening, validating, connecting, and closing audio units. For these types of events, an audio unit built from the Core Audio SDK typically relies on code in its supplied superclasses.</p></li><li class="li"><p><em>Operational events</em> that invoke your custom code. These events, initiated by the host or by your audio unit’s view, include initialization, configuration, rendering, resetting, real-time or offline changes to the rendering, uninitialization, reinitialization, and clean-up upon closing. For some simple audio units, some operational events (especially initialization) can also rely on code from SDK superclasses.</p></li></ul><a name="//apple_ref/doc/uid/TP40003278-CH12-SW18" title="Bringing an Audio Unit to Life"></a><h3>Bringing an Audio Unit to Life</h3><p>Even before instantiation, an audio unit has a sort of ghostly presence in Mac OS X. This is because during user login, the Component Manager builds a list of available audio units. It does this without opening them. Host applications can then find and open audio units using the Component Manager.</p><p>Life begins for an audio unit when a host application asks the Component Manager to instantiate it. This typically happens when a host application launches—at which time a host typically instantiates every installed audio unit. Hosts such as AU Lab and Logic Pro do this, for example, to learn about input and output data stream formats as well as the number of inputs and outputs for each installed audio unit. Hosts typically cache this information and then close the audio units.</p><p>A host application instantiates a particular audio unit again when a user tells the host that they want to use it by picking it from a menu.</p><p>Instantiation results in invocation of the audio unit’s constructor method. To not interfere with the opening speed of host applications, it’s important to keep the constructor method lightweight and fast. The constructor is the place for defining the audio unit’s parameters and setting their initial values. It’s not the place for resource-intensive work.</p><p>An <em>n</em>-channel to <em>n</em>-channel effect unit (built from the <code>AUEffectBase</code> class) doesn’t instantiate its kernel object or objects until the audio unit is initialized. For this reason, and for this type of audio unit, it’s appropriate to perform resource-intensive work, such as setting up wave tables, during kernel instantiation. For more on this, see <span class="content_text"><a href="TheAudioUnit.html#//apple_ref/doc/uid/TP40003278-CH12-SW5">“Kernel Instantiation in n-to-n Effect Units.”</a></span></p><p>Most properties should be implemented and configured in the constructor as well, as described in the next section.</p><a name="//apple_ref/doc/uid/TP40003278-CH12-DontLinkElementID_44" title="Property Configuration"></a><h3>Property Configuration</h3><p>When possible, an audio unit should configure its properties in its constructor method. However, audio unit properties can be configured at a variety of times and by a variety of entities. Each individual property is usually configured in one of the following ways:</p><ul class="spaceabove"><li class="li"><p>By the audio unit itself, typically during instantiation</p></li><li class="li"><p>By the application hosting the audio unit, before or after audio unit initialization</p></li><li class="li"><p>By the audio unit’s view, as manipulated by a user, when the audio unit is initialized or uninitialized</p></li></ul><p>This variability in configuring audio unit properties derives from the requirements of the various properties, the type of the audio unit, and the needs of the host application.</p><p>For some properties, the SDK superclasses define whether configuration can take place while an audio unit is initialized or only when it is uninitialized. For example, a host application cannot change an audio unit’s stream format (using the <code>kAudioUnitProperty_StreamFormat</code> property) unless it ensures that the audio unit is uninitialized.</p><p>For other properties, such as the <code>kAudioUnitProperty_SetRenderCallback</code> property, the audio unit specification prohibits hosts from changing the property on an initialized audio unit but there is no programmatic enforcement against it.</p><p>For yet other properties, such as the <code>kAudioUnitProperty_OfflineRender</code> property, it is up to the audio unit to determine whether to require uninitialization before changing the property value. If the audio unit can handle the change gracefully while initialized, it can allow it.</p><p>The audio unit specification details the configuration requirements for each Apple defined property.</p><a name="//apple_ref/doc/uid/TP40003278-CH12-SW3" title="Audio Unit Initialization and Uninitialization"></a><h3>Audio Unit Initialization and Uninitialization</h3><p>The place for time-intensive and resource-intensive audio unit startup operations is in an audio unit’s initialization method. The idea is to postpone as much work in your audio unit as possible until it is about to be used. For example, the AU Lab application doesn’t initialize an audio unit installed on the system until the user specifically adds the audio unit to an AU Lab channel. This strategy improves user experience by minimizing untoward delays on host application startup, especially for users who have large numbers of audio units installed.</p><p>Here are some examples of work that’s appropriate for initialization time:</p><ul class="spaceabove"><li class="li"><p>An instrument unit acquires a MIDI sound bank for the unit to use when responding to MIDI data</p></li><li class="li"><p>An effect unit allocates memory buffers for use during rendering</p></li><li class="li"><p>An effect unit calculates wave tables for use during rendering</p></li></ul><p>Generally speaking, each of these operations should be performed in an override of the <code><a href="../../../Reference/CoreAudio/audiocodec/audiocodec.html#//apple_ref/doc/c_ref/Initialize" target="_top">Initialize</a></code> method from the <code>AUBase</code> class.</p><p>If you define an override of the <code><a href="../../../Reference/CoreAudio/audiocodec/audiocodec.html#//apple_ref/doc/c_ref/Initialize" target="_top">Initialize</a></code> method for an effect unit, begin it with a call to <code><!--a-->AUEffectBase::Initialize<!--/a--></code>. This will ensure that housekeeping tasks, like proper channel setup, are taken care of for your audio unit.</p><p>If you are setting up internal buffers for processing, you can find out how large to make them by calling the <code><!--a-->AUBase::GetMaxFramesPerSlice<!--/a--></code> method. This accesses a value that your audio unit’s host application defines before it invokes initialization. The actual number of frames per render call can vary. It is set by the host application by using the <code>inFramesToProcess</code> parameter of the <code><!--a-->AUEffectBase::Process<!--/a--></code> or <code><!--a-->AUBase::DoRender<!--/a--></code> methods.</p><p>Initialization is also the appropriate time to invoke an audio unit’s copy protection. Copy protection can include such things as a password challenge or checking for the presence of a hardware dongle.</p><p>The audio unit class hierarchy in the Core Audio SDK provides specialized <code><a href="../../../Reference/CoreAudio/audiocodec/audiocodec.html#//apple_ref/doc/c_ref/Initialize" target="_top">Initialize</a></code> methods for the various types of audio units. Effect units, for example, use the <code><a href="../../../Reference/CoreAudio/audiocodec/audiocodec.html#//apple_ref/doc/c_ref/Initialize" target="_top">Initialize</a></code> method in the <code>AUEffectBase</code> class. This method performs a number of important housekeeping tasks, including:</p><ul class="spaceabove"><li class="li"><p>Protecting the effect unit against a host application which attempts to connect it up in ways that won’t work</p></li><li class="li"><p>Determining the number of input and output channels supported by the effect unit, as well as the channel configuration to be used for the current initialization.</p><p>(Effect units can be designed to support a variable number of input and output channels, and the number used can change from one initialization to the next.)</p></li><li class="li"><p>Setting up or updating the kernel objects for the effect unit, ensuring they are ready to do their work</p></li></ul><p>In many cases, such as in the effect unit you’ll create in <span class="content_text"><a href="../Tutorial-BuildingASimpleEffectUnitWithAGenericView/Tutorial-BuildingASimpleEffectUnitWithAGenericView.html#//apple_ref/doc/uid/TP40003278-CH5-SW4">“Tutorial: Building a Simple Effect Unit with a Generic View,”</a></span> effect units don’t need additional initialization work in the audio unit’s class. They can simply use the <code>Initialize</code> method from <code>AUBase</code> as is, by inheritance. The effect unit you’ll build in the tutorial does this.</p><p>In the specific case of an effect unit based on the <code>AUEffectBase</code> superclass, you can put resource-intensive initialization code into the constructor for the DSP kernel object. This works because kernels are instantiated during effect unit initialization. The example effect unit that you build later in this document describes this part of an effect unit’s life cycle.</p><p>Once instantiated, a host application can initialize and uninitialize an audio unit repeatedly, as appropriate for what the user wants to do. For example, if a user wants to change sampling rate, the host application can do so without first closing the audio unit. (Some other audio plug-in technologies do not offer this feature.)</p><a name="//apple_ref/doc/uid/TP40003278-CH12-SW5" title="Kernel Instantiation in n-to-n Effect Units"></a><h3>Kernel Instantiation in n-to-n Effect Units</h3><p>In an effect unit built using the <code>AUEffectBase</code> superclass—such as the tremolo unit you build in the effect unit tutorial—processing takes place in one or more so-called kernel objects. These objects are subclassed from the <code>AUKernelBase</code> class, as described in <span class="content_text"><a href="TheAudioUnit.html#//apple_ref/doc/uid/TP40003278-CH12-SW18">“Bringing an Audio Unit to Life.”</a></span></p><p>Such effect units instantiate one kernel object for each channel being used. Kernel object instantiation takes place during the audio unit initialization, as part of this sequence:</p><ol class="ol"><li class="li"><p>An <em>n</em>-channel to <em>n</em>-channel effect unit gets instantiated</p></li><li class="li"><p>The effect unit gets initialized</p></li><li class="li"><p>During initialization, the effect unit instantiates an appropriate number of kernel objects</p></li></ol><p>This sequence of events makes the kernel object constructor a good place for code that you want invoked during audio unit initialization. For example, the tremolo unit in this document’s tutorial builds its tremolo wave tables during kernel instantiation.</p><a name="//apple_ref/doc/uid/TP40003278-CH12-SW6" title="Audio Processing Graph Interactions"></a><h3>Audio Processing Graph Interactions</h3><p></p><p></p><p>Audio processing graphs—hookups of multiple audio units—are the most common way that your audio unit will be used. This section describes what happens in a graph.</p><p>Interactions of an audio unit with an audio processing graph include:</p><ul class="spaceabove"><li class="li"><p>Establishing input and output connections</p></li><li class="li"><p>Breaking input and output connections</p></li><li class="li"><p>Responding to rendering requests</p></li></ul><p>A host application is responsible for making and breaking connections for an audio processing graph. Performing connection and disconnection takes place by way of setting properties, as discussed earlier in this chapter in <span class="content_text"><a href="TheAudioUnit.html#//apple_ref/doc/uid/TP40003278-CH12-SW17">“Audio Processing Graph Connections.”</a></span> For an audio unit to be added to or removed from a graph, it must be uninitialized.</p><p>Audio data flow in graphs proceeds according to a pull model, as described in <span class="content_text"><a href="../AudioUnitDevelopmentFundamentals/AudioUnitDevelopmentFundamentals.html#//apple_ref/doc/uid/TP40003278-CH7-SW12">“Audio Processing Graphs and the Pull Model.”</a></span></p><!-- <Para>Signal flow in audio processing graphs proceeds from the first (input) to last (output) node, but is invoked from the last node back to the first. This so-called “pull model“ of signal flow for audio units proceeds as follows:</Para><List-Number><Item><Para>The host application asks the audio processing graph to start rendering</Para></Item><Item><Para>The graph starts its final audio unit—which is the default output unit or some other output unit</Para></Item><Item><Para>The final audio unit starts the audio unit(s) connected to its input(s)</Para></Item><Item><Para>The start signal proceeds upstream, audio unit to audio unit, back to the first audio unit in the graph</Para></Item><Item><Para>The first audio unit in the graph reads the audio data in its input buffer and processes it</Para></Item></List-Number><Para>At this point, audio data begins flowing from the first audio unit through to the last.</Para><Para>Except for an audio processing graph’s output unit, the rendering request for each audio unit comes from the audio unit connected to its output. In the case of the output audio unit, the rendering request comes from the host application by way of the audio processing graph object.</Para><Para><query>Previously in the Fundamentals chapter:</query>Here’s a typical processing sequence for the common case of a host working with an audio processing graph:</Para><List-Bullet><Item><Para>The host application sets up an audio processing graph, and sets up a render callback to feed audio data to the first audio unit in the graph.</Para></Item><Item><Para>The host invokes a rendering call to the final audio unit in the graph, asking for a batch of processed audio samples—the host determines how many samples to ask for. This rendering call begins the so-called “pull” on the graph.</Para></Item><Item><Para>At this point, the final audio unit in the graph has no audio data to hand off to the host. The audio unit then asks its upstream neighbor for audio data. The pull request proceeds up the graph to the first audio unit.</Para></Item><Item><Para>The first audio unit in the graph, needing audio data, calls the host’s render callback. The audio unit then processes a batch of samples and feeds the result to its downstream neighbor.</Para></Item><Item><Para>Each audio unit in the graph, upon getting a batch of audio data, processes it and makes it available to its downstream audio unit.</Para></Item><Item><Para>The final audio unit in the graph, finally receiving some samples, provides the result of its processing to the host application.</Para></Item></List-Bullet><Para>This pull continues as long as the host keeps asking the audio processing graph for samples.</Para><Para><query>From the Audio Unit Programmatic Structure and Life Cycle section in the Fundamentals chapter:</query>Here is how the audio data flow proceeds between a host application and an audio unit:</Para><List-Number><Item><Para>The host invokes the audio unit’s render method, effectively asking the audio unit for a slice of processed audio data</Para></Item><Item><Para>The audio unit responds by calling the host’s callback function to get a slice of audio data samples to process</Para></Item><Item><Para>The audio unit processes the audio data samples and places the result in an output buffer for the host to retrieve</Para></Item><Item><Para>The host retrieves the processed data and then again invokes the audio unit’s render method</Para></Item></List-Number><Para><query>Yet another redundant description:</query>Processing begins with a “pull” from a host application or a downstream audio unit. This pull consists of a call to the <functionName>AudioUnitRender</functionName> function, which is passed on to the audio unit’s <functionName>Process</functionName> method.</Para><Para>Then, the audio unit, needing audio data to process, makes a call to the render callback function belonging to its upstream neighbor. If the audio unit is at the start of a processing graph, or if a host is using the audio unit directly without a graph, it is the host’s render callback function that the audio unit calls. If the audio unit is in the middle of a graph, it calls the render callback function belonging to its upstream neighbor.</Para> --><a name="//apple_ref/doc/uid/TP40003278-CH12-DontLinkElementID_45" title="Audio Unit Processing"></a><h3>Audio Unit Processing</h3><p></p><p>Depending on its type, an audio unit does one of the following:</p><ul class="spaceabove"><li class="li"><p>Processes audio (for example, effect units and music effect units)</p></li><li class="li"><p>Generates audio from MIDI data (instrument units) or otherwise, such as by reading a file (generator units)</p></li><li class="li"><p>Transforms audio data (format converter units) such as by changing sample rate, bit depth, encoding scheme, or some other audio data characteristic</p></li></ul><p>In effect units built using the Core Audio SDK, the processing work takes place in a C++ method called <code><!--a-->Process<!--/a--></code>. This method, from the <code>AUKernelBase</code> class, is declared in the <code>AUEffectBase.h</code> header file in the SDK. In instrument units built using the SDK, the audio generation work takes place in a method called <code><!--a-->Render<!--/a--></code>, defined in the <code>AUInstrumentBase</code> class.</p><p></p><p>In an effect unit, processing starts when the unit receives a call to its <code>Process</code> method. This call typically comes from the downstream audio unit in an audio processing graph. As described in <span class="content_text"><a href="TheAudioUnit.html#//apple_ref/doc/uid/TP40003278-CH12-SW6">“Audio Processing Graph Interactions,”</a></span> the call is the result of a cascade originating from the host application, by way of the graph object, asking the final node in the graph to start.</p><p>The processing call that the audio unit receives specifies the input and output buffers as well as the amount of data to process:</p><a name="//apple_ref/doc/uid/TP40003278-CH12-SW7" title="Listing 2-13The Process method from the AUKernelBase class"></a><p class="codesample"><strong>Listing 2-13&nbsp;&nbsp;</strong>The Process method from the AUKernelBase class</p><div class="codesample"><table><tr><td scope="row"><pre>virtual void Process (<span></span></pre></td></tr><tr><td scope="row"><pre>    const Float32    *inSourceP,<span></span></pre></td></tr><tr><td scope="row"><pre>    Float32          *inDestP,<span></span></pre></td></tr><tr><td scope="row"><pre>    UInt32           inFramesToProcess,<span></span></pre></td></tr><tr><td scope="row"><pre>    UInt32           inNumChannels,<span></span></pre></td></tr><tr><td scope="row"><pre>    bool &amp;           ioSilence) = 0;<span></span></pre></td></tr></table></div>	<p>For an example implementation of the Process method, see <span class="content_text"><a href="../Tutorial-BuildingASimpleEffectUnitWithAGenericView/Tutorial-BuildingASimpleEffectUnitWithAGenericView.html#//apple_ref/doc/uid/TP40003278-CH5-SW4">“Tutorial: Building a Simple Effect Unit with a Generic View.”</a></span></p><p>Processing is the most computationally expensive part of an audio unit’s life cycle. Within the processing loop, avoid the following actions:</p><ul class="spaceabove"><li class="li"><p>Mutually exclusive (mutex) resource locking</p></li><li class="li"><p>Memory or resource allocation</p></li></ul><div class="notebox"><a name="//apple_ref/doc/uid/TP40003278-CH12-DontLinkElementID_79" title="Note"></a><p><strong>Note:</strong>&nbsp;Some Core Foudation calls, such as CFRetain and CFRelease, employ mutex locks. For this reason, it’s best to avoid Core Foundation calls during processing.</p></div><a name="//apple_ref/doc/uid/TP40003278-CH12-SW4" title="Closing"></a><h3>Closing</h3><p></p><p>When a host is finished using an audio unit, it should close it by calling the Component Manager’s <code>CloseComponent</code> function. This function invokes the audio unit’s destructor method. Audio units themselves must take care of freeing any resources they have allocated.</p><p>If you’re using copy protection in your audio unit, you should end it only on object destruction. </p>

        <br /><br /> 
        
        <div class="mini_nav_text" align="left">
        <span class="navButtons">
        <a href="../AudioUnitDevelopmentFundamentals/AudioUnitDevelopmentFundamentals.html">&lt; Previous Page</a><span style="margin-left: 8px"><a href="../TheAudioUnitView/TheAudioUnitView.html">Next Page &gt;</a></span>
        </span>
        <span id="showHideTOCLowerSpan">
        <a href="#" onclick="showHideTOC();"><img src="../../../../Resources/Images/show_toc_icon.gif" width="15" height="14" border="0" style="margin-bottom: -2px;" alt="" /></a> <a href="#" onclick="showHideTOC();">Hide TOC</a>
        </span>
        </div>

        <br/><hr /><div align="center"><p class="content_text" lang="en" dir="ltr"> <!--#if expr="0=1" -->&#x00a9; 2007 Apple Inc. All Rights Reserved. &#40;<!--#endif -->Last updated: 2007-10-31<!--#if expr="0=1" -->&#041;<!--#endif --></p></div>

        
        <div class="hideOnPrint hideInXcode">
        <!-- start of footer -->
        	<table width="100%" border="0" cellpadding="0" cellspacing="0">
		<tr>
			<td><div style="width: 100%; height: 1px; background-color: #919699; margin-top: 5px; margin-bottom: 15px"></div></td>
		</tr>
		<tr>
			<td align="center"><br/>
				<table border="0" cellpadding="0" cellspacing="0" class="graybox">
					<tr>
						<th>Did this document help you?</th>
					</tr>
					<tr>
						<td>
						    <div style="margin-bottom: 8px"><a href="http://developer.apple.com/feedback/?v=1&url=/documentation/MusicAudio/Conceptual/AudioUnitProgrammingGuide/TheAudioUnit/TheAudioUnit.html%3Fid%3DTP40003278-1.2&media=dvd" target=_new>Yes</a>:  Tell us what works for you.</div>
							<div style="margin-bottom: 8px"><a href="http://developer.apple.com/feedback/?v=2&url=/documentation/MusicAudio/Conceptual/AudioUnitProgrammingGuide/TheAudioUnit/TheAudioUnit.html%3Fid%3DTP40003278-1.2&media=dvd" target=_new>It&#8217;s good, but:</a> Report typos, inaccuracies, and so forth.</div>
							<div><a href="http://developer.apple.com/feedback/?v=3&url=/documentation/MusicAudio/Conceptual/AudioUnitProgrammingGuide/TheAudioUnit/TheAudioUnit.html%3Fid%3DTP40003278-1.2&media=dvd" target=_new>It wasn&#8217;t helpful</a>: Tell us what would have helped.</div>
						</td>
					</tr>
				</table>
			</td>
		</tr>
	</table>

        <!--#include virtual="/includes/framesetfooter" -->
        <!-- end of footer -->
        </div>
    </div>
</body>
</html>