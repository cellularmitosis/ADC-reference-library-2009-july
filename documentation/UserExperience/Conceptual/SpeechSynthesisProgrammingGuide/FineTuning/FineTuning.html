<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
	<title>Speech Synthesis Programming Guide: Techniques for Customizing Synthesized Speech</title>
	<meta id="Generator" name="Generator" content="Gutenberg"/>
	<meta id="GeneratorVersion" name="GeneratorVersion" content="v132"/>
	<meta http-equiv="content-type" content="text/html;charset=utf-8"/>
	<meta id="Copyright" name="Copyright" content="Copyright 2009 Apple Inc. All Rights Reserved."/>
	<meta id="IndexTitle" name="IndexTitle" content="Techniques for Customizing Synthesized Speech"/>
	<meta id="xcode-display" name="xcode-display" content="render"/>
	<meta id="toc-file" name="toc-file" content="../toc.html"/>
	<meta id="RESOURCES" content="../../../../Resources" />
	
	<link rel="stylesheet" type="text/css" href="../../../../Resources/CSS/frameset_styles.css"/>
	<script language="JavaScript" type="text/javascript" src="../../../../Resources/JavaScript/lib/prototype.js"></script>
	<script language="JavaScript" type="text/javascript" src="../../../../Resources/JavaScript/lib/scriptaculous.js"></script>
	<script language="JavaScript" type="text/javascript" src="../../../../Resources/JavaScript/page.js"></script>
	<script language="JavaScript" type="text/javascript" src="../../../../Resources/JavaScript/pedia.js"></script>
	<!--[if lte IE 6]>
		<style type="text/css">
			/*<![CDATA[*/ 
			html {overflow-x:auto; overflow-y:hidden;  }
			/*]]>*/
		</style>
	<![endif]-->
</head>    
<body bgcolor="#ffffff" onload="initialize_page();"><a name="//apple_ref/doc/uid/TP40004365-CH5" title="Techniques for Customizing Synthesized Speech"></a>
    <noscript>
    <div id="tocMenu">
        <iframe id="toc_content" name="toc_content" SRC="../toc.html" width="210" height="100%" align="left" frameborder="0">This document set is best viewed in a browser that supports iFrames.</iframe>
    </div>
    </noscript>
    <div id="bodyText">
        <a name="top"></a>
        <div class="hideOnPrint hideInXcode">
        <!-- start of header -->
        <!--#include virtual="/includes/framesetheader" -->
        <!-- end of header -->
        </div>
        
        <!-- start of path -->
<div class="breadcrumb hideOnPrint hideInXcode"><a href="http://developer.apple.com/" target="_top">ADC Home</a> &gt; <a href="../../../../../referencelibrary/index.html#//apple_ref/doc/uid/TP30000943" target="_top">Reference Library</a> &gt; <a href="../../../../index.html#//apple_ref/doc/uid/TP30000440" target="_top">Guides</a> &gt; <a href="../../../index.html#//apple_ref/doc/uid/TP30000440-TP30000437" target="_top">User Experience</a> &gt; <a href="../../../SpeechTechnologies-date.html#//apple_ref/doc/uid/TP30000440-TP30000437-TP30000576" target="_top">Speech Technologies</a> &gt; <a href="../Introduction/Introduction.html#//apple_ref/doc/uid/TP40004365-CH1-DontLinkElementID_41">Speech Synthesis Programming Guide</a> &gt; </div><br class="hideInXcode"/><!-- end of path -->
        
        <div class="mini_nav_text" align="left">
        <span class="navButtons">
        <a href="../UsingSpeech/UsingSpeech.html">&lt; Previous Page</a><span style="margin-left: 8px"><a href="../CommandSyntax/CommandSyntax.html">Next Page &gt;</a></span>
        </span>
        <span id="showHideTOCUpperSpan">
        <a href="#" onclick="showHideTOC();"><img src="../../../../Resources/Images/show_toc_icon.gif" width="15" height="14" border="0" style="margin-bottom: -2px;" alt="" hideText="Hide TOC" showText="Show TOC" /></a> <a href="#" onclick="showHideTOC();">Hide TOC</a>
        </span>
        </div>

        <hr />
        
        
        <a name="//apple_ref/doc/uid/TP40004365-CH5-SW3" title="Techniques for Customizing Synthesized Speech"></a><h1>Techniques for Customizing Synthesized Speech</h1><p>This chapter describes how to fine-tune the speech your application generates. It provides guidelines for using speech synthesis APIs and embedded speech commands and it includes a number of examples of specific tasks. This chapter also describes several ways you can improve your application’s spoken output. If you need fine-grained control over the generated speech in your application, you should read this chapter to learn how to take advantage of the advanced features of the Speech Synthesis framework.</p><p>Some of the advanced techniques described in this chapter are supported only by the Carbon speech synthesis API, although any application that has access to the Application Services framework can use them. If you haven’t decided which API to use, you should read <span class="content_text"><a href="../UsingSpeech/UsingSpeech.html#//apple_ref/doc/uid/TP40004365-CH4-SW1">“Carbon and Cocoa Speech Synthesis APIs Compared”</a></span> to find out which API supports the level of customization you want to implement. Other techniques described in this chapter involve the use of embedded speech commands and other text modifiers, which are available to all applications.</p>
<!-- This template is being used for both PDF and HTML. -->

    
    <h4>In this section:</h4>
    
    
    <p class="blockquote">
    
        
			
			
				<a href="FineTuning.html#//apple_ref/doc/uid/TP40004365-CH5-SW13">Adjust Speech Channel Settings Using the Carbon Speech Synthesis API</a>
				
			<br/>
			
        
			
			
				<a href="FineTuning.html#//apple_ref/doc/uid/TP40004365-CH5-SW10">Use Embedded Speech Commands to Fine-Tune Spoken Output</a>
				
			<br/>
			
        
			
			
				<a href="FineTuning.html#//apple_ref/doc/uid/TP40004365-CH5-SW4">Use Phoneme Modifiers to Adjust Pronunciation</a>
				
			<br/>
			
        
			
			
				<a href="FineTuning.html#//apple_ref/doc/uid/TP40004365-CH5-DontLinkElementID_2">Use Punctuation Correctly</a>
				
			<br/>
			
        
			
			
				<a href="FineTuning.html#//apple_ref/doc/uid/TP40004365-CH5-SW7">Use the TUNE Format to Supply Complex Pitch Contours</a>
				
			<br/>
			
        
			
			
				<a href="FineTuning.html#//apple_ref/doc/uid/TP40004365-CH5-SW9">Synchronize Speech with Application-Specific Actions</a>
				
			<br/>
			
        
			
			
				<a href="FineTuning.html#//apple_ref/doc/uid/TP40004365-CH5-SW1">Avoid Cross-Talk</a>
				
			<br/>
			
        
			
			
				<a href="FineTuning.html#//apple_ref/doc/uid/TP40004365-CH5-SW2">Four Ways to Improve Spoken Output</a>
				
			<br/>
			
        

    </p><br/>

<a name="//apple_ref/doc/uid/TP40004365-CH5-SW13" title="Adjust Speech Channel Settings Using the Carbon Speech Synthesis API"></a><h2>Adjust Speech Channel Settings Using the Carbon Speech Synthesis API</h2><p>The Carbon speech synthesis API allows you to get and set speech attributes, such as rate and volume, and specify other settings on speech channels, such as input mode. In addition to a couple of functions that focus on specific attributes, the Carbon speech synthesis API defines the <code><a href="../../../../Carbon/Reference/Speech_Synthesis_Manager/Reference/reference.html#//apple_ref/c/func/GetSpeechInfo" target="_top">GetSpeechInfo</a></code> and <code><a href="../../../../Carbon/Reference/Speech_Synthesis_Manager/Reference/reference.html#//apple_ref/c/func/SetSpeechInfo" target="_top">SetSpeechInfo</a></code> functions, which act upon a speech channel using an attribute, setting, or other value specified in a selector parameter. This section describes how you can use the attribute-specific functions and the <code>SetSpeechInfo</code> function to adjust the speech attributes and other settings on speech channels.</p><p>The Carbon speech synthesis API defines the following functions to get and set the rate and pitch attributes on a speech channel:</p><ul class="simple"><li><p><code><a href="../../../../Carbon/Reference/Speech_Synthesis_Manager/Reference/reference.html#//apple_ref/c/func/GetSpeechRate" target="_top">GetSpeechRate</a></code></p></li><li><p><code><a href="../../../../Carbon/Reference/Speech_Synthesis_Manager/Reference/reference.html#//apple_ref/c/func/SetSpeechRate" target="_top">SetSpeechRate</a></code></p></li><li><p><code><a href="../../../../Carbon/Reference/Speech_Synthesis_Manager/Reference/reference.html#//apple_ref/c/func/GetSpeechPitch" target="_top">GetSpeechPitch</a></code></p></li><li><p><code><a href="../../../../Carbon/Reference/Speech_Synthesis_Manager/Reference/reference.html#//apple_ref/c/func/SetSpeechPitch" target="_top">SetSpeechPitch</a></code></p></li></ul><p>For example, an application might get a new speech rate value from the user and use it to change the speech rate used by the speech channel, as shown below:</p><div class="codesample"><table><tr><td scope="row"><pre>/* fRateField is associated with a button in the UI and fCurSpeechChannel is a pointer to a speech channel structure created earlier in the application. */<span></span></pre></td></tr><tr><td scope="row"><pre>Fixed theNewValue   = [fRateField doubleValue] * 65536.0;<span></span></pre></td></tr><tr><td scope="row"><pre>theErr = SetSpeechRate(fCurSpeechChannel, theNewValue);<span></span></pre></td></tr></table></div><p>In a similar way, an application can use the <code>SetSpeechPitch</code> function to set a speech channel’s pitch attribute to a new value. To get or set other speech attributes and settings on a speech channel, however, you use the <code>GetSpeechInfo</code> and <code>SetSpeechInfo</code> functions with the appropriate selectors. The one exception to this is the rate attribute, which can be retrieved and set using either the <code>GetSpeechRate</code> and <code>SetSpeechRate</code> functions mentioned above or the <code>SetSpeechInfo</code> function with the <code>soRate</code> selector, as shown below:</p><div class="codesample"><table><tr><td scope="row"><pre>/* As above, fRateField is associated with a button in the UI and fCurSpeechChannel is a pointer to a speech channel structure created earlier in the application. */<span></span></pre></td></tr><tr><td scope="row"><pre>Fixed theNewValue   = [fRateField doubleValue] * 65536.0;<span></span></pre></td></tr><tr><td scope="row"><pre>theErr = SetSpeechInfo(fCurSpeechChannel, soRate, &amp;theNewValue);<span></span></pre></td></tr></table></div><p></p><p>The selectors defined in the Carbon speech synthesis API act upon a wide range of properties associated with speech channels. The selectors divide into the following categories:</p><ul class="ul"><li class="li"><p>Attribute selectors, which specify a speech attribute, such as volume or rate, that the speech channel should use when generating speech.</p><p>All attribute selectors work with both the <code>GetSpeechInfo</code> and <code>SetSpeechInfo</code> functions. As an alternative to using an attribute selector with the <code>SetSpeechInfo</code> function, you can use an equivalent embedded speech command to set a speech attribute (see <span class="content_text"><a href="FineTuning.html#//apple_ref/doc/uid/TP40004365-CH5-SW6">Table 3-1</a></span> for more about these commands). The currently available selectors in this category are:</p><ul class="nested"><li class="nested li"><p><code>soRate</code></p></li><li class="nested li"><p><code>soPitchBase</code></p></li><li class="nested li"><p><code>soPitchMod</code></p></li><li class="nested li"><p><code>soVolume</code></p></li></ul></li><li class="li"><p>Input-mode selectors, which specify the mode, such as phoneme or character, that the speech channel should use when processing text.</p><p>All input-mode selectors work with both the <code>GetSpeechInfo</code> and <code>SetSpeechInfo</code> functions. As an alternative to using a mode selector with the <code>SetSpeechInfo</code> function, you can use an equivalent embedded speech command to specify an input-processing mode (see <span class="content_text"><a href="FineTuning.html#//apple_ref/doc/uid/TP40004365-CH5-SW6">Table 3-1</a></span> for more about these commands). The currently available selectors in this category are:</p><ul class="nested"><li class="nested li"><p><code>soInputMode</code></p></li><li class="nested li"><p><code>soCharacterMode</code></p></li><li class="nested li"><p><code>soNumberMode</code></p></li></ul></li><li class="li"><p>Callback selectors, which associate with a speech channel an application-defined callback function to be called when a particular speech event occurs, such as the conclusion of speech.</p><p>All callback selectors work with only the <code>SetSpeechInfo</code> function. One callback selector, <code>soSyncCallBack</code>, has a related embedded speech command you can use to trigger a callback to your synchronization callback procedure (see <span class="content_text"><a href="FineTuning.html#//apple_ref/doc/uid/TP40004365-CH5-SW6">Table 3-1</a></span> for more about this command). The currently available selectors in this category are:</p><ul class="nested"><li class="nested li"><p><code>soTextDoneCallBack</code></p></li><li class="nested li"><p><code>soSpeechDoneCallBack</code></p></li><li class="nested li"><p><code>soSyncCallBack</code></p></li><li class="nested li"><p><code>soErrorCallBack</code></p></li><li class="nested li"><p><code>soPhonemeCallBack</code></p></li><li class="nested li"><p><code>soWordCallBack</code></p></li></ul></li><li class="li"><p>Information selectors, which provide information about the current speech channel and the synthesizer associated with it, such as the set of phoneme symbols recognized by the synthesizer.</p><p>All information selectors work only with the <code>GetSpeechInfo</code> function. There are no embedded speech commands equivalent to the information selectors. The currently available selectors in this category are:</p><ul class="nested"><li class="nested li"><p><code>soStatus</code></p></li><li class="nested li"><p><code>soErrors</code></p></li><li class="nested li"><p><code>soSynthType</code></p></li><li class="nested li"><p><code>soRecentSync</code></p></li><li class="nested li"><p><code>soPhonemeSymbols</code></p></li></ul></li><li class="li"><p>Setting selectors, which specify a speech channel setting, such as the current voice.</p><p>Some setting selectors work with both the <code>GetSpeechInfo</code> and <code>SetSpeechInfo</code> functions, others with only the <code>SetSpeechInfo</code> function. One setting selector, <code>soCommandDelimiter</code>, has an equivalent embedded speech command you can use to change the default command delimiter strings (see <span class="content_text"><a href="FineTuning.html#//apple_ref/doc/uid/TP40004365-CH5-SW6">Table 3-1</a></span> for more about this command). The currently available selectors in this category are:</p><ul class="nested"><li class="nested li"><p><code>soCurrentVoice</code></p></li><li class="nested li"><p><code>soCommandDelimiter</code></p></li><li class="nested li"><p><code>soReset</code></p></li><li class="nested li"><p><code>soRefCon</code></p></li><li class="nested li"><p><code>soSynthExtension</code></p></li><li class="nested li"><p><code>soOutputToFileWithCFURL</code></p></li></ul></li></ul><a name="//apple_ref/doc/uid/TP40004365-CH5-SW10" title="Use Embedded Speech Commands to Fine-Tune Spoken Output"></a><h2>Use Embedded Speech Commands to Fine-Tune Spoken Output</h2><p>As described in <span class="content_text"><a href="../SpeechOverview/SpeechOverview.html#//apple_ref/doc/uid/TP40004365-CH3-SW5">“Control Speech Quality Using Embedded Commands,”</a></span> you use embedded commands to fine-tune the pronunciation of individual words in the text your application passes to a synthesizer. Even if you use only a few of the embedded speech commands described in this section, you may significantly increase the understandability of your application’s spoken output. This section provides an overview of embedded speech command syntax, lists the available commands, and illustrates how to use them to achieve different effects.</p><p>Note that some embedded speech commands have functional equivalents provided by the Carbon selector mechanism (for a complete list of available selectors, see  <em><a href="../../../../Carbon/Reference/Speech_Synthesis_Manager/index.html#//apple_ref/doc/uid/TP30000211" target="_top">Speech Synthesis Manager Reference</a></em>.) This means that to achieve some effects, you can either insert the embedded command in the text, or you can pass the equivalent selector to the Carbon <code><a href="../../../../Carbon/Reference/Speech_Synthesis_Manager/Reference/reference.html#//apple_ref/doc/c_ref/SetSpeechInfo" target="_top">SetSpeechInfo</a></code> function. If you use the <code><a href="../../../../Carbon/Reference/Speech_Synthesis_Manager/Reference/reference.html#//apple_ref/doc/c_ref/SetSpeechInfo" target="_top">SetSpeechInfo</a></code> function (described in <span class="content_text"><a href="FineTuning.html#//apple_ref/doc/uid/TP40004365-CH5-SW13">“Adjust Speech Attributes Using Carbon Speech Synthesis Functions”</a></span>), the effect applies to all speech passing through the current speech channel, subject to synthesizer capabilities. If you use the embedded command to achieve the same effect, however, it applies only to the word immediately preceded by the embedded command. </p><a name="//apple_ref/doc/uid/TP40004365-CH5-DontLinkElementID_1" title="Embedded Speech Command Delimiters"></a><h3>Embedded Speech Command Delimiters</h3><p>When processing an input string or buffer, speech synthesizers look for special strings of characters called command delimiters. These character strings are usually defined to be pairings of printable characters that do not typically appear in the text. One character string is defined as the begin command delimiter and another character string is defined as the end command delimiter. When the synthesizer encounters the begin command delimiter string, it interprets the characters following it as one or more embedded commands until it reaches the end command delimiter string.</p><p>The default begin and end command delimiter strings recognized by the MacinTalk synthesizer are “[[“ and “]],“ respectively. You can change these strings if necessary, but you should take care to use printable characters that you do not expect to see in the text your application processes. Also, if you change the default delimiters, be sure to change them back to the default characters when you have finished with the text, because the change is persistent for the current speech channel. For example, if you expect square brackets to appear in the text you’ll be sending to the synthesizer, you can change the default command delimiters to strings containing other printable characters that do not naturally occur in your text. </p><p>You can disable the processing of all embedded commands by setting both the begin and end command delimiters to two NUL bytes. You might want to do this if your application speaks text over which you have no control and you’re absolutely sure the text contains no embedded commands. To disable processing of embedded commands programmatically, use the <code>soCommandDelimiter</code> selector with the <code><a href="../../../../Carbon/Reference/Speech_Synthesis_Manager/Reference/reference.html#//apple_ref/doc/c_ref/SetSpeechInfo" target="_top">SetSpeechInfo</a></code> function, as shown below:</p><div class="codesample"><table><tr><td scope="row"><pre>// Create a structure to hold the new delimiter values<span></span></pre></td></tr><tr><td scope="row"><pre>DelimiterInfo MyNewDelimiters;<span></span></pre></td></tr><tr><td scope="row"><pre>MyNewDelimiters.startDelimiter[0] = 0;<span></span></pre></td></tr><tr><td scope="row"><pre>MyNewDelimiters.startDelimiter[1] = 0;<span></span></pre></td></tr><tr><td scope="row"><pre>MyNewDelimiters.endDelimiter[0] = 0;<span></span></pre></td></tr><tr><td scope="row"><pre>MyNewDelimiters.endDelimiter[1] = 0;<span></span></pre></td></tr><tr><td scope="row"><pre>SetSpeechInfo(CurrentSpeechChannel, soCommandDelimiter, &amp;MyNewDelimiters);<span></span></pre></td></tr></table></div><a name="//apple_ref/doc/uid/TP40004365-CH5-SW12" title="Overview of Embedded Speech Command Syntax"></a><h3>Overview of Embedded Speech Command Syntax</h3><div class="notebox"><a name="//apple_ref/doc/uid/TP40004365-CH5-DontLinkElementID_23" title="Note"></a><p><strong>Note:</strong>&nbsp;This section describes enough of the embedded command syntax for you to be able to understand the examples in this document. For a formal description of the syntax of embedded speech commands and their parameters, see <span class="content_text"><a href="../CommandSyntax/CommandSyntax.html#//apple_ref/doc/uid/TP40004365-CH7-SW1">“Syntax of Embedded Speech Commands.”</a></span></p></div><p>All embedded commands consist of a 4-character command code and a parameter, enclosed by the begin and end command delimiter strings. For example, the <code>emph</code> command requires a parameter that tells the synthesizer to increase or decrease the emphasis with which to speak the next word, as shown below:</p><p><code>[[emph +]]</code> The + parameter tells the synthesizer to increase emphasis for the following word.</p><p>More than one command may occur within a single pair of delimiter strings if they are separated by semicolons, as shown below:</p><p><code>[[emph +; rate 165]]</code> Together, these commands tell the synthesizer to speak the following word or phrase with increased emphasis and at a rate of 165 words per minute.</p><p>A parameter may consist of a string, a numeric type, or an operating-system type, and may be accompanied by the + or - characters (the exact format of a parameter depends on the command with which it’s associated). Some commands allow you to use the parameter to specify either an absolute value or a relative value. For example, the <code>volm</code> command allows you to specify a particular volume or an amount by which to increase or decrease the current volume, as shown below:</p><p><code>[[volm 0.3]]</code> This command sets the volume with which the following word is spoken to 0.3.</p><p><code>[[volm +0.1]]</code> This command increases the volume with which the following word is spoken by 0.1.</p><p>The speech synthesizer ignores all whitespace within an embedded command, so you may insert as many spaces as you need to make your command text more readable.</p><p>In addition, this document uses the following characters to express the syntax of embedded speech commands (these characters do not appear in actual embedded speech commands):</p><ul class="spaceabove"><li class="li"><p>The &lt; and > characters enclose items that represent logical units, such as string, character, integer, or real value. When you insert an embedded command in your text, you replace the logical unit with an actual value. For example, you might replace "<code>&lt;RealValue></code>“ with <code>3.0</code>. For precise definitions of each logical unit, see the formal description of the syntax in <span class="content_text"><a href="../CommandSyntax/CommandSyntax.html#//apple_ref/doc/uid/TP40004365-CH7-SW1">“Syntax of Embedded Speech Commands.”</a></span></p></li><li class="li"><p>The | character means “or" and appears between members in a list of possible items, any single one of which may be used. For example, the <code>emph</code> command accepts either the + character or the - character for its parameter. Therefore, the syntax of the <code>emph</code> command is expressed as <code>emph + | -</code>.</p></li><li class="li"><p>The [ and ] characters enclose an optional item or list of items. For example, the <code>rate</code> command accepts the optional addition of the + or - character to its numerical parameter to indicate a change relative to the current value. Therefore, the syntax of the <code>rate</code> command is expressed as <code>rate [+ | -] &lt;RealValue></code>.</p></li><li class="li"><p>Items followed by an ellipsis character (...) may be repeated one or more times.</p></li></ul><a name="//apple_ref/doc/uid/TP40004365-CH5-SW11" title="The Mac OS X Embedded Speech Commands"></a><h3>The Mac OS X Embedded Speech Commands</h3><p><span class="content_text">Table 3-1</span> describes the embedded speech commands, their parameters, equivalent speech information selectors (if they exist), and in which versions of Mac OS X the commands are available. The syntax of each command in <span class="content_text">Table 3-1</span> is expressed using the conventions described in <span class="content_text"><a href="FineTuning.html#//apple_ref/doc/uid/TP40004365-CH5-SW12">“Overview of Embedded Speech Command Syntax.”</a></span> </p><div class="notebox"><a name="//apple_ref/doc/uid/TP40004365-CH5-DontLinkElementID_24" title="Note"></a><p><strong>Note:</strong>&nbsp;All embedded speech commands, except for <code>ctxt</code>, are available in Mac OS X v10.0 and later. The <code>ctxt</code> command is available in Mac OS X v10.4 and later.</p></div><a name="//apple_ref/doc/uid/TP40004365-CH5-SW6" title="Table 3-1Embedded speech commands"></a><div class="tableholder"><table class="graybox" border = "0" cellspacing="0" cellpadding="5"><caption class="tablecaption"><strong>Table 3-1&nbsp;&nbsp;</strong>Embedded speech commands</caption><tr><th scope="col" align="left" style="font-weight: bold" bgcolor="#CCCCCC"><p>Command</p></th><th scope="col" align="left" style="font-weight: bold" bgcolor="#CCCCCC"><p>Syntax and description</p></th><th scope="col" align="left" style="font-weight: bold" bgcolor="#CCCCCC"><p>Selector</p></th></tr><tr><td  scope="row"><p><code>char</code></p></td><td ><p><code>char NORM | LTRL</code></p><p>The character mode command sets the word-speaking mode of the speech channel. When the <code>NORM</code> parameter is used, the synthesizer attempts to automatically convert words into speech. This is the most basic function of the synthesizer. When the <code>LTRL</code> parameter is used, the synthesizer speaks the individual characters of every word, number, and symbol following the command (all other embedded commands are processed normally). For example, to cause the synthesizer to speak the word “cat” as “C-A-T,” you would include the following in a text buffer or string:</p><p><code>[[char LTRL]] cat [[char NORM]]</code></p></td><td ><p><code>SoCharacterMode</code></p></td></tr><tr><td  scope="row"><p><code>cmnt</code></p></td><td ><p><code>cmnt [&lt;Character>...]</code></p><p>The comment command is ignored by speech synthesizers. It enables you to add arbitrary content to the text buffer that will never be included in the spoken output. Note that the comment text itself must be included within the begin and end command delimiters of the <code>cmnt</code> command.</p><p><code>[[cmnt This is a comment that will be ignored by the synthesizer.]]</code></p></td><td ><p>None</p></td></tr><tr><td  scope="row"><p><code>ctxt</code></p></td><td ><p><code>ctxt [WSKP | WORD | NORM | TSKP | TEXT]</code></p><p>The context command allows you to identify the context of a word to help the synthesizer generate the correct pronunciation of that word, even if no other words in the surrounding phrase or sentence are spoken. Because the pronunciation of words can be different depending on the context in which they appear, you can use the context command to specify the pronunciation used in a particular context.</p><p>The context command recognizes two modes: word-by-word and text fragment. In both modes, you use the appropriate “skip” parameter (<code>WSKP</code> or <code>TSKP</code>) to identify the text that provides context and the <code>WORD</code> or <code>TEXT</code> parameter to identify the word or phrase whose pronunciation is affected by the context. The synthesizer parses the entire phrase or sentence to determine the correct pronunciation of the word or phrase, but does not speak the portions of the text marked as “skipped.“ Use the <code>[[ctxt NORM]]</code> command to signal a return to the default input-processing mode.</p><p>In word-by-word mode, the synthesizer parses the complete text selection to determine the part of speech (such as noun or verb) of the specified word. The synthesizer pronounces the word according to its part of speech, but it does not make any  intonation or duration adjustments to the pronunciation. For example, the word “coordinates” is pronounced differently depending on whether it is used as a noun or a verb. The two sentences below illustrate how to use the context command to tell the synthesizer which pronunciation of the word to use:</p><p><code>[[ctxt WSKP]] GPS provides [[ctxt WORD]] coordinates. [[ctxt NORM]]</code></p><p><code>[[ctxt WSKP]] The post office [[ctxt WORD]] coordinates [[ctxt WSKP]] its deliveries. [[ctxt NORM]]</code></p><p>In text fragment mode, the synthesizer parses the complete text selection to determine the part of speech and the intonation and duration of the specified word or phrase. For example, the different pronunciations of the phrase “first step” are informed by the context provided by the surrounding words in the following two sentences: </p><p><code>[[ctxt TSKP]] Your [[ctxt TEXT]] first step [[ctxt TSKP]] should be to relax. [[ctxt NORM]]</code></p><p><code>[[ctxt TSKP]] To relax should be your [[ctxt TEXT]] first step. [[ctxt NORM]]</code></p></td><td ><p>None</p></td></tr><tr><td  scope="row"><p><code>dlim</code></p></td><td ><p><code>dlim &lt;BeginDelimiter> &lt;EndDelimiter></code></p><p>The delimiter command changes the character sequences that indicate the beginning and end of all subsequent embedded speech commands. The new delimiters take effect after the command list containing the <code>dlim</code> command has been completely processed. If the delimiter strings are empty, an error is generated. If you want to disable embedded command processing for the remainder of the text buffer, you can pass two NUL bytes in the <code>BeginDelimiter</code> and <code>EndDelimiter</code> parameters.</p><p>[<code>[dlim $$ $$] </code></p></td><td ><p><code>soCommandDelimiter</code></p></td></tr><tr><td  scope="row"><p><code>emph</code></p></td><td ><p><code>emph + | -</code></p><p>The emphasis command causes the synthesizer to speak the next word with greater or less emphasis than it is currently using. The + parameter increases emphasis and the - parameter decreases emphasis.</p><p>For example, to emphasize the word “not” in the following phrase, use the <code>emph</code> command as follows:</p><p><code>Do [[emph +]] not [[emph -]] overtighten the screw.</code></p></td><td ><p>None</p></td></tr><tr><td  scope="row"><p><code>inpt</code></p></td><td ><p><code>inpt TEXT | PHON | TUNE</code></p><p>The input mode command switches the input-processing mode to textual mode, phoneme mode, or TUNE format mode. Note that some synthesizers may define additional speech input modes you can use. The default input-processing mode is textual, and you should always use the <code>[[inpt TEXT]]</code> command to revert to textual mode after you’re finished providing content in one of the other modes. In phoneme mode, the synthesizer interprets characters as representing phonemes (listed in <span class="content_text"><a href="../Phonemes/Phonemes.html#//apple_ref/doc/uid/TP40004365-CH9-SW1">“Phonemes”</a></span>). In the TUNE format mode, the synthesizer recognizes the same set of phonemes but also interprets additional information that specifies a precise spoken contour, or tune, for the words. For more information about the TUNE format, see <span class="content_text"><a href="FineTuning.html#//apple_ref/doc/uid/TP40004365-CH5-SW7">“Use the TUNE Format to Supply Complex Pitch Contours.”</a></span></p><p>For example, to supply the phonemic representation of a name that synthesizers frequently mispronounce, you can use the <code>inpt</code> command as follows:</p><p><code>My name is [[inpt PHON]] AY1yIY2SAX [[inpt TEXT]].</code></p></td><td ><p><code>soInputMode</code></p></td></tr><tr><td  scope="row"><p><code>nmbr</code></p></td><td ><p><code>nmbr NORM | LTRL</code></p><p>The number mode command sets the number-speaking mode of the synthesizer. The <code>NORM</code> parameter causes the synthesizer to speak the number 46 as “forty-six,” whereas the <code>LTRL</code> parameter causes the synthesizer to speak the same number as “four six.“ </p><p>For example, to make it clear that the following 7-digit number is a phone number, you can use the <code>nmbr</code> command to tell the synthesizer to say each digit separately, as follows:</p><p><code>Please call me at [[nmbr LTRL]] 5551990 [[nmbr NORM]].</code></p></td><td ><p><code>soNumberMode</code></p></td></tr><tr><td  scope="row"><p><code>pbas</code></p></td><td ><p><code>pbas [+ | -] &lt;RealValue></code></p><p>The baseline pitch command changes the current speech pitch for the speech channel to the specified real value. If the pitch value is preceded by the + or - character, the speech pitch is adjusted relative to its current value. Baseline pitch values are always positive numbers in the range of 1.000 to 127.000.</p></td><td ><p><code>soPitchBase</code></p></td></tr><tr><td  scope="row"><p><code>pmod</code></p></td><td ><p><code>pmod [+ | -] &lt;RealValue></code></p><p>The pitch modulation command changes the modulation range for the speech channel, based on the specified modulation-depth real value.</p></td><td ><p><code>soPitchMode</code></p></td></tr><tr><td  scope="row"><p><code>rate</code></p></td><td ><p><code>rate [+ | -] &lt;RealValue></code></p><p>The speech rate command sets the speech rate on the speech channel to the specified real value. Speech rates fall in the range 0.000 to 65535.999, which translates into a range of 50 to 500 words per minute. If the rate is preceded by a + or - character, the speech rate is increased or decreased relative to its current value.</p></td><td ><p><code>soRate</code></p></td></tr><tr><td  scope="row"><p><code>rset</code></p></td><td ><p><code>rset &lt;32BitValue></code></p><p>The reset command resets the speech channel’s voice and attributes to default values. The parameter has no effect; it should be set to <code>0</code>.</p></td><td ><p><code>soReset</code></p></td></tr><tr><td  scope="row"><p><code>slnc</code></p></td><td ><p><code>slnc &lt;32BitValue></code></p><p>The silence command causes the synthesizer to generate silence for the specified number of milliseconds. You might want to insert extra silence between two sentences to allow listeners to fully absorb the meaning of the first one. Note that the precise timing of the silence will vary among synthesizers. </p></td><td ><p>none</p></td></tr><tr><td  scope="row"><p><code>sync</code></p></td><td ><p><code>sync &lt;32BitValue></code></p><p>The synchronization command causes an application’s synchronization callback procedure to be executed. The callback is made as the audio corresponding to the next word begins to sound. The 32-bit value is set by the application and is passed to the callback procedure.</p><p>You can use the <code>sync</code> command to trigger a callback at times other than those defined by the built-in callbacks (such as the phoneme and speech-done callbacks). For example, you might want to perform some custom processing each time a date is spoken to highlight its place on a graphical timeline. To do this, you would define a synchronization callback procedure and refcon values, and insert a <code>sync</code> command after each date in the text, as follows: </p><p><code>In 1066 [[sync 0x000000A1]], William the Conqueror invaded England and by 1072 [[sync 0x000000A2]], the whole of England was conquered and united.</code></p></td><td ><p><code>soSyncCallback</code></p></td></tr><tr><td  scope="row"><p><code>vers</code></p></td><td ><p><code>vers &lt;32BitValue></code></p><p>The format version command tells the speech synthesizer which embedded command format version will be used by all subsequent embedded speech commands.</p></td><td ><p>none</p></td></tr><tr><td  scope="row"><p><code>volm</code></p></td><td ><p><code>volm [+ | -] &lt;RealValue></code></p><p>The speech volume command sets the speech volume on the current speech channel to the specified real value. If the volume value is preceded by a + or - character, the speech volume is increased or decreased relative to its current value.</p></td><td ><p><code>soVolume</code></p></td></tr><tr><td  scope="row"><p><code>xtnd</code></p></td><td ><p><code>xtnd &lt;OSType> [&lt;Parameter> ...]</code></p><p>The synthesizer-specific <code>xtnd</code> command enables other synthesizer-specific commands to be embedded in the text. The first parameter (<code>OSType</code>) must be the creator ID of the synthesizer. The remaining optional parameters are synthesizer-specific.</p></td><td ><p><code>soSynthExtension</code></p></td></tr></table></div><a name="//apple_ref/doc/uid/TP40004365-CH5-SW14" title="Embedded Speech Command Errors"></a><h3>Embedded Speech Command Errors</h3><p>While embedded speech commands are being processed, errors might be detected and reported to your application. If you enable error callbacks using the <code>SetSpeechInfo</code> function with the <code>soErrorCallBack</code> selector, your error callback procedure will be executed once for every error that is detected (for more information on the error callback, see <code><!--a target="_top" -->SpeechErrorProcPtr<!--/a--></code>). If you don’t enable error callbacks, you can still get information about these errors by calling the <code>GetSpeechInfo</code> function with the <code>soErrors</code> selector.</p><p>During processing of embedded speech commands, the following errors can be detected:</p><div class="tableholder"><table class="graybox" border = "0" cellspacing="0" cellpadding="5"><tr><th scope="col" align="left" style="font-weight: bold" bgcolor="#CCCCCC"><p>Result code</p></th><th scope="col" align="left" style="font-weight: bold" bgcolor="#CCCCCC"><p>Value</p></th><th scope="col" align="left" style="font-weight: bold" bgcolor="#CCCCCC"><p>Description</p></th></tr><tr><td  scope="row"><p><code>badParmVal</code></p></td><td ><p><code>-245</code></p></td><td ><p>Parameter value is invalid</p></td></tr><tr><td  scope="row"><p><code>badCmdText</code></p></td><td ><p><code>-246</code></p></td><td ><p>Embedded command syntax or parameter problem</p></td></tr><tr><td  scope="row"><p><code>unimplCmd</code></p></td><td ><p><code>-247</code></p></td><td ><p>Embedded command is not implemented on synthesizer</p></td></tr><tr><td  scope="row"><p><code>unimplMsg</code></p></td><td ><p><code>-248</code></p></td><td ><p>Unimplemented message</p></td></tr><tr><td  scope="row"><p><code>badVoiceID</code></p></td><td ><p><code>-250</code></p></td><td ><p>Specified voice has not been preloaded</p></td></tr><tr><td  scope="row"><p><code>badParmCount</code></p></td><td ><p><code>-252</code></p></td><td ><p>Incorrect number of embedded command arguments</p></td></tr></table></div><a name="//apple_ref/doc/uid/TP40004365-CH5-SW4" title="Use Phoneme Modifiers to Adjust Pronunciation"></a><h2>Use Phoneme Modifiers to Adjust Pronunciation</h2><p>As described in <span class="content_text"><a href="../SpeechOverview/SpeechOverview.html#//apple_ref/doc/uid/TP40004365-CH3-SW1">“Representations of Speech,”</a></span> the Speech Synthesis framework allows you to represent some or all of the words in a string or buffer as phonemes. When you supply the phonemic representation of a word, you specify the precise combination of sounds you want the synthesizer to pronounce. In addition, you can add phoneme modifiers to increase or decrease the stress with which phonemes and words are pronounced.</p><p>Recall that phonemes are represented by combinations of uppercase or lowercase characters, such as OW for the long “o” sound in the English word “boat.“ (Other languages use different phonemes and phoneme symbols; this document focuses on the set of North American English phonemes the MacinTalk synthesizer recognizes.) The complete set of phonemes is listed in <span class="content_text"><a href="../Phonemes/Phonemes.html#//apple_ref/doc/uid/TP40004365-CH9-SW1">“Phonemes.”</a></span> </p><p>Because a synthesizer has no reliable way to detect the difference between characters that represent phonemes and characters that represent words, you must state the appropriate mode. There are two ways you can do this:</p><ul class="ul"><li class="li"><p>In your application, use the <code>soPhonemeMode</code> selector with the <code><a href="../../../../Carbon/Reference/Speech_Synthesis_Manager/Reference/reference.html#//apple_ref/doc/c_ref/SetSpeechInfo" target="_top">SetSpeechInfo</a></code> function to tell the current speech channel to process subsequent text as phonemes. This is a mode change for the speech channel, which means that it will interpret <em>all</em> text as phonemes until you call the <code><a href="../../../../Carbon/Reference/Speech_Synthesis_Manager/Reference/reference.html#//apple_ref/doc/c_ref/SetSpeechInfo" target="_top">SetSpeechInfo</a></code> function with the <code>soTextMode</code> selector, to return to the default, text-processing mode (or until the speech channel closes). For examples of how to use this function, see <span class="content_text"><a href="FineTuning.html#//apple_ref/doc/uid/TP40004365-CH5-SW13">“Adjust Speech Channel Settings Using the Carbon Speech Synthesis API.”</a></span></p></li><li class="li"><p>In your text buffer or string, precede the phonemic representation of a word or phrase with the <code>[[inpt PHON]]</code> embedded speech command and follow it with the <code>[[inpt TEXT]]</code> command. This causes the synthesizer to switch to the phoneme input-processing mode for the content after the <code>[[inpt PHON]]</code> command and switch back again when it encounters the <code>[[inpt TEXT]]</code> command.</p></li></ul><p>Within the phonemic representation of a word or phrase, you can insert modifiers that allow you to adjust the stress the synthesizer places on words and syllables. These modifiers are called prosodic controls.</p><p>Unlike embedded speech commands, prosodic controls do not require command delimiter strings and they do not allow parameters. Because prosodic controls are valid only within the phonemic representation of text, the symbols that represent them consist of characters that are not used to represent phonemes. To use prosodic control symbols in the phonemic representation of your text, place the appropriate symbol before the phoneme you want to modify. The effect of the prosodic control symbol is limited to the phoneme that immediately follows it; it has no effect on any subsequent phonemes.</p><p><span class="content_text">Table 3-2</span> lists the available prosodic control symbols and describes how they modify individual phonemes. If you’d like to listen to the spoken version of any of the examples in <span class="content_text">Table 3-2</span>, you can copy it to a Text Edit document, precede it with the <code>[[inpt PHON]]</code> command, and select Speech > Start Speaking Text from the Services menu item.</p><a name="//apple_ref/doc/uid/TP40004365-CH5-SW8" title="Table 3-2Prosodic control symbols and descriptions"></a><div class="tableholder"><table class="graybox" border = "0" cellspacing="0" cellpadding="5"><caption class="tablecaption"><strong>Table 3-2&nbsp;&nbsp;</strong>Prosodic control symbols and descriptions</caption><tr><th scope="col" align="left" style="font-weight: bold" bgcolor="#CCCCCC"><p>Category</p></th><th scope="col" align="left" style="font-weight: bold" bgcolor="#CCCCCC"><p>Action</p></th><th scope="col" align="left" style="font-weight: bold" bgcolor="#CCCCCC"><p>Symbol</p></th><th scope="col" align="left" style="font-weight: bold" bgcolor="#CCCCCC"><p>Description and example</p></th></tr><tr><td  scope="row"><p>Lexical stress</p></td><td ><p>Primary stress</p></td><td ><p>1</p></td><td ><p>Marks the primary stress within a word</p><p>For example, the word “developer” is pronounced with the primary stress on the second syllable, as shown below:</p><p><code>dIHv1EHlAXpAXr</code></p></td></tr><tr><td  scope="row"><p></p></td><td ><p>Secondary stress</p></td><td ><p>2</p></td><td ><p>Marks the secondary stress within a word</p><p>For example, the word “application” is pronounced with the primary stress on the third syllable and a secondary stress on the first syllable, as shown below:</p><p><code>2AEplIHk1EYSIXn</code></p></td></tr><tr><td  scope="row"><p>Syllable breaks</p></td><td ><p>Syllable mark</p></td><td ><p>= (equal)</p></td><td ><p>Marks syllable breaks within a word</p><p>For example, the word “cheaply” is pronounced with a subtle syllable break between “cheap” and “ly.” To ensure that a synthesizer pronounces this word correctly (and not with a syllable break between “chea” and “ply”), you can insert a syllable mark, as shown below:</p><p><code>C1IYp=lIY</code></p></td></tr><tr><td  scope="row"><p>Word prominence</p></td><td ><p>Destressed</p></td><td ><p>~ (tilde)</p></td><td ><p>Marks words that should be destressed in a sentence</p><p>Words that carry minimal information can be destressed to lessen their prominence in a sentence. For example, in the sentence “What is in the bag?,“ the words “in” and “the” are unimportant, relative to “What,” “is,” and “bag.” Therefore, “in” and “the” can be marked as not needing stress, as shown below:</p><p><code>_w1UXt _1IHz ~2IHn ~nAX _b1AEg?</code></p></td></tr><tr><td  scope="row"><p></p></td><td ><p>Normal stress</p></td><td ><p>_ (underscore)</p></td><td ><p>Marks words that should receive normal stress</p><p>Words that bear information should be spoken with normal stress to differentiate them from less important words. For example, in the sentence “What is in the bag?,“ the words “What,“ “is,“ and “bag” should be spoken with normal stress because they convey more information to the listener than the words “in” and “the.“ Therefore, these information-bearing words can be marked as needing normal stress, as shown below:</p><p><code>_w1UXt _1IHz ~2IHn ~nAX _b1AEg?</code></p></td></tr><tr><td  scope="row"><p></p></td><td ><p>Emphatic stress</p></td><td ><p>+ (plus)</p></td><td ><p>Marks words that require special emphasis</p><p>The most important words in a sentence should receive emphatic stress to make them stand out from the rest of the sentence. For example, in the sentence “Don’t ever do that again!,“ the word “that” can be given extra emphasis to draw attention to it, as shown below:</p><p><code>~dOWnt ~1EHvAXr ~d1UW +DAEt _AXg1EHn!</code></p></td></tr></table></div><div class="notebox"><a name="//apple_ref/doc/uid/TP40004365-CH5-DontLinkElementID_25" title="Note"></a><p><strong>Note:</strong>&nbsp;As with other embedded commands, the exact nature of the effects you can achieve with prosodic control symbols is dependent on synthesizer capabilities.</p></div><a name="//apple_ref/doc/uid/TP40004365-CH5-DontLinkElementID_2" title="Use Punctuation Correctly"></a><h2>Use Punctuation Correctly</h2><p>Punctuation marks are not embedded commands, but they appear in text and can affect the prosody of synthesized speech in some similar ways. This section describes how English-language synthesizers are likely to interpret punctuation marks.</p><p>For the most part, punctuation marks affect the pitch of synthesized speech and the duration of pauses. For example, the period at the end of a sentence generally causes a synthesizer to lower the pitch and insert a pause. Most speech synthesizers strive to mimic the pauses and changes in pitch of human speakers in response to punctuation marks, so you’ll obtain the best results by punctuating your text according to standard grammatical rules. </p><p><span class="content_text">Table 3-3</span> lists the standard English punctuation marks and how they affect sentence prosody. Be aware that some languages do not use some of these punctuation marks, so synthesizers for other languages might not interpret them as described in <span class="content_text">Table 3-3</span>, if at all.</p><a name="//apple_ref/doc/uid/TP40004365-CH5-SW5" title="Table 3-3Effects of punctuation marks on synthesized speech"></a><div class="tableholder"><table class="graybox" border = "0" cellspacing="0" cellpadding="5"><caption class="tablecaption"><strong>Table 3-3&nbsp;&nbsp;</strong>Effects of punctuation marks on synthesized speech</caption><tr><th scope="col" align="left" style="font-weight: bold" bgcolor="#CCCCCC"><p>Symbol</p></th><th scope="col" align="left" style="font-weight: bold" bgcolor="#CCCCCC"><p>Effect on pitch</p></th><th scope="col" align="left" style="font-weight: bold" bgcolor="#CCCCCC"><p>Effect on timing</p></th></tr><tr><td  scope="row"><p>,</p></td><td ><p>Rise in pitch</p></td><td ><p>Short pause follows</p></td></tr><tr><td  scope="row"><p>(</p></td><td ><p>Start range of reduced pitch</p></td><td ><p>Short pause follows</p></td></tr><tr><td  scope="row"><p>)</p></td><td ><p>End range of reduced pitch</p></td><td ><p>Short pause follows</p></td></tr><tr><td  scope="row"><p>.</p></td><td ><p>Fall in pitch</p></td><td ><p>Pause follows</p></td></tr><tr><td  scope="row"><p>"</p></td><td ><p>Expand pitch range</p></td><td ><p>A short pause precedes an opening quote and follows a closing quote</p></td></tr></table></div><p>Even among English-language synthesizers, the specific pitch contours associated with the punctuation marks listed in <span class="content_text">Table 3-3</span> might vary according to other considerations arising from analysis of the text. For example, if a synthesizer determines that a question is rhetorical, the pitch might fall at the question mark, instead of rise. Also, the timing effects associated with the punctuation marks can vary according to current speech rate settings. Consequently, you should view the information in <span class="content_text">Table 3-3</span> as guidance only; test your application’s spoken output with a particular synthesizer to find out how the punctuation is actually interpreted.</p><a name="//apple_ref/doc/uid/TP40004365-CH5-SW7" title="Use the TUNE Format to Supply Complex Pitch Contours"></a><h2>Use the TUNE Format to Supply Complex Pitch Contours</h2><p>In addition to supporting the phoneme input-processing mode, the MacinTalk synthesizer available in Mac OS X v10.2 and later supports the TUNE input-processing mode. This mode accepts directives in the TUNE format, which allows you to supply a complex pitch contour, or tune, with which a word or phrase should be spoken. Such a tune can represent the pitch and speech-rate changes you hear when a person speaks in an expressive way. For example, adults speaking to small children often vary the pitch of their speech much more than they do when speaking to other adults. As described in <span class="content_text"><a href="FineTuning.html#//apple_ref/doc/uid/TP40004365-CH5-SW4">“Use Phoneme Modifiers to Adjust Pronunciation,”</a></span> phoneme modifiers can be used to adjust the stress placed on particular phonemes, but you cannot use them to cause multiple variations in pitch during the pronunciation of a single phoneme. To do this, you must use the TUNE format. </p><p>Apple provides the Repeat After Me developer tool to help you create the set of symbols that describe a tune. Using the Repeat After Me application (located in <code>/Developer/Applications/Utilities/Speech</code>), you can record an utterance that exhibits your desired pitch contour and use that to shape any other utterance in your application. </p><p>Similar to the way you enter and exit the phoneme input-processing mode, you use the <code>inpt</code> embedded command to turn on and off the TUNE input-processing mode. Specifically, you insert <code>[[inpt TUNE]]</code> before the content in the TUNE format and insert <code>[[inpt TEXT]]</code> after it. The TUNE format recognizes the same set of phoneme symbols used in the phoneme input-processing mode (see <span class="content_text"><a href="../Phonemes/Phonemes.html#//apple_ref/doc/uid/TP40004365-CH9-SW1">“Phonemes”</a></span> for a description of these symbols).</p><p>The TUNE format defines a command syntax you use to specify pitch and duration attributes for each phoneme. Each phoneme may be followed by a pair of braces, enclosing a single duration attribute, preceded by the symbol “D,” and an arbitrary number of pitch attributes, preceded by the symbol “P.“ The duration attribute indicates the total duration of the phoneme in milliseconds. Each pitch attribute consists of a pair of numbers separated by a colon. The first number is decimal value that specifies a pitch in hertz (Hz) and the second number is an integer that specifies the location of that pitch within the phoneme, expressed as an integer percentage of the total duration of the phoneme. </p><p>To illustrate the syntax of the TUNE format, consider the sentence “Are you sure you brushed your teeth?“ The default pronunciation of this sentence is perfectly understandable, but the intonation is uninteresting. (If you’re reading this document in Safari, Preview, or Xcode, select “Are you sure you brushed your teeth?“ and choose Speech > Start Speaking Text from the Services menu item to hear the default pronunciation.) Imagine that you want this sentence to be spoken as a parent might speak it to a child, with emphasis on “sure” and an exaggerated rise in pitch through the end of the sentence. Using the Repeat After Me application, you can record a person speaking the sentence in this way, apply the resulting pitch and duration information to the text, and get the representation in the TUNE format. Following this process, you might end up with something similar to the following:</p><div class="codesample"><table><tr><td scope="row"><pre>[[inpt TUNE]]
~
AA {D 120; P 176.9:0 171.4:22 161.7:61}
r {D 60; P 166.7:0}
~
y {D 210; P 161.0:0}
UW {D 70; P 178.5:0}
_
S {D 290; P 173.3:0 178.2:8 184.9:19 222.9:81}
1AX {D 280; P 234.5:0 246.1:39}
r {D 170; P 264.2:0}
~
y {D 200; P 276.9:0 274.9:17 271.0:50}
UW {D 40; P 265.0:0 264.3:50}
_
b {D 140; P 263.6:0 263.5:13 263.3:60}
r {D 110; P 263.1:0 260.4:43}
1UX {D 30; P 256.8:0 256.8:6}
S {D 190; P 256.1:0}
t {D 20; P 252.0:0 253.6:47}
~
y {D 30; P 255.5:0 257.8:45}
AO {D 40; P 260.6:0 260.0:56}
r {D 40; P 259.5:0}
_
t {D 190; P 251.3:0 250.0:16 245.9:68}
1IY {D 260; P 243.4:0 248.1:8 286.1:72 288.5:84}
T {D 220; P 291.6:0 262.8:27 220.0:67 184.8:100}
? {D 300}
[[inpt TEXT]]
<span></span></pre></td></tr></table></div><p>To listen to this version of the sentence, select the lines above (be sure to include the “<code>[[inpt TUNE]]</code>‘‘ at the beginning and the “<code>[[inpt TEXT]]</code>“ at the end), copy them, and paste them into a Text Edit document. Make sure all the lines are still selected and then select Speech > Start Speaking Text from the Services menu item in the Text Edit menu.</p><p>The TUNE format also includes optional settings that describe the beginning value and range of the pitch, expressed in hertz, and the speech rate, expressed in words per minute. You can use these settings to state the pitch and rate conditions that were in effect when you created the tune. If either of these settings have nonzero values, the synthesizer will scale the pitch and duration attribute values you supply for the phonemes according to voice conditions in effect during synthesis. This is analogous to transposing a song to a different key and playing it at a different tempo. If both of these settings are missing, the synthesizer interprets the pitch and duration attribute values as literal values that should be reproduced exactly, which is analogous to playing a song in the key and time signature in which it was composed. </p><a name="//apple_ref/doc/uid/TP40004365-CH5-SW9" title="Synchronize Speech with Application-Specific Actions"></a><h2>Synchronize Speech with Application-Specific Actions</h2><p>As mentioned in <span class="content_text"><a href="../SpeechOverview/SpeechOverview.html#//apple_ref/doc/uid/TP40004365-CH3-SW13">“Notifications, Callbacks, and Speech Synchronization,”</a></span> you can synchronize your application’s spoken output with other tasks in your application. Both the Cocoa and the Carbon speech synthesis APIs provide mechanisms you can use to get notifications when, for example, a word or phoneme is about to be spoken or has just been spoken. This section describes how you can receive these notifications and some of the ways you might use them.</p><p>The <code>NSSpeechSynthesizer</code> class defines a few delegate methods you can use to synchronize tasks with speech-related actions. For example, you can implement the <code>speechSynthesizer:willSpeakWord:ofString</code> delegate method to highlight a word as it’s being spoken. For an example of an implementation that does this, see <span class="content_text"><a href="../UsingSpeech/UsingSpeech.html#//apple_ref/doc/uid/TP40004365-CH4-SW7">Listing 2-2</a></span>. The <code>NSSpeechSynthesizer</code> class also defines the <code>speechSynthesizer:willSpeakPhoneme</code> and <code>speechSynthesizer:didFinishSpeaking</code> delegate methods you can use to find out when a phoneme is about to be spoken and when an <code>NSSpeechSynthesizer</code> object has finished speaking, respectively. <span class="content_text"><a href="../UsingSpeech/UsingSpeech.html#//apple_ref/doc/uid/TP40004365-CH4-SW7">Listing 2-2</a></span> includes an implementation of the <code>speechSynthesizer:didFinishSpeaking</code> delegate method that resets the cursor to the beginning of the line of text and re-enables buttons in the application window.</p><p>The Carbon speech synthesis API defines several callback function types you can use to create and install callback functions in a speech channel. For each event to which you want to respond, you create a callback function that adheres to the prototype defined by the callback pointer (see <em><a href="../../../../Carbon/Reference/Speech_Synthesis_Manager/index.html#//apple_ref/doc/uid/TP30000211" target="_top">Speech Synthesis Manager Reference</a></em> for these prototypes). Then, you install each callback function in a speech channel by passing the appropriate selector to the <code><a href="../../../../Carbon/Reference/Speech_Synthesis_Manager/Reference/reference.html#//apple_ref/doc/c_ref/SetSpeechInfo" target="_top">SetSpeechInfo</a></code> function, as shown below:</p><div class="codesample"><table><tr><td scope="row"><pre>// Install MyWordCallback callback function in the current speech channel<span></span></pre></td></tr><tr><td scope="row"><pre>error = SetSpeechInfo(currentSpeechChannel, soWordCallBack, MyWordCallback);<span></span></pre></td></tr></table></div><p>When the Speech Synthesis Manager encounters one of the events handled by these callbacks, it calls the callback function you’ve installed, allowing you to synchronize custom processing with that speech event. The six callback function types defined in the Carbon speech synthesis API are listed below, each accompanied by the selector you use to install the callback function:</p><div class="tableholder"><table class="graybox" border = "0" cellspacing="0" cellpadding="5"><tr><th scope="col" align="left" style="font-weight: bold" bgcolor="#CCCCCC"><p>Callback</p></th><th scope="col" align="left" style="font-weight: bold" bgcolor="#CCCCCC"><p>Selector</p></th></tr><tr><td  scope="row"><p><code>SpeechWordProcPtr</code></p></td><td ><p><code>soWordCallBack</code></p></td></tr><tr><td  scope="row"><p><code>SpeechPhonemeProcPtr</code></p></td><td ><p><code>soPhonemeCallBack</code></p></td></tr><tr><td  scope="row"><p><code>SpeechDoneProcPtr</code></p></td><td ><p><code>soSpeechDoneCallBack</code></p></td></tr><tr><td  scope="row"><p><code>SpeechErrorProcPtr</code></p></td><td ><p><code>soErrorCallBack</code></p></td></tr><tr><td  scope="row"><p><code>SpeechSyncProcPtr</code></p></td><td ><p><code>soSyncCallBack</code></p></td></tr><tr><td  scope="row"><p><code>SpeechTextDoneProcPtr</code></p></td><td ><p><code>soTextDoneCallBack</code></p></td></tr></table></div><p>The <code>SpeechWordProcPtr</code>, <code>SpeechPhonemeProcPtr</code>, and <code>SpeechDoneProcPtr</code> callbacks are triggered by the same events as the <code>NSSpeechSynthesizer</code> delegate methods. Therefore, you can use these to perform custom processing when a word or phoneme is about to be spoken and when speaking has stopped. See <span class="content_text"><a href="../UsingSpeech/UsingSpeech.html#//apple_ref/doc/uid/TP40004365-CH4-SW4">Listing 2-4</a></span> for an example usage of the <code>SpeechWordProcPtr</code> callback.</p><p>The Carbon speech synthesis API uses the <code>SpeechErrorProcPtr</code> pointer to call a speech channel’s error callback function when it encounters syntax errors in a text buffer’s embedded commands (see <span class="content_text"><a href="FineTuning.html#//apple_ref/doc/uid/TP40004365-CH5-SW14">“Embedded Speech Command Errors”</a></span> for a list of possible errors). In addition to helping you find such errors during application development, this callback allows you to display an alert or perform some other action if there are errors in the embedded commands users supply.</p><p>The <code>SpeechSyncProcPtr</code> defines a callback function you can implement to synchronize application-specific actions with the presence of the <code>sync</code> embedded speech command. When the Speech Synthesis Manager encounters a <code>sync</code> command in a string or buffer of text, it calls the callback function you’ve installed in the speech channel. You can use the parameter of the <code>sync</code> command to provide an arbitrary value that gets passed to your callback function, allowing you to distinguish among different usages of the command. Although you can use the <code>sync</code> command to trigger a callback when a word or phoneme is about to be spoken, it’s best to use the provided callback mechanisms for these events, reserving the <code>sync</code> command for application-defined events.</p><p>The <code>SpeechTextDoneProcPtr</code> defines a callback function that gets called when the Speech Synthesis Manager finishes processing a buffer of text. This can happen before the synthesizer finishes speaking the text or before the synthesizer even starts speaking the text. You might supply a callback function for this event if you want to be able to dispose of the original text buffer as soon as the Speech Synthesis Manager finishes copying it.</p><p></p><a name="//apple_ref/doc/uid/TP40004365-CH5-SW1" title="Avoid Cross-Talk"></a><h2>Avoid Cross-Talk</h2><p>Just as it’s confusing to listen to more than one person talking at the same time, it’s confusing for users to hear more than one application speaking at the same time. With the popularity of VoiceOver and an increasing number of applications capable of producing speech, the potential for overlapping or interrupted speech is significant. This section explains how VoiceOver implements speech arbitration and describes ways you can avoid interrupting the spoken output of other applications and processes.</p><p>While VoiceOver is running, there is an automatic arbitration mechanism in place that causes all other spoken output to stop when VoiceOver starts to speak. Because VoiceOver provides the accessibility interface to Mac OS X and visually impaired users rely on it to navigate and control the system, it is appropriate to give it priority over other types of spoken output.</p><p>While VoiceOver is not running, however, there is no arbitration mechanism in place. For this reason, it’s a good idea for your application to ascertain if another application or process is currently speaking before beginning to speak. Both the Carbon and Cocoa speech synthesis APIs provide a way to do this.</p><p>If you’re using the Cocoa <code>NSSpeechSynthesizer</code> class to produce spoken output, you can invoke the <code><a href="../../../../Cocoa/Reference/ApplicationKit/Classes/NSSpeechSynthesizer_Class/Reference/Reference.html#//apple_ref/occ/clm/NSSpeechSynthesizer/isAnyApplicationSpeaking" target="_top">isAnyApplicationSpeaking</a></code> class method to find out if another application or a system component (such as VoiceOver) is currently producing speech. This method returns a Boolean value your application can use to decide when it’s appropriate to speak. Depending on the needs of your application, you might use this method in the following way:</p><div class="codesample"><table><tr><td scope="row"><pre>if ([NSSpeechSynthesizer isAnyApplicationSpeaking]) {<span></span></pre></td></tr><tr><td scope="row"><pre>    // Wait.<span></span></pre></td></tr><tr><td scope="row"><pre>} else {<span></span></pre></td></tr><tr><td scope="row"><pre>    [_mySpeechSynthesizer startSpeakingString:myTextToSpeak];<span></span></pre></td></tr><tr><td scope="row"><pre>}<span></span></pre></td></tr></table></div><p>If you’re using the Carbon speech synthesis API, you use a combination of two functions to determine whether any other application or system component is currently speaking. First, use the <code><a href="../../../../Carbon/Reference/Speech_Synthesis_Manager/Reference/reference.html#//apple_ref/c/func/SpeechBusySystemWide" target="_top">SpeechBusySystemWide</a></code> function to get the total number of speech channels (including paused speech channels) that are currently synthesizing speech on the computer. This includes the speech channels the Speech Synthesis Manager automatically creates in response to the <code><a href="../../../../Carbon/Reference/Speech_Synthesis_Manager/Reference/reference.html#//apple_ref/c/func/SpeakString" target="_top">SpeakString</a></code> function and all speech channels your application is using. To find out if there are other applications or processes currently producing speech, therefore, you must subtract the speech channels your application is using from the number of speech channels you get from <code>SpeechBusySystemWide</code>. To get the total number of speech channels associated with your application, use the <code><a href="../../../../Carbon/Reference/Speech_Synthesis_Manager/Reference/reference.html#//apple_ref/c/func/SpeechBusy" target="_top">SpeechBusy</a></code> function, as shown below:</p><div class="codesample"><table><tr><td scope="row"><pre>short totalChannels, myTotalChannels;<span></span></pre></td></tr><tr><td scope="row"><pre>totalChannels = SpeechBusySystemWide();<span></span></pre></td></tr><tr><td scope="row"><pre>myTotalChannels = SpeechBusy();<span></span></pre></td></tr><tr><td scope="row"><pre>if ((totalChannels - myTotalChannels) > 0) {<span></span></pre></td></tr><tr><td scope="row"><pre>    // Wait.<span></span></pre></td></tr><tr><td scope="row"><pre>} else {<span></span></pre></td></tr><tr><td scope="row"><pre>    SpeakText(mySpeechChannel, myTextToSpeak, strlen(myTextToSpeak));<span></span></pre></td></tr><tr><td scope="row"><pre>}<span></span></pre></td></tr></table></div><a name="//apple_ref/doc/uid/TP40004365-CH5-SW2" title="Four Ways to Improve Spoken Output"></a><h2>Four Ways to Improve Spoken Output</h2><p>A synthesizer follows a predetermined set of rules about language production when it converts text to spoken output. But no matter how sophisticated and extensive those rules are, there will always be situations they don’t cover. As the developer, you know a lot more about how your application’s speech should sound than any synthesizer does, so you should take advantage of the available customization opportunities to produce the best possible spoken output.</p><p>If you’re viewing this document in Safari, Preview, or Xcode, you can listen to any example in this section by selecting it and then choosing Speech > Start Speaking Text from the Services menu item in the Application menu. If you’d like to experiment with the samples, one way to do this is to type or copy and paste them into a Text Edit window. After you’ve made adjustments and you want to listen to the result, select it and choose Speech > Start Speaking Text from the Services menu item in the Text Edit menu.</p><p></p><a name="//apple_ref/doc/uid/TP40004365-CH5-DontLinkElementID_3" title="Adjust the Pronunciation of Troublesome Words"></a><h3>Adjust the Pronunciation of Troublesome Words</h3><p>As described in <span class="content_text"><a href="../SpeechOverview/SpeechOverview.html#//apple_ref/doc/uid/TP40004365-CH3-SW3">“Opportunities for the Customization of Synthesized Speech,”</a></span> you can use embedded commands to adjust the pronunciation of words a synthesizer is likely to mispronounce, such as proper nouns. Another category of words a synthesizer may have difficulty with is words that are spelled the same but pronounced differently depending on semantic context. A common developer reaction to either of these situations is to deliberately misspell the word in an attempt to trick the synthesizer into pronouncing it correctly. Although this approach might work with a particular version of a synthesizer, it is ultimately unreliable. This is because future enhancements to a synthesizer can result in a more accurate pronunciation of the original word and an even worse pronunciation of the misspelled version. A much better approach is to represent the word phonemically and apply the appropriate prosodic controls.</p><p>Although you can select individual phonemes and create the phonemic representation of a word “by hand,” it’s usually more efficient to start with a synthesizer’s default phonemic representation and adjust it as necessary. This is because a synthesizer often mispronounces only one or two phonemes in a word, which means the remaining phonemes are accurate.</p><p>For example, the default pronunciation of the name “Matthias” places the stress on the first syllable and pronounces the first “a” the same as the “a” in the English word “father.“ (The phonemic representation of this pronunciation is <code>m1AAtIYIXs</code>.) To hear the default pronunciation, listen to the spoken version of the following sentence:</p><p><code>My name is Matthias.</code></p><p>If you wanted to change the pronunciation so that the stress is on the second syllable and the first “a“ sounds like the “a” in “about,” you would change the phonemic representation of the name to <code>mAXt1IYIXs</code>. To hear how this changes the synthesizer’s pronunciation, listen to the spoken version of the following sentence:</p><p><code>My name is [[inpt PHON]]mAXt1IYIXs[[inpt TEXT]].</code></p><a name="//apple_ref/doc/uid/TP40004365-CH5-DontLinkElementID_4" title="Let the User Catch Up"></a><h3>Let the User Catch Up</h3><p>Listening to speech is a mentally intensive process, whether the speech is produced by another person or generated by a synthesizer. For this reason, most human speakers naturally insert pauses into their speech to allow listeners enough time to absorb the content. Including pauses in the spoken output of an application is especially important, because the computer can’t adjust its delivery in response to verbal or nonverbal feedback from the listener.</p><p>Adding pauses to synthesized speech is primarily a matter of inserting units of silence at specific places in the text. You can do this in any of the following ways:</p><ul class="spaceabove"><li class="li"><p>Use appropriate punctuation within sentences. The correct use of commas, colons, and semicolons is as important for listeners as it is for readers.</p><p>Listen to both versions of the sentence below:</p><p><code>Today I feel well yesterday I felt terrible.</code></p><p><code>Today I feel well; yesterday I felt terrible.</code></p><p>The second version conveys the juxtaposition of the two states of the speaker’s condition much more clearly than the first version.</p></li><li class="li"><p>Use short, declarative sentences when possible. Although complex sentences can be acceptable in text, they can be difficult to understand when spoken. The synthesizer automatically adds a noticeable pause between sentences, which helps users assimilate the information in one sentence before turning their attention to the next sentence. For this reason, an idea expressed in a couple of short sentences will include more silence than the same idea expressed in a single, long sentence.</p><p>Listen to the following long sentence:</p><p><code>After you insert a section break, you can use the layout tool (located in the Tools menu) to format the new section, which can have different margins and numbers of columns than other sections in the document.</code></p><p>Although the synthesizer pauses briefly at the commas and the parentheses, the pauses that accompany the periods in the 3-sentence version of this information make it easier to absorb:</p><p><code>After you insert a section break, you can use the layout tool to format the new section. The layout tool is located in the Tools menu. Each section can have different margins and numbers of columns than other sections in the document.</code></p></li><li class="li"><p>Use the <code>slnc</code> (silence) embedded speech command. You can add an arbitrary amount of silence anywhere in the text by inserting the <code>[[slnc x]]</code> command (where <code>x</code> is a number of milliseconds). </p><p>For example, inserting extra silence between the items in a list makes it easier for people to take note of each item. Listen to the following sentence, which lists four items, separated by commas:</p><p><code>Don't forget to bring your hat, sunglasses, sandals, and towel.</code></p><p>Now listen to the same sentence, with 400 milliseconds of silence inserted between the listed items, and notice that you hear each item more distinctly:</p><p><code>Don't forget to bring your hat, [[slnc 400]] sunglasses, [[slnc 400]] sandals, [[slnc 400]] and towel.</code></p></li></ul><a name="//apple_ref/doc/uid/TP40004365-CH5-DontLinkElementID_5" title="Focus the User&acirc;&#128;&#153;s Attention"></a><h3>Focus the User’s Attention</h3><p>Listen closely to people speaking and you’ll notice that they tend to emphasize the words in a sentence that carry new and important information and deemphasize less important and repetitive words. These differences in emphasis make it easier for listeners to recognize the important ideas in a sentence. Adding appropriate emphasis (or deemphasis) to words in your application’s speech can make the spoken output much easier for listeners to understand.</p><p>The following three sentences all follow the same pattern, but each provides different information. Without adjustments in emphasis, the sentences are very similar and it’s hard to focus on the differences in the times and the names.</p><p><code>On May tenth, you have a meeting in Cupertino. On June tenth, you have a meeting in Tokyo. On July tenth, you have a meeting in Paris.</code></p><p>Now listen to these three sentences with embedded commands that emphasize the important words and deemphasize the less-important, repetitive words:</p><p><code>On May tenth, you have a meeting in Cupertino. On [[emph +]] June [[emph -]] tenth, you [[emph -]] have a [[emph -]] meeting in [[emph +]] Tokyo. On [[emph +]] July [[emph -]] tenth, you [[emph -]] have a [[emph -]] meeting in [[emph +]] Paris.</code></p><a name="//apple_ref/doc/uid/TP40004365-CH5-DontLinkElementID_6" title="Liven It Up!"></a><h3>Liven It Up!</h3><p>People naturally express emotion in their speech to add other layers of meaning and to keep listeners engaged. Adding the illusion of emotion to synthesized speech is not as easy as inserting pauses and fine-tuning pronunciations, but you can achieve satisfactory results by carefully adjusting the pitch and timing of your spoken output.</p><p>For example, when people are sad or depressed, their speech is usually slower, more monotone, and often quieter than normal. Conversely, when people are happy or excited, their speech generally exhibits greater range in pitch and is often faster and louder than normal. You can use the TUNE format to approximate these qualities to give the impression of emotion to the speech your application generates.</p><p>For example, the default pronunciation of the sentence “Sorry, Dave, I can’t do that right now.“ is emotionally bland. To give listeners the impression that the speaker is perhaps a bit regretful, but nonetheless implacable, you might use the TUNE format to create the following utterance:</p><div class="codesample"><table><tr><td scope="row"><pre>[[inpt TUNE]]
~
s {D 250; P 212.0:0 212.0:35 212.0:54 212.0:85 212.0:96}
1AA {D 190; P 232.0:0 218.0:35 222.0:80}
r {D 80; P 216.0:0}
IY {D 150; P 177.0:0 162.0:29 162.0:68 162.0:77 162.0:90 162.0:100}
, {D 20}
~
d {D 60; P 162.0:0 162.0:36 162.0:57 160.0:93}
1EY {D 350; P 162.0:0 150.0:27 150.0:41 150.0:70}
v {D 30; P 150.0:0 150.0:29 150.0:52 150.0:67 150.0:90 150.0:100}
, {D 510}
~
2AY {D 140; P 173.0:0 196.0:45}
~
k {D 100; P 196.0:0 196.0:95}
AE {D 180; P 198.0:0 232.0:56}
n {D 80; P 232.0:0}
t {D 20; P 232.0:0 232.0:38}
~
d {D 40; P 232.0:0 232.0:85 208.0:92}
1UW {D 180; P 210.0:0 232.0:32 253.0:60 245.0:76}
~
D {D 60; P 245.0:0 186.0:92}
AE {D 240; P 186.0:0 168.0:37}
t {D 30; P 155.0:0 155.0:60 155.0:93}
~
r {D 70; P 155.0:0 149.0:53}
1AY {D 180; P 157.0:0 137.0:61}
t {D 40; P 128.0:0 132.2:56 135.0:94}
~
n {D 80; P 129.0:0 153.0:31 147.0:94}
1AW {D 340; P 147.0:0 140.8:22 169.2:88 148.0:100}
. {D 780}
[[inpt TEXT]]
<span></span></pre></td></tr></table></div>

        <br /><br /> 
        
        <div class="mini_nav_text" align="left">
        <span class="navButtons">
        <a href="../UsingSpeech/UsingSpeech.html">&lt; Previous Page</a><span style="margin-left: 8px"><a href="../CommandSyntax/CommandSyntax.html">Next Page &gt;</a></span>
        </span>
        <span id="showHideTOCLowerSpan">
        <a href="#" onclick="showHideTOC();"><img src="../../../../Resources/Images/show_toc_icon.gif" width="15" height="14" border="0" style="margin-bottom: -2px;" alt="" /></a> <a href="#" onclick="showHideTOC();">Hide TOC</a>
        </span>
        </div>

        <br/><hr /><div align="center"><p class="content_text" lang="en" dir="ltr"> <!--#if expr="0=1" -->&#x00a9; 2006 Apple Computer, Inc. All Rights Reserved. &#40;<!--#endif -->Last updated: 2006-09-05<!--#if expr="0=1" -->&#041;<!--#endif --></p></div>

        
        <div class="hideOnPrint hideInXcode">
        <!-- start of footer -->
        	<table width="100%" border="0" cellpadding="0" cellspacing="0">
		<tr>
			<td><div style="width: 100%; height: 1px; background-color: #919699; margin-top: 5px; margin-bottom: 15px"></div></td>
		</tr>
		<tr>
			<td align="center"><br/>
				<table border="0" cellpadding="0" cellspacing="0" class="graybox">
					<tr>
						<th>Did this document help you?</th>
					</tr>
					<tr>
						<td>
						    <div style="margin-bottom: 8px"><a href="http://developer.apple.com/feedback/?v=1&url=/documentation/UserExperience/Conceptual/SpeechSynthesisProgrammingGuide/FineTuning/FineTuning.html%3Fid%3DTP40004365-1.0&media=dvd" target=_new>Yes</a>:  Tell us what works for you.</div>
							<div style="margin-bottom: 8px"><a href="http://developer.apple.com/feedback/?v=2&url=/documentation/UserExperience/Conceptual/SpeechSynthesisProgrammingGuide/FineTuning/FineTuning.html%3Fid%3DTP40004365-1.0&media=dvd" target=_new>It&#8217;s good, but:</a> Report typos, inaccuracies, and so forth.</div>
							<div><a href="http://developer.apple.com/feedback/?v=3&url=/documentation/UserExperience/Conceptual/SpeechSynthesisProgrammingGuide/FineTuning/FineTuning.html%3Fid%3DTP40004365-1.0&media=dvd" target=_new>It wasn&#8217;t helpful</a>: Tell us what would have helped.</div>
						</td>
					</tr>
				</table>
			</td>
		</tr>
	</table>

        <!--#include virtual="/includes/framesetfooter" -->
        <!-- end of footer -->
        </div>
    </div>
</body>
</html>