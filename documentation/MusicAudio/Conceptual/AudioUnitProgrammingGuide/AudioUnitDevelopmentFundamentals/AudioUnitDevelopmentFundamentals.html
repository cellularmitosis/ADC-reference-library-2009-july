<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
	<title>Audio Unit Programming Guide: Audio Unit Development Fundamentals</title>
	<meta id="Generator" name="Generator" content="Gutenberg"/>
	<meta id="GeneratorVersion" name="GeneratorVersion" content="v132"/>
	<meta http-equiv="content-type" content="text/html;charset=utf-8"/>
	<meta id="Copyright" name="Copyright" content="Copyright 2009 Apple Inc. All Rights Reserved."/>
	<meta id="IndexTitle" name="IndexTitle" content="Audio Unit Development Fundamentals"/>
	<meta id="xcode-display" name="xcode-display" content="render"/>
	<meta id="toc-file" name="toc-file" content="../toc.html"/>
	<meta id="RESOURCES" content="../../../../Resources" />
	
	<link rel="stylesheet" type="text/css" href="../../../../Resources/CSS/frameset_styles.css"/>
	<script language="JavaScript" type="text/javascript" src="../../../../Resources/JavaScript/lib/prototype.js"></script>
	<script language="JavaScript" type="text/javascript" src="../../../../Resources/JavaScript/lib/scriptaculous.js"></script>
	<script language="JavaScript" type="text/javascript" src="../../../../Resources/JavaScript/page.js"></script>
	<script language="JavaScript" type="text/javascript" src="../../../../Resources/JavaScript/pedia.js"></script>
	<!--[if lte IE 6]>
		<style type="text/css">
			/*<![CDATA[*/ 
			html {overflow-x:auto; overflow-y:hidden;  }
			/*]]>*/
		</style>
	<![endif]-->
</head>    
<body bgcolor="#ffffff" onload="initialize_page();"><a name="//apple_ref/doc/uid/TP40003278-CH7" title="Audio Unit Development Fundamentals"></a>
    <noscript>
    <div id="tocMenu">
        <iframe id="toc_content" name="toc_content" SRC="../toc.html" width="210" height="100%" align="left" frameborder="0">This document set is best viewed in a browser that supports iFrames.</iframe>
    </div>
    </noscript>
    <div id="bodyText">
        <a name="top"></a>
        <div class="hideOnPrint hideInXcode">
        <!-- start of header -->
        <!--#include virtual="/includes/framesetheader" -->
        <!-- end of header -->
        </div>
        
        <!-- start of path -->
<div class="breadcrumb hideOnPrint hideInXcode"><a href="http://developer.apple.com/" target="_top">ADC Home</a> &gt; <a href="../../../../../referencelibrary/index.html#//apple_ref/doc/uid/TP30000943" target="_top">Reference Library</a> &gt; <a href="../../../../index.html#//apple_ref/doc/uid/TP30000440" target="_top">Guides</a> &gt; <a href="../../../index.html#//apple_ref/doc/uid/TP30000440-TP30000428" target="_top">Audio</a> &gt; <a href="../../../CoreAudio-date.html#//apple_ref/doc/uid/TP30000440-TP30000428-TP30000500" target="_top">Core Audio</a> &gt; <a href="../Introduction/Introduction.html#//apple_ref/doc/uid/TP40003278-CH1-SW2">Audio Unit Programming Guide</a> &gt; </div><br class="hideInXcode"/><!-- end of path -->
        
        <div class="mini_nav_text" align="left">
        <span class="navButtons">
        <a href="../Introduction/Introduction.html">&lt; Previous Page</a><span style="margin-left: 8px"><a href="../TheAudioUnit/TheAudioUnit.html">Next Page &gt;</a></span>
        </span>
        <span id="showHideTOCUpperSpan">
        <a href="#" onclick="showHideTOC();"><img src="../../../../Resources/Images/show_toc_icon.gif" width="15" height="14" border="0" style="margin-bottom: -2px;" alt="" hideText="Hide TOC" showText="Show TOC" /></a> <a href="#" onclick="showHideTOC();">Hide TOC</a>
        </span>
        </div>

        <hr />
        
        
        <a name="//apple_ref/doc/uid/TP40003278-CH7-SW5" title="Audio Unit Development Fundamentals"></a><h1>Audio Unit Development Fundamentals</h1><p>When you set out to create an audio unit, the power and flexibility of Core Audio’s <strong>Audio Unit framework</strong> give you the ability to go just about anywhere with sound. However, this power and flexibility also mean that there is a lot to learn to get started on the right foot. In this chapter, you get a bird’s-eye view of this leading edge technology, to serve you as you take the first steps toward becoming an audio unit developer.</p><p>You begin here with a quick look at the audio unit development cycle. Then, you focus in on what audio units are, and discover the important role of the <strong>Core Audio SDK</strong> in audio unit development. You learn how audio units function as plug-ins in Mac OS X and in concert with the applications that use them. Finally, you get introduced to the <em>Audio Unit Specification</em>, and how it defines the <strong>plug-in API</strong> that audio unit developers and  application developers both write to.</p><p>After reading this chapter you’ll be ready to dig in to the architectural and development details presented in <span class="content_text"><a href="../TheAudioUnit/TheAudioUnit.html#//apple_ref/doc/uid/TP40003278-CH12-SW1">“The Audio Unit.”</a></span></p><p>If you want to get your hands on building an audio unit right away, you can skip this chapter for now and go straight to <span class="content_text"><a href="../Tutorial-BuildingASimpleEffectUnitWithAGenericView/Tutorial-BuildingASimpleEffectUnitWithAGenericView.html#//apple_ref/doc/uid/TP40003278-CH5-SW4">“Tutorial: Building a Simple Effect Unit with a Generic View.”</a></span> As you build the audio unit, you can refer back to this chapter, and to other sections in this document, for conceptual information related to what you’re doing.</p>
<!-- This template is being used for both PDF and HTML. -->

    
    <h4>In this section:</h4>
    
    
    <p class="blockquote">
    
        
			
			
				<a href="AudioUnitDevelopmentFundamentals.html#//apple_ref/doc/uid/TP40003278-CH7-DontLinkElementID_8">The Audio Unit Development Cycle</a>
				
			<br/>
			
        
			
			
				<a href="AudioUnitDevelopmentFundamentals.html#//apple_ref/doc/uid/TP40003278-CH7-DontLinkElementID_9">What Is An Audio Unit?</a>
				
			<br/>
			
        
			
			
				<a href="AudioUnitDevelopmentFundamentals.html#//apple_ref/doc/uid/TP40003278-CH7-DontLinkElementID_12">Audio Units as Plug-Ins</a>
				
			<br/>
			
        
			
			
				<a href="AudioUnitDevelopmentFundamentals.html#//apple_ref/doc/uid/TP40003278-CH7-DontLinkElementID_17">Audio Units as Instances of the Model-View-Controller Design Pattern</a>
				
			<br/>
			
        
			
			
				<a href="AudioUnitDevelopmentFundamentals.html#//apple_ref/doc/uid/TP40003278-CH7-DontLinkElementID_18">Audio Units in Action</a>
				
			<br/>
			
        
			
			
				<a href="AudioUnitDevelopmentFundamentals.html#//apple_ref/doc/uid/TP40003278-CH7-DontLinkElementID_25">Audio Unit Validation and Testing</a>
				
			<br/>
			
        

    </p><br/>

<a name="//apple_ref/doc/uid/TP40003278-CH7-DontLinkElementID_8" title="The Audio Unit Development Cycle"></a><h2>The Audio Unit Development Cycle</h2><p>Audio unit development typically follows these steps:</p><ol class="ol"><li class="li"><p>Design the audio unit: specify the audio unit’s action, programmatic and user interface, and bundle configuration information.</p></li><li class="li"><p>Create and configure an appropriate Xcode project.</p></li><li class="li"><p>Implement the audio unit including parameters, factory presets, and properties—all described in the next chapter; implement copy protection, if desired; implement the synthesis, DSP, or data format conversion code.</p></li><li class="li"><p>Implement a graphical user interface—known as a custom view—if desired. Implement parameter automation support, if desired.</p></li><li class="li"><p>Validate and test the audio unit.</p></li><li class="li"><p>Deploy the audio unit bundle by packaging it in an installer or by providing installation instructions.</p></li></ol><p>As with any software development, each of these steps typically entails iteration.</p><p>The tutorial later in this document, <span class="content_text"><a href="../Tutorial-BuildingASimpleEffectUnitWithAGenericView/Tutorial-BuildingASimpleEffectUnitWithAGenericView.html#//apple_ref/doc/uid/TP40003278-CH5-SW4">“Tutorial: Building a Simple Effect Unit with a Generic View,”</a></span> leads you through most of these steps.</p><a name="//apple_ref/doc/uid/TP40003278-CH7-DontLinkElementID_9" title="What Is An Audio Unit?"></a><h2>What Is An Audio Unit?</h2><p>An <strong>audio unit</strong> (often abbreviated as <em>AU</em> in header files and elsewhere) is a Mac OS X plug-in that enhances digital audio applications such as Logic Pro and GarageBand. You can also use audio units to build audio features into your own application. Programmatically, an audio unit is packaged as a <strong>bundle</strong> and configured as a <strong>component</strong> as defined by the Mac OS X Component Manager.</p><p>At a deeper level, and depending on your viewpoint, an audio unit is one of two very different things.</p><p>From the inside—as seen by an audio unit developer—an audio unit is executable implementation code within a standard plug-in API. The API is standard so that any application designed to work with audio units will know how to use yours. The API is defined by the <em>Audio Unit Specification</em>.</p><p>An audio unit developer can add the ability for users or applications to control an audio unit in real time through the audio unit parameter mechanism. Parameters are self-describing; their values and capabilities are visible to applications that use audio units.</p><p>From the outside—as seen from an application that uses the audio unit—an audio unit is just its plug-in API. This plug-in API lets applications query an audio unit about its particular features, defined by the audio unit developer as parameters and properties.</p><p>Because of this encapsulation, how you implement an audio unit is up to you. The quickest way, the one endorsed by Apple, and the one described in this document, is to subclass the appropriate C++ superclasses of the freely-downloadable Core Audio SDK.</p><a name="//apple_ref/doc/uid/TP40003278-CH7-SW10" title="Audio Unit Programmatic Structure and Life Cycle"></a><h3>Audio Unit Programmatic Structure and Life Cycle</h3><p>The following figure represents a running audio unit built with the SDK. The figure shows the audio unit in context with its view and with an application—known as a <strong>host</strong>—that is using the audio unit:</p><br/><div><a name="//apple_ref/doc/uid/TP40003278-CH7-SW6" title="Figure 1-1A running audio unit, built using the Core Audio SDK"></a><p><strong>Figure 1-1&nbsp;&nbsp;</strong>A running audio unit, built using the Core Audio SDK</p><img src = "../Art/au_host_app.jpg" alt = "A running audio unit, built using the Core Audio SDK" ></div><br/><p>The figure shows two distinct internal parts of an audio unit bundle: the audio unit itself, on the left, and the audio unit view, on the right. The audio unit performs the audio work. The view provides a graphical user interface for the audio unit, and, if you provide it, support for parameter automation. (See <span class="content_text"><a href="AudioUnitDevelopmentFundamentals.html#//apple_ref/doc/uid/TP40003278-CH7-SW13">“Supporting Parameter Automation.”</a></span>) When you create an audio unit, you normally package both pieces in the same bundle—as you learn to do later—but they are logically separate pieces of code.</p><p>The audio unit, its view, and the host application communicate with each other by way of a notification center set up by the host application. This allows all three entities to remain synchronized. The functions for the notification center are part of the Core Audio <strong>Audio Unit Event API</strong>.</p><p>When a user first launches a host application, neither the audio unit nor its view is instantiated. In this state, none of the pieces shown in <span class="content_text">Figure 1-1</span> are present except for the host application.</p><p>The audio unit and its view come into existence, and into play, in one of two ways:</p><ul class="spaceabove"><li class="li"><p>Typically, a user indicates to a host application that they’d like to use an audio unit. For example, a user could ask the host to apply a reverb effect to a channel of audio.</p></li><li class="li"><p>For an audio unit that you supply to add a feature to your own application, the application opens the audio unit directly, probably upon application launch.</p></li></ul><p>When the host opens the audio unit, it hooks the audio unit up to the host’s audio data chain—represented in the figure by the light yellow (audio data) arrows. This hook up has two parts: providing fresh audio data to the audio unit, and retrieving processed audio data from the audio unit.</p><ul class="spaceabove"><li class="li"><p>To provide fresh audio data to an audio unit, a host defines a callback function (to be called by the audio unit) that supplies audio data one slice at a time. A <strong>slice</strong> is a number of frames of audio data. A <strong>frame</strong> is one sample of audio data across all channels.</p></li><li class="li"><p>To retrieve processed audio data from an audio unit, a host invokes an audio unit’s render method.</p></li></ul><p>Here is how the audio data flow proceeds between a host application and an audio unit:</p><ol class="ol"><li class="li"><p>The host invokes the audio unit’s render method, effectively asking the audio unit for a slice of processed audio data</p></li><li class="li"><p>The audio unit responds by calling the host’s callback function to get a slice of audio data samples to process</p></li><li class="li"><p>The audio unit processes the audio data samples and places the result in an output buffer for the host to retrieve</p></li><li class="li"><p>The host retrieves the processed data and then again invokes the audio unit’s render method</p></li></ol><p>In the depiction of the audio unit in <span class="content_text"><a href="AudioUnitDevelopmentFundamentals.html#//apple_ref/doc/uid/TP40003278-CH7-SW6">Figure 1-1</a></span>, the outer cube represents the plug-in API. Apple provides the <em>Audio Unit Specification</em> that defines the plug-in API for a variety of audio unit types. When you develop your audio unit to this specification, it will work with any host application that also follows the specification.</p><p>Inside, an audio unit contains programmatic scaffolding to connect the plug-in API to your custom code. When you use the Core Audio SDK to build your audio unit, this scaffolding is supplied in the form of glue code for the Component Manager along with a C++ class hierarchy. <span class="content_text"><a href="AudioUnitDevelopmentFundamentals.html#//apple_ref/doc/uid/TP40003278-CH7-SW6">Figure 1-1</a></span> (rather figuratively) represents your custom code as an inner cube within the audio unit, and represents the SDK’s classes and glue code as struts connecting the inner cube to the outer cube.</p><p>You can build an audio unit without using the Core Audio SDK, but doing so entails a great deal more work. Apple recommends that you use the Core Audio SDK for all but the most specialized audio unit development.</p><p>To learn about the internal architecture of an audio unit, read <span class="content_text"><a href="../TheAudioUnit/TheAudioUnit.html#//apple_ref/doc/uid/TP40003278-CH12-SW16">“Audio Unit Architecture”</a></span> in <span class="content_text"><a href="../TheAudioUnit/TheAudioUnit.html#//apple_ref/doc/uid/TP40003278-CH12-SW1">“The Audio Unit.”</a></span></p><a name="//apple_ref/doc/uid/TP40003278-CH7-DontLinkElementID_10" title="Audio Unit File Structure"></a><h3>Audio Unit File Structure</h3><p>An audio unit looks like this within the Mac OS X file system:</p><br/><div><a name="//apple_ref/doc/uid/TP40003278-CH7-SW7" title="Figure 1-2An audio unit in the Mac OS X file system"></a><p><strong>Figure 1-2&nbsp;&nbsp;</strong>An audio unit in the Mac OS X file system</p><img src = "../Art/au_file_structure_with_view.jpg" alt = "An audio unit in the Mac OS X file system" ></div><br/><p>When you build an audio unit using Xcode and a supplied audio unit template, your Xcode project takes care of packaging all these pieces appropriately.</p><p>As a component, an audio unit has the following file system characteristics:</p><ul class="spaceabove"><li class="li"><p>It is a bundle with a <code>.component</code> file name extension</p></li><li class="li"><p>It is a package; users see the bundle as opaque when they view it in the Finder</p></li></ul><p>The information property list (<code>Info.plist</code>) file within the bundle’s top-level <code>Contents</code> folder provides critical information to the system and to host applications that want to use the audio unit. For example, this file provides:</p><ul class="spaceabove"><li class="li"><p>The unique bundle identifier string in the form of a reverse domain name (or uniform type identifier). For example, for the FilterDemo audio unit provided in the Core Audio SDK, this identifier is <code>com.apple.demo.audiounit.FilterDemo</code>.</p></li><li class="li"><p>The name of the file, within the bundle, that is the audio unit proper. This file is within the <code>MacOS</code> folder in the bundle.</p></li></ul><p>An audio unit bundle can contain a custom user interface, called a view. The standard location for the view is in the audio unit bundle’s <code>Resources</code> folder. The audio unit shown in <span class="content_text">Figure 1-2</span> includes such a view, packaged as an opaque bundle itself. Looking inside the audio unit view bundle shows the view bundle file structure:</p><br/><div><a name="//apple_ref/doc/uid/TP40003278-CH7-DontLinkElementID_70" title="Figure 1-3An audio unit view in the Mac OS X file system"></a><p><strong>Figure 1-3&nbsp;&nbsp;</strong>An audio unit view in the Mac OS X file system</p><img src = "../Art/au_view_file_structure.jpg" alt = "An audio unit view in the Mac OS X file system" ></div><br/><p>When a host application opens an audio unit, it can ask the audio unit if it has a custom view. If there is one, the audio unit can respond by providing the path to the view bundle. You can put the view bundle anywhere, including a network location. Typically, however, views are packaged as shown here.</p><p>An audio unit bundle typically contains one audio unit, as described in this section. But a single audio unit bundle can contain any number of audio units. For example, Apple packages all of its audio units in one bundle, <code>System/Library/Components/CoreAudio.component</code>. The <code>CoreAudio.component</code> bundle includes a single file of executable code containing all of the Apple audio units, and another file containing all of the supplied custom views:</p><br/><div><a name="//apple_ref/doc/uid/TP40003278-CH7-DontLinkElementID_71" title="Figure 1-4The Apple audio units in the Mac OS X file system"></a><p><strong>Figure 1-4&nbsp;&nbsp;</strong>The Apple audio units in the Mac OS X file system</p><img src = "../Art/mac_os_x_au_bundle.jpg" alt = "The Apple audio units in the Mac OS X file system" ></div><br/><a name="//apple_ref/doc/uid/TP40003278-CH7-DontLinkElementID_11" title="Some Basic Terminology"></a><h3>Some Basic Terminology</h3><p>To understand this document, it’s important to understand the terms “audio unit,“ “audio unit view,“ and “audio unit bundle,“ as well as their relationships to each other.</p><ul class="spaceabove"><li class="li"><p>“Audio unit” usually refers to the executable code within the <code>MacOS</code> folder in the audio unit bundle, as shown in <span class="content_text"><a href="AudioUnitDevelopmentFundamentals.html#//apple_ref/doc/uid/TP40003278-CH7-SW7">Figure 1-2</a></span>. This is the part that performs the audio work. Sometimes, as in the title of this document, “audio unit” refers in context to the entire audio unit bundle and its contents. In this case, the term “audio unit” corresponds to a user’s view of a plug-in in the Mac OS X file system.</p></li><li class="li"><p>“Audio unit view” refers to the graphical user interface for an audio unit, as described in <span class="content_text"><a href="../TheAudioUnitView/TheAudioUnitView.html#//apple_ref/doc/uid/TP40003278-CH13-SW1">“The Audio Unit View.”</a></span> As shown in <span class="content_text">Figure 1-2</span>, the code for a custom view typically lives in its own bundle in the <code>Resources</code> folder inside the audio unit bundle. Views are optional, because the <code>AudioUnit</code> framework lets a host application create a generic view based on parameter and property code in the audio unit.</p></li><li class="li"><p>“Audio unit bundle” refers to the file system packaging that contains an audio unit and, optionally, a custom view. When this document uses “audio unit bundle,“ it is the characteristics of the packaging, such as the file name extension and the <code>Info.plist</code> file, that are important. Sometimes, as in the description of where to install audio units, “audio unit bundle” refers to the contents as well as the packaging. In this case, it’s analogous to talking about a folder while meaning the folder and its contents.</p></li></ul><a name="//apple_ref/doc/uid/TP40003278-CH7-DontLinkElementID_12" title="Audio Units as Plug-Ins"></a><h2>Audio Units as Plug-Ins</h2><p>In this section you learn about audio units from the outside in. First you take a look at using an audio unit in Apple’s AU Lab host application. From there, you see how an audio unit plays a role as a component in Mac OS X.</p><a name="//apple_ref/doc/uid/TP40003278-CH7-DontLinkElementID_13" title="The Nature of Plug-Ins"></a><h3>The Nature of Plug-Ins</h3><p>A plug-in is executable code with some special characteristics. As a library rather than a program, a plug-in cannot run by itself. Instead, a plug-in exists to provide features to host applications. For example, an audio unit could provide GarageBand with the ability to add tube-amplifier distortion to an audio signal.</p><p>Mac OS X provides two plug-in technologies: Core Foundation’s CFPlugin architecture, and the Component Manager. Audio units are Component Manager–based plug-ins. Later in this section you learn about supporting the Component Manager in your audio units.</p><p>Host applications can ship with plug-ins, in which case the plug-in’s use is transparent to a user. In other cases, a user can acquire a plug-in and explicitly add it to a running application.</p><p>What makes plug-ins special relative to other code libraries is their ability to contribute features dynamically to running host applications. You can see this in the AU Lab application, part of the Xcode Tools installation.</p><a name="//apple_ref/doc/uid/TP40003278-CH7-SW8" title="Tutorial: Using an Audio Unit in a Host Application"></a><h3>Tutorial: Using an Audio Unit in a Host Application</h3><p>This mini-tutorial illustrates the dynamic nature of plug-ins by:</p><ul class="spaceabove"><li class="li"><p>Adding an audio unit to a running host application</p></li><li class="li"><p>Using the audio unit</p></li><li class="li"><p>Removing the audio unit from the running host application</p></li></ul><p>Along the way, this tutorial shows you how to get started with the very useful AU Lab application.</p><p>1. Launch the AU Lab audio unit host application (in <code>/Developer/Applications/Audio/</code>) and create a new AU Lab document. Unless you've configured AU Lab to use a default document style, the Create New Document window opens. If AU Lab was already running, choose File > New to get this window.</p><br/><div><img src = "../Art/au_lab_new_doc_1.jpg" alt = "image: ../Art/au_lab_new_doc_1.jpg" ></div><br/><p>Ensure that the configuration matches the settings shown in the figure: Built-In Audio for the Audio Device, Line In for the Input Source, and Stereo for Output Channels. Leave the window's Inputs tab unconfigured; you will specify the input later. Click OK.</p><p>A new AU Lab window opens, showing the output channel you specified.</p><br/><div><img src = "../Art/au_lab_new_doc_2.jpg" alt = "image: ../Art/au_lab_new_doc_2.jpg" ></div><br/><p>At this point, AU Lab has already instantiated all of the available audio units on your computer, queried them to find out such things as how each can be used in combination with other audio units, and has then closed them all again.</p><p>(More precisely, the Mac OS X Component Manager has invoked the instantiation and closing of the audio units on behalf of AU Lab. <span class="content_text"><a href="AudioUnitDevelopmentFundamentals.html#//apple_ref/doc/uid/TP40003278-CH7-SW9">“Component Manager Requirements for Audio Units,”</a></span> below, explains this.)</p><p>2. In AU Lab, choose Edit > Add Audio Unit Generator. A dialog opens from the AU Lab window to let you specify the generator unit to serve as the audio source.</p><br/><div><img src = "../Art/au_lab_add_generator.jpg" alt = "image: ../Art/au_lab_add_generator.jpg" ></div><br/><p>In the dialog, ensure that the AUAudioFilePlayer generator unit is selected in the Generator pop-up. To follow this example, change the Group Name to Player. Click OK.</p><p>You can change the group name at any time by double-clicking it in the AU Lab window.</p><p>The AU Lab window now shows a stereo input track. In addition, an inspector window has opened for the generator unit. If you close the inspector, you can reopen it by clicking the rectangular "AU" button near the top of the Player track.</p><br/><div><img src = "../Art/au_lab_file_player_1.jpg" alt = "image: ../Art/au_lab_file_player_1.jpg" ></div><br/><p>3. Add one or more audio files to the Audio Files list in the player inspector window. Do this by dragging audio files from the Finder, as shown in the figure. Putting some audio files in the player inspector window lets you send audio through the AU Lab application, and through an audio unit that you add to the Player track. Just about any audio file will do. For this example, a music file works well.</p><br/><div><img src = "../Art/au_lab_copy_sound_file.jpg" alt = "image: ../Art/au_lab_copy_sound_file.jpg" ></div><br/><p>Now AU Lab is configured and ready for you to add an audio unit.</p><p>4. To dynamically add an audio unit to the AU Lab host application, click the triangular menu button in the first row of the Effects section in the Player track in AU Lab, as shown in the figure.</p><br/><div><img src = "../Art/au_lab_add_effect_1.jpg" alt = "image: ../Art/au_lab_add_effect_1.jpg" ></div><br/><p>A menu opens, listing all the audio units available on your system, arranged by category and manufacturer. AU Lab gets this list from the Component Manager, which maintains a registry of installed audio units.</p><br/><div><img src = "../Art/au_lab_add_effect_2a.jpg" alt = "image: ../Art/au_lab_add_effect_2a.jpg" ></div><br/><p>Choose an audio unit from the pop-up. To follow this example, choose the AUParametricEQ audio unit from the Apple submenu. (This audio unit, supplied as part of Mac OS X, is a single-band equalizer with controls for center frequency, gain, and Q.)</p><p>AU Lab asks the Component Manager to instantiate the audio unit you have chosen. AU Lab then initializes the audio unit. AU Lab also opens the audio unit’s Cocoa generic view, which appears as a utility window:</p><br/><div><img src = "../Art/generic_view_1a.jpg" alt = "image: ../Art/generic_view_1a.jpg" ></div><br/><p>You have now dynamically added the AUParametricEQ audio unit to the running AU Lab host application.</p><p>5. To demonstrate the features of the audio unit in AU Lab, click the Play button in the AUAudioFilePlayer inspector to send audio through the audio unit. Vary the sliders in the generic view to hear the audio unit working.</p><p>6. To remove the audio unit from the host application, once again click the triangular menu button in the first row of the Effects section in the Player track, as shown in the figure.</p><br/><div><img src = "../Art/au_lab_remove_effect_1.jpg" alt = "image: ../Art/au_lab_remove_effect_1.jpg" ></div><br/><p>From the pop-up menu, choose Remove AUParametricEQ.</p><br/><div><img src = "../Art/au_lab_remove_effect_2.jpg" alt = "image: ../Art/au_lab_remove_effect_2.jpg" ></div><br/><p>The Component Manager closes the audio unit on behalf of AU Lab. You have now dynamically removed the audio unit, and its features, from the running AU Lab host application.</p><a name="//apple_ref/doc/uid/TP40003278-CH7-DontLinkElementID_14" title="The Role of the Core Audio SDK"></a><h3>The Role of the Core Audio SDK</h3><p>When you build an audio unit using the Core Audio SDK, you get Component Manager scaffolding for free. You also get comprehensive support for most of the <em>Audio Unit Specification</em>. This lets you concentrate on the more interesting aspects of audio unit development: the audio processing and the user interface.</p><p>You create an SDK-based audio unit by subclassing the appropriate classes in the SDK’s audio unit C++ class hierarchy. <span class="content_text"><a href="../AudioUnitClassHierarchy/AudioUnitClassHierarchy.html#//apple_ref/doc/uid/TP40003278-CH9-SW2">“Appendix: Audio Unit Class Hierarchy”</a></span> shows this hierarchy. </p><p>Host applications communicate with audio units through their plug-in API and by way of the Component Manager. In all, there are six bodies of code that cooperate to support a running audio unit: </p><ul class="spaceabove"><li class="li"><p>The audio unit bundle. The bundle wraps the audio unit and its view (if you provide a custom view), and provides identification for the audio unit that lets Mac OS X and the Component Manager use the audio unit.</p></li><li class="li"><p>The audio unit itself. When you build your audio unit with the Core Audio SDK, as recommended, the audio unit inherits from the SDK’s class hierarchy.</p></li><li class="li"><p>The audio unit view.</p></li><li class="li"><p>The Core Audio API frameworks.</p></li><li class="li"><p>The Component Manager.</p></li><li class="li"><p>The host application.</p></li></ul><p>Refer to <span class="content_text"><a href="../AQuickTouroftheCoreAudioSDK/AQuickTouroftheCoreAudioSDK.html#//apple_ref/doc/uid/TP40003278-CH11-SW1">“A Quick Tour of the Core Audio SDK”</a></span> if you’d like to learn about the rest of the SDK.</p><a name="//apple_ref/doc/uid/TP40003278-CH7-SW9" title="Component Manager Requirements for Audio Units"></a><h3>Component Manager Requirements for Audio Units</h3><p>The Component Manager acts as a go-between for a host application and the audio units it uses—finding, opening, instantiating, and closing audio units on behalf of the host.</p><p>For Mac OS X to recognize your audio units, they must meet certain requirements. They must:</p><ul class="spaceabove"><li class="li"><p>Be packaged as a component, as defined by the Component Manager</p></li><li class="li"><p>Have a single entry point that the Component Manager recognizes</p></li><li class="li"><p>Have a resource (<code>.rsrc</code>) file that specifies a system wide unique identifier and version string</p></li><li class="li"><p>Respond to Component Manager calls</p></li></ul><p>Satisfying these requirements from scratch is a significant effort and requires a strong grasp of the Component Manager API. However, the Core Audio SDK insulates you from this. As demonstrated in the chapter <span class="content_text"><a href="../Tutorial-BuildingASimpleEffectUnitWithAGenericView/Tutorial-BuildingASimpleEffectUnitWithAGenericView.html#//apple_ref/doc/uid/TP40003278-CH5-SW4">“Tutorial: Building a Simple Effect Unit with a Generic View,”</a></span> accommodating the Component Manager requires very little work when you use the SDK.</p><a name="//apple_ref/doc/uid/TP40003278-CH7-SW2" title="Audio Unit Installation and Registration"></a><h4>Audio Unit Installation and Registration</h4><p>The Mac OS X Component Manager looks for audio units in some specific locations, one of which is reserved for use by Apple.</p><p>When you install your audio units during development or deployment, you typically put them in one of the following two locations:</p><ul class="spaceabove"><li class="li"><p><code>~/Library/Audio/Plug-Ins/Components/</code></p><p>Audio units installed here can be used only by the owner of the home folder</p></li><li class="li"><p><code>/Library/Audio/Plug-Ins/Components/</code></p><p>Audio units installed here can be used by all users on the computer</p></li></ul><p>It is up to you which of these locations you use or recommend to your users.</p><p>The Mac OS X preinstalled audio units go in a location reserved for Apple’s use:</p><div class="codesample"><table><tr><td scope="row"><pre>/System/Library/Components/<span></span></pre></td></tr></table></div>	<p>The Component Manager maintains a cached registry of the audio units in these locations (along with any other plug-ins it finds in other standard locations). Only registered audio units are available to host applications. The Component Manager refreshes the registry on system boot, on user log-in, and whenever the modification timestamp of one of the three <code>Components</code> folders changes.</p><p>A host application can explicitly register audio units installed in arbitrary locations by using the Component Manager’s <code><a href="../../../../Carbon/Reference/Component_Manager/Reference/reference.html#//apple_ref/doc/c_ref/RegisterComponent" target="_top">RegisterComponent</a></code>, <code><a href="../../../../Carbon/Reference/Component_Manager/Reference/reference.html#//apple_ref/doc/c_ref/RegisterComponentResource" target="_top">RegisterComponentResource</a></code>, or <code><a href="../../../../Carbon/Reference/Component_Manager/Reference/reference.html#//apple_ref/doc/c_ref/RegisterComponentResourceFile" target="_top">RegisterComponentResourceFile</a></code> functions. Audio units registered in this way are available only to the host application that invokes the registration. This lets you use audio units to add features to a host application you are developing, without making your audio units available to other hosts.</p><a name="//apple_ref/doc/uid/TP40003278-CH7-SW1" title="Audio Unit Identification"></a><h4>Audio Unit Identification</h4><p>Every audio unit on a system must have a unique signature. The <em>Audio Unit Specification</em> takes advantage of this to let host applications know the plug-in API for any audio unit, based on its signature. This section describes how this works.</p><p>The Component Manager identifies audio units by a triplet of four-character codes:</p><ul class="spaceabove"><li class="li"><p>The “type” specifies the general type of functionality provided by an audio unit. In so doing, the type also identifies the audio unit’s plug-in API. In this way, the type code is programmatically significant. For example, a host application knows that any audio unit of type <code>'aufx'</code> (for “audio unit effect”) provides DSP functionality.</p><p>The <em>Audio Unit Specification</em> specifies the available type codes for audio units, as well as the plug-in API for each audio unit type.</p></li><li class="li"><p>The “subtype” describes more precisely what an audio unit does, but is not programmatically significant for audio units.</p><p>For example, Mac OS X includes an effect unit of subtype <code>'lpas'</code>, named to suggest that it provides low-pass filtering. If, for your audio unit, you use one of the subtypes listed in the <code>AUComponent.h</code> header file in the Audio Unit framework (such as <code>'lpas'</code>), you are suggesting to users of your audio unit that it behaves like the named subtype. However, host applications make no assumptions about your audio unit based on its subtype. You are free to use any subtype code, including subtypes named with only lowercase letters.</p></li><li class="li"><p>The “manufacturer code” identifies the developer of an audio unit.</p><p>Apple expects each developer to register a manufacturer code, as a “creator code,“ on the <span class="content_text"><a href="http://developer.apple.com/datatype/" target="_top">Data Type Registration</a></span> page. Manufacturer codes must contain at least one uppercase character. Once registered, you can use the same manufacturer code for all your audio units.</p></li></ul><p>In addition to these four-character codes, each audio unit must specify a correctly formatted version number. When the Component Manager registers audio units, it picks the most recent version if more than one is present on a system.</p><p>As a component, an audio unit identifies its version as an eight-digit hexadecimal number in its resource (<code>.rsrc</code>) file. As you’ll see in <span class="content_text"><a href="../Tutorial-BuildingASimpleEffectUnitWithAGenericView/Tutorial-BuildingASimpleEffectUnitWithAGenericView.html#//apple_ref/doc/uid/TP40003278-CH5-SW4">“Tutorial: Building a Simple Effect Unit with a Generic View,”</a></span> you specify this information using Xcode.</p><p>Here is an example of how to construct a version number. It uses an artificially large number to illustrate the format unambiguously. For a decimal version number of <code>29.33.40</code>, the hexadecimal equivalent is <code>0x001d2128</code>. The format works as follows for this number:</p><br/><div><a name="//apple_ref/doc/uid/TP40003278-CH7-DontLinkElementID_72" title="Figure 1-5Constructing an audio unit version number"></a><p><strong>Figure 1-5&nbsp;&nbsp;</strong>Constructing an audio unit version number</p><img src = "../Art/au_hex_conversion.jpg" alt = "Constructing an audio unit version number" ></div><br/><p>The four most significant hexadecimal digits represent the major version number. The next two represent the minor version number. The two least significant digits represent the dot release number.</p><p>When you release a new version of an audio unit, you must ensure that its version number has a higher value than the previous version—not equal to the previous version, and not lower. Otherwise, users who have a previous version of your audio unit installed won’t be able to use the new version.</p><a name="//apple_ref/doc/uid/TP40003278-CH7-DontLinkElementID_15" title="Plug-in API Requirements for Audio Units"></a><h3>Plug-in API Requirements for Audio Units</h3><p>Audio units are typed, as described above in <span class="content_text"><a href="AudioUnitDevelopmentFundamentals.html#//apple_ref/doc/uid/TP40003278-CH7-SW1">“Audio Unit Identification.”</a></span> When a host application sees an audio unit’s type, it knows how to communicate with it.</p><p>Implementing the plug-in API for any given type of audio unit from scratch is a significant effort. It requires a strong grasp of the Audio Unit and Audio Toolbox framework APIs and of the <em>Audio Unit Specification</em>. However, the Core Audio SDK insulates you from much of this as well. Using the SDK, you need to implement only those methods and properties that are relevant to your audio unit. (You learn about the audio unit property mechanism in the next chapter, <span class="content_text"><a href="../TheAudioUnit/TheAudioUnit.html#//apple_ref/doc/uid/TP40003278-CH12-SW1">“The Audio Unit.”</a></span>)</p><a name="//apple_ref/doc/uid/TP40003278-CH7-DontLinkElementID_16" title="The Audio Unit Specification"></a><h4>The Audio Unit Specification</h4><p>The <em>Audio Unit Specification</em> defines the common interface that audio unit developers and host application developers must support.</p><div class="notebox"><a name="//apple_ref/doc/uid/TP40003278-CH7-DontLinkElementID_73" title="Note"></a><p><strong>Note:</strong>&nbsp;The <em>Audio Unit Specification</em> document is currently in development. The following header files contain information relevant to the <em>Audio Unit Specification</em>: <code>AUComponent.h</code>, <code>AudioUnitProperties.h</code>, <code>MusicDevice.h</code>, and <code>OutputUnit.h</code>.</p>In addition, the following tutorial files contain information relevant to the Audio Unit Specification: <code>AUPannerUnits.text</code>, <code>OfflineRendering.rtf</code>, and <code>OfflineAPIAdditions.text</code>.</p></div><p>The <em>Audio Unit Specification</em> describes:</p><ul class="spaceabove"><li class="li"><p>The various Apple types defined for audio units, as listed in the “AudioUnit component types and subtypes” enumeration in the <code>AUComponent.h</code> header file in the Audio Unit framework</p></li><li class="li"><p>The functional and behavioral requirements for each type of audio unit</p></li><li class="li"><p>The plug-in API for each type of audio unit, including required and optional properties</p></li></ul><p>You develop your audio units to conform to the <em>Audio Unit Specification</em>. You then test this conformance with the <code>auval</code> command-line tool, described in the next section.</p><p>The <em>Audio Unit Specification</em> defines the plug-in API for the following audio unit types:</p><ul class="spaceabove"><li class="li"><p>Effect units (<code>'aufx'</code>), such as volume controls, equalizers, and reverbs, which modify an audio data stream</p></li><li class="li"><p>Music effect units (<code>'aumf'</code>), such as loopers, which combine features of instrument units (such as starting and stopping a sample) with features of effect units</p></li><li class="li"><p>Offline effect units (<code>'auol'</code>), which let you do things with audio that aren’t practical in real time, such as time reversal or look-ahead level normalization</p></li><li class="li"><p>Instrument units (<code>'aumu'</code>), which take MIDI and soundbank data as input and provide audio data as output—letting a user play a virtual instrument</p></li><li class="li"><p>Generator units (<code>'augn'</code>), which programmatically generate an audio data stream or play audio from a file</p></li><li class="li"><p>Data format converter units  (<code>'aufc'</code>), which change characteristics of an audio data stream such as bit depth, sample rate, or playback speed</p></li><li class="li"><p>Mixer units (<code>'aumx'</code>), which combine audio data streams</p></li><li class="li"><p>Panner units (<code>'aupn'</code>), which distribute a set of input channels, using a spatialization algorithm, to a set of output channels</p></li></ul><a name="//apple_ref/doc/uid/TP40003278-CH7-DontLinkElementID_17" title="Audio Units as Instances of the Model-View-Controller Design Pattern"></a><h2>Audio Units as Instances of the Model-View-Controller Design Pattern</h2><p>Apple’s Core Audio team designed the Audio Unit technology around one of the more popular software design patterns, the Model-View-Controller, or MVC. See <span class="content_text"><!--a target="_top" -->The Model-View-Controller Design Pattern<!--/a--></span> for more about this pattern.</p><p>Keep the MVC pattern in mind as you build your audio units:</p><ul class="ul"><li class="li"><p>The audio unit serves as the model, encapsulating all of the knowledge to perform the audio work</p></li><li class="li"><p>The audio unit’s view serves, naturally, as the view, displaying the audio unit’s current settings and allowing a user to change them</p></li><li class="li"><p>The Audio Unit Event API, and the code in an audio unit and its view that calls this API, corresponds to the controller, supporting communication between the audio unit, its view, and a host application</p></li></ul><a name="//apple_ref/doc/uid/TP40003278-CH7-DontLinkElementID_18" title="Audio Units in Action"></a><h2>Audio Units in Action</h2><a name="//apple_ref/doc/uid/TP40003278-CH7-DontLinkElementID_19" title="Opening and Closing Audio Units"></a><h3>Opening and Closing Audio Units</h3><p>Host applications are responsible—with the help of the Component Manager—for finding, opening, and closing audio units. Audio units, in turn, need to be findable, openable, and closable. Your audio unit gets these attributes when you build it from the Core Audio SDK and use the Xcode audio unit templates.</p><p>There is a two-step sequence for an audio unit becoming available for use in a host. These two steps are opening and initializing. Opening an audio unit amounts to instantiating an object of the audio unit’s main class. Initializing amounts to allocating resources so the audio unit is ready to do work.</p><p>To be a well-behaved, host-friendly plug-in, your audio unit’s instantiation must be fast and lightweight. Resource intensive startup work for an audio unit goes into the initialization step. For example, an instrument unit that employs a large bank of sample data should load it on initialization, not instantiation.</p><p>For more on finding, opening, and closing from your perspective as an audio unit developer, see <span class="content_text"><a href="../TheAudioUnit/TheAudioUnit.html#//apple_ref/doc/uid/TP40003278-CH12-SW3">“Audio Unit Initialization and Uninitialization”</a></span> and <span class="content_text"><a href="../TheAudioUnit/TheAudioUnit.html#//apple_ref/doc/uid/TP40003278-CH12-SW4">“Closing”</a></span> in <span class="content_text"><a href="../TheAudioUnit/TheAudioUnit.html#//apple_ref/doc/uid/TP40003278-CH12-SW1">“The Audio Unit.”</a></span></p><a name="//apple_ref/doc/uid/TP40003278-CH7-DontLinkElementID_20" title="Adding Copy Protection"></a><h4>Adding Copy Protection</h4><p>If you choose to add copy protection to your audio unit, it’s especially important to consider the audio unit’s opening sequence. The time for copy protection is during audio unit initialization—not instantiation. Therefore, you put copy protection code into an override of the <code>Initialize</code> method from the SDK’s <code>AUBase</code> superclass. You do not put copy protection code into an audio unit’s constructor.</p><p>Here is a scenario where this matters. Suppose a user doesn’t have the required hardware dongle for your (copy protected) audio unit. Perhaps he left it at home when he brought his laptop to a performance. If your audio unit invokes its copy protection on instantiation, this could prevent a host application from opening. If your audio unit invokes its copy protection on initialization, as recommended, the performer could at least use the host application.</p><a name="//apple_ref/doc/uid/TP40003278-CH7-DontLinkElementID_21" title="Multiple Instantiation"></a><h4>Multiple Instantiation</h4><p>An audio unit can be instantiated any number of times by a host application and by any number of hosts. More precisely, the Component Manager invokes audio unit instantiation on behalf of host applications. The Component Manager infrastructure ensures that each audio unit instance exists and behaves independently.</p><p>You can demonstrate multiple instantiation in AU Lab. First add one instance of the AUParametricEQ effect unit to an AU Lab document, as described above in <span class="content_text"><a href="AudioUnitDevelopmentFundamentals.html#//apple_ref/doc/uid/TP40003278-CH7-SW8">“Tutorial: Using an Audio Unit in a Host Application.”</a></span> Then invoke the pop-up menus in additional rows of the Effects section in the Player track. You can add as many one-band parametric equalizers to the track as you like. Each of these instances of the audio unit behaves independently, as you can see by the varied settings in the figure:</p><br/><div><a name="//apple_ref/doc/uid/TP40003278-CH7-DontLinkElementID_74" title="Figure 1-6Multiple instantiation of audio units in AU Lab"></a><p><strong>Figure 1-6&nbsp;&nbsp;</strong>Multiple instantiation of audio units in AU Lab</p><img src = "../Art/multiple_instantiation.jpg" alt = "Multiple instantiation of audio units in AU Lab" ></div><br/><a name="//apple_ref/doc/uid/TP40003278-CH7-SW12" title="Audio Processing Graphs and the Pull Model"></a><h3>Audio Processing Graphs and the Pull Model</h3><p>Host applications can connect audio units to each other so that the output of one audio unit feeds the input of the next. Such an interconnected series of audio units is called an <strong>audio processing graph</strong>. In the context of a graph, each connected audio unit is called a <strong>node</strong>.</p><p>When you worked through the <span class="content_text"><a href="AudioUnitDevelopmentFundamentals.html#//apple_ref/doc/uid/TP40003278-CH7-SW8">“Tutorial: Using an Audio Unit in a Host Application”</a></span> section earlier in this chapter, the AU Lab application constructed an audio processing graph for you. This graph consisted of the AUAudioFilePlayer generator unit, the AUParametricEQ effect unit, and finally (not represented in the user interface of AU Lab) the Apple-supplied AUHAL I/O unit that interfaces with external hardware such as loudspeakers.</p><p><span class="content_text"><a href="../TheAudioUnit/TheAudioUnit.html#//apple_ref/doc/uid/TP40003278-CH12-SW17">“Audio Processing Graph Connections”</a></span> provides details on how these connections work.</p><p>The Audio Processing Graph API, declared in the Audio Toolbox framework, provides interfaces for assisting host applications to create and manage audio processing graphs. When a host application employs this API, it uses an opaque data type called a <strong>graph object</strong> (of type AUGraph).</p><p>Some applications, such as AU Lab, always use graph objects when interconnecting audio units. Others, like Logic, connect audio units to each other directly. An individual audio unit, however, is not aware whether its connections are managed by a graph object on behalf of a host application, or by a host directly.</p><!-- <Para>This seems like too much information; not relevant to audio unit developers (although very relevant to host application developers). Audio processing graphs are managed by an opaque data type called a <bold>graph object</bold>, of type <codeVoice>AUGraph</codeVoice>. Graph objects, and their associated functions, are declared in Audio Processing Graph Services in the <codeVoice>AUGraph.h</codeVoice> header file. In the Audio Processing Graph Services API, each connected audio unit is called a node.</Para><Para>Probably no longer needed, now that I’ve written the “When you worked through the….“ paragraph: Use of even a single audio unit typically involves an audio processing graph. For example, a host can send an effect unit’s output through an output unit. An output unit can interface with the file system to record audio data to disk, or can interface to external hardware to play audio over speakers.</Para> --><p>Audio data flow in graphs proceeds from the first (input) to last (output) node, as you’d expect. Control, however, flows from the last node back to the first. In Core Audio, this is called the <strong>pull model</strong>. The host application is in charge of invoking the pull.</p><p>You can think of the pull model in terms of a straw in a glass of water. The water in the glass represents fresh audio data waiting to be processed. The straw represents an audio processing graph, or even a single audio unit. Acting as a host application, you begin the flow of audio data by “pulling” (sipping) on the end of the straw. Specifically, a host application initiates the flow of audio data by calling the rendering method of the final node in a graph. Each sip that you take through the straw corresponds to another pull of a slice of audio data frames—another call to the final node’s rendering method.</p><p>Before audio or control flow can start, the host application performs the work to hook up audio units to each other. In a case such as the one shown in the figure, the host also makes connections to and from the audio processing graph. But hosts do not necessarily feed audio data to graphs that they use. In a case where the first audio unit in a graph is a generator unit, there is no input connection; the generator provides audio data algorithmically or by playing a file.</p><p><span class="content_text">Figure 1-7</span> shows the pull model in the particular case of a host application employing two effect units in sequence.</p><br/><div><a name="//apple_ref/doc/uid/TP40003278-CH7-SW14" title="Figure 1-7The pull model in action with two effect units"></a><p><strong>Figure 1-7&nbsp;&nbsp;</strong>The pull model in action with two effect units</p><img src = "../Art/pull_model.jpg" alt = "The pull model in action with two effect units" ></div><br/><p>Here is how the pull proceeds in <span class="content_text">Figure 1-7</span>:</p><ol class="ol"><li class="li"><p>The host application calls the render method of the final node (effect unit B) in the graph, asking for one slice worth of processed audio data frames.</p></li><li class="li"><p>The render method of effect unit B looks in its input buffers for audio data to process, to satisfy the call to render. If there is audio data waiting to be processed, effect unit B uses it. Otherwise, and as shown in the figure, effect unit B (employing a superclass in the SDK’s audio unit class hierarchy) calls the render method of whatever the host has connected to effect unit B’s inputs. In this example, effect unit A is connected to B’s inputs—so effect unit B pulls on effect unit A, asking for a slice audio data frames.</p></li><li class="li"><p>Effect unit A behaves just as effect unit B does. When it needs audio data, it gets it from its input connection, which was also established by the host. The host connected effect unit A’s inputs to a render callback in the host. Effect unit A pulls on the host’s render callback.</p></li><li class="li"><p>The host’s render callback supplies the requested audio data frames to effect unit A.</p></li><li class="li"><p>Effect unit A processes the slice of data supplied by the host. Effect unit A then supplies the processed audio data frames that were previously requested (in step 2) to effect unit B.</p></li><li class="li"><p>Effect unit B processes the slice of data provided by effect unit A. Effect unit B then supplies the processed audio data frames that were originally requested (in step 1) to the host application. This completes one cycle of pull.</p></li></ol><!-- <Para>A host can also use an audio unit directly, without a graph, as mentioned above in <xName DestinationChapterID="CH7" Id="SW10" targetElementType="Section">“Audio Unit Programmatic Structure and Life Cycle”</xName>:</Para><List-Bullet><Item><Para>To send audio to an audio unit, the host establishes what is known as a <bold>render callback function</bold> (or “render callback” for short). When the audio unit needs another slice of audio data frames to process, it calls the hosts render callback, which in turn supplies the frames.</Para></Item><Item><Para>To get processed audio data out of an audio unit, the host calls the audio unit’s rendering code directly. The audio unit then performs a rendering cycle and places processed audio data in output buffers for the host application to retrieve.</Para></Item></List-Bullet><Para>This sentence seems out of place. Why would a reader need to know this now?: No matter how a host uses an audio unit—directly, by way of a render callback; or indirectly, in the context of an audio processing graph—the host employs the audio unit property mechanism, described in the next chapter, for passing information to and from the audio unit.</Para> --><p>Audio units normally do not know whether their inputs and outputs are connected to other audio units, or to host applications, or to something else. Audio units simply respond to rendering calls. Hosts are in charge of establishing connections, and superclasses (for audio units built with the Core Audio SDK) take care of implementing the pull.</p><p>As an audio unit developer, you don’t need to work directly with audio processing graphs except to ensure that your audio unit plays well with them. You do this, in part, by ensuring that your audio unit passes Apple’s validation test, described in <span class="content_text"><a href="AudioUnitDevelopmentFundamentals.html#//apple_ref/doc/uid/TP40003278-CH7-SW3">“Audio Unit Validation with the auval Tool.”</a></span> You should also perform testing by hooking up your audio unit in various processing graphs using host applications, as described in <span class="content_text"><a href="AudioUnitDevelopmentFundamentals.html#//apple_ref/doc/uid/TP40003278-CH7-SW4">“Audio Unit Testing and Host Applications.”</a></span></p><a name="//apple_ref/doc/uid/TP40003278-CH7-SW11" title="Processing: The Heart of the Matter"></a><h3>Processing: The Heart of the Matter</h3><p>Audio units process audio data, of course. They also need to know how to stop processing gracefully, and how to modify their processing based on user adjustments. This section briefly discusses these things. <span class="content_text"><a href="../TheAudioUnit/TheAudioUnit.html#//apple_ref/doc/uid/TP40003278-CH12-SW1">“The Audio Unit”</a></span> describes processing in greater detail.</p><a name="//apple_ref/doc/uid/TP40003278-CH7-DontLinkElementID_22" title="Processing"></a><h4>Processing</h4><p>An audio unit that processes audio data, such as an effect unit, works in terms of rendering cycles. In each rendering cycle, the audio unit:</p><ul class="spaceabove"><li class="li"><p>Gets a slice of fresh audio data frames to process. It does this by calling the rendering callback function that has been registered in the audio unit.</p></li><li class="li"><p>Processes the audio data frames.</p></li><li class="li"><p>Puts the resulting audio data frames into the audio unit’s output buffers.</p></li></ul><p>An audio unit does this work at the beck and call of its host application. The host application also sets number of audio data frames per slice. For example, AU Lab uses 512 frames per slice as a default, and you can vary this number from 24 to 4,096. See <span class="content_text"><a href="AudioUnitDevelopmentFundamentals.html#//apple_ref/doc/uid/TP40003278-CH7-SW15">“Testing with AU Lab.”</a></span></p><p>The programmatic call to render the next slice of frames can arrive from either of two places:</p><ul class="spaceabove"><li class="li"><p>From the host application itself, in the case of the host using the audio unit directly</p></li><li class="li"><p>From the downstream neighbor of the audio unit, in the case of the audio unit being part of an audio processing graph</p></li></ul><p>Audio units behave exactly the same way regardless of the calling context—that is, regardless of whether it is a host application or a downstream audio unit asking for audio data.</p><a name="//apple_ref/doc/uid/TP40003278-CH7-DontLinkElementID_23" title="Resetting"></a><h4>Resetting</h4><p>Audio units also need to be able to gracefully stop rendering. For example, an audio unit that implements an IIR filter uses an internal buffer of samples. It uses the values of these buffered samples when applying a frequency curve to the samples it is processing. Say that a user of such an audio unit stops playing an audio file and then starts again at a different point in the file. The audio unit, in this case, must start with an empty processing buffer to avoid inducing artifacts.</p><p>When you develop an audio unit’s DSP code, you implement a <code><a href="../../../Reference/CoreAudio/audiocodec/audiocodec.html#//apple_ref/doc/c_ref/Reset" target="_top">Reset</a></code> method to return the DSP state of the audio unit to what it was when the audio unit was first initialized. Host applications call the <code><a href="../../../Reference/CoreAudio/audiocodec/audiocodec.html#//apple_ref/doc/c_ref/Reset" target="_top">Reset</a></code> method as needed.</p><a name="//apple_ref/doc/uid/TP40003278-CH7-DontLinkElementID_24" title="Adjustments While Rendering"></a><h4>Adjustments While Rendering</h4><p>While an audio unit is rendering, a user can adjust the rendering behavior using the audio unit’s view. For example, in a parametric filter audio unit, a user can adjust the center frequency. It’s also possible for host applications to alter rendering using parameter automation, described in the next section.</p><a name="//apple_ref/doc/uid/TP40003278-CH7-SW13" title="Supporting Parameter Automation"></a><h3>Supporting Parameter Automation</h3><p>Parameters let users adjust audio units. For instance, Apple’s low-pass filter audio unit has parameters for cut-off frequency and resonance.</p><p><strong>Parameter automation</strong> lets users program parameter adjustments along a time line. For example, a user might want to use a low pass filter audio unit to provide an effect like a guitar wah-wah pedal. With parameter automation, the user could record the wah-wah effect and make it part of a musical composition. The host application records the manual changes along with synchronization information, tying the changes to time markers for an audio track. The host can then play back the parameter changes to provide automated control of the audio unit.</p><p>Host applications can also provide the ability for a user to indirectly specify parameter manipulation. For example, a host could let a user draw a gain or panning curve along an audio track’s waveform representation. The host could then translate such graphical input into parameter automation data.</p><p>Parameter automation relies on three things:</p><ul class="spaceabove"><li class="li"><p>The ability of an audio unit to change its parameter values programmatically on request from a host application</p></li><li class="li"><p>The ability of an audio unit view to post notifications as parameter values are changed by a user</p></li><li class="li"><p>The ability of a host application to support recording and playback of parameter automation data</p></li></ul><p>Some hosts that support parameter automation with audio units are Logic Pro, Ableton Live, and Sagan Metro.</p><p>Parameter automation uses the Audio Unit Event API, declared in the <code>AudioUnitUtilties.h</code> header file as part of the Audio Toolbox framework. This thread-safe API provides a notification mechanism that supports keeping audio units, their views, and hosts in sync.</p><p>To support parameter automation in your audio unit, you must create a custom view. You add automation support to the view’s executable code, making use of the Audio Unit Event API to support some or all of the following event types:</p><ul class="spaceabove"><li class="li"><p>Parameter <strong>gestures</strong>, which include the <code>kAudioUnitEvent_BeginParameterChangeGesture</code> and <code>kAudioUnitEvent_EndParameterChangeGesture</code> event types</p></li><li class="li"><p>Parameter value changes, identified by the <code>kAudioUnitEvent_ParameterValueChange</code> event type</p></li><li class="li"><p>Property changes, identified by the <code>kAudioUnitEvent_PropertyChange</code> event type</p></li></ul><p>In some unusual cases you may need to add support for parameter automation to the audio unit itself. For example, you may create a bandpass filter with adjustable upper and lower corner frequencies. Your audio unit then needs to ensure that the upper frequency is never set below the lower frequency. When an audio unit invokes a parameter change in a case like this, it needs to issue a parameter change notification.</p><p><span class="content_text"><a href="../TheAudioUnitView/TheAudioUnitView.html#//apple_ref/doc/uid/TP40003278-CH13-SW1">“The Audio Unit View”</a></span> and <span class="content_text"><a href="../TheAudioUnit/TheAudioUnit.html#//apple_ref/doc/uid/TP40003278-CH12-SW15">“Defining and Using Parameters”</a></span> give more information on parameter automation.</p><a name="//apple_ref/doc/uid/TP40003278-CH7-DontLinkElementID_25" title="Audio Unit Validation and Testing"></a><h2>Audio Unit Validation and Testing</h2><a name="//apple_ref/doc/uid/TP40003278-CH7-SW3" title="Audio Unit Validation with the auval Tool"></a><h3>Audio Unit Validation with the auval Tool</h3><p>Apple strongly recommends validating your audio units using the <code>auval</code> command-line tool during development. The <code>auval</code> tool (named as a contraction of "audio unit validation") comes with Mac OS X. It performs a comprehensive suite of tests on:</p><ul class="spaceabove"><li class="li"><p>An audio unit’s plug-in API, as defined by its programmatic type</p></li><li class="li"><p>An audio unit’s basic functionality including such things as which audio data channel configurations are available, time required to instantiate the audio unit, and the ability of the audio unit to render audio</p></li></ul><p>The <code>auval</code> tool tests only an audio unit proper. It does not test any of the following:</p><ul class="spaceabove"><li class="li"><p>Audio unit views</p></li><li class="li"><p>Audio unit architecture, in terms of using the recommended model-view-controller design pattern for separation of concerns</p></li><li class="li"><p>Correct use of the Audio Unit Event API</p></li><li class="li"><p>Quality of DSP, quality of audio generation, or quality of audio data format conversion</p></li></ul><p>The <code>auval</code> tool can validate every type of audio unit defined by Apple. When you run it, it outputs a test log and summarizes the results with a “pass” or "fail” indication.</p><p>For more information,  refer to the <code>auval</code> built-in help system. To see <code>auval</code> help text, enter the following command at a prompt in the Terminal application:</p><div class="codesample"><table><tr><td scope="row"><pre>auval -h<span></span></pre></td></tr></table></div><a name="//apple_ref/doc/uid/TP40003278-CH7-SW4" title="Audio Unit Testing and Host Applications"></a><h3>Audio Unit Testing and Host Applications</h3><p>When you build to the <em>Audio Unit Specification</em>, you’ve done the right thing. Such an audio unit should work with all hosts. But practically speaking, development isn’t complete until  you test your audio units in commercial applications. The reasons include:</p><ul class="spaceabove"><li class="li"><p>Evolution of the Core Audio frameworks and SDK</p></li><li class="li"><p>Variations across host application versions</p></li><li class="li"><p>Idiosyncrasies in the implementation of some host applications</p></li></ul><p>As host applications that recognize audio units proliferate, the task of testing your audio unit in all potential hosts becomes more involved.</p><p>The situation is somewhat analogous to testing a website in various browsers: your code may perfectly fit the relevant specifications, but nonconformance in one or another browser requires you to compensate.</p><p>With this in mind, the following sections provide an overview of host-based audio unit testing.</p><a name="//apple_ref/doc/uid/TP40003278-CH7-SW15" title="Testing with AU Lab"></a><h4>Testing with AU Lab</h4><p>AU Lab, the application you used in <span class="content_text">“Tutorial: Using an Audio Unit in a Host Application,”</span> is the reference audio unit host. It is in active development by Apple’s Core Audio team. They keep it in sync with the <code>auval</code> tool, with the Core Audio frameworks and SDK, and with Mac OS X itself. This makes AU Lab the first place to test your audio units.</p><a name="//apple_ref/doc/uid/TP40003278-CH7-DontLinkElementID_26" title="What You Can Test with AU Lab"></a><h5>What You Can Test with AU Lab</h5><p>Testing your audio unit with AU Lab lets you test:</p><ul class="spaceabove"><li class="li"><p>Behavior, in terms of being found by a host, displayed in a menu, and opened</p></li><li class="li"><p>View, both generic and custom</p></li><li class="li"><p>Audible performance</p></li><li class="li"><p>Interaction with other audio units when placed in an audio processing graph</p></li><li class="li"><p>I/O capabilities, such as sidechains and multiple outputs, as well as basic testing of monaural and stereophonic operation</p></li></ul><p>In Mac OS X v10.4 “Tiger,” AU Lab lets you test the following types of audio units:</p><ul class="spaceabove"><li class="li"><p>Converter units</p></li><li class="li"><p>Effect units</p></li><li class="li"><p>Generator units</p></li><li class="li"><p>Instrument units</p></li></ul><a name="//apple_ref/doc/uid/TP40003278-CH7-DontLinkElementID_27" title="Varying the Host Application&acirc;&#128;&#153;s Characteristics"></a><h5>Varying the Host Application’s Characteristics</h5><p>AU Lab lets you control some of its hosting characteristics, which lets you test the behavior of your audio unit under varying conditions. For example, you can change the number of frames of audio data to process in each rendering cycle. You do this using Devices Preferences.</p><p>In AU Lab, choose Preferences from the AU Lab menu. Click Devices to show Devices Preferences:</p><br/><div><img src = "../Art/au_lab_prefs_1.jpg" alt = "image: ../Art/au_lab_prefs_1.jpg" ></div><br/><p>Click the Frames pop-up menu. You can choose the number of frames for your audio unit to process in each rendering cycle:</p><br/><div><img src = "../Art/au_lab_prefs_2.jpg" alt = "image: ../Art/au_lab_prefs_2.jpg" ></div><br/><p>Click the disclosure triangle for Expert Settings. You can vary the slider to choose the percentage of CPU time to devote to audio processing. This lets you test the behavior of your audio unit under varying load conditions:</p><br/><div><img src = "../Art/au_lab_prefs_3.jpg" alt = "image: ../Art/au_lab_prefs_3.jpg" ></div><br/><a name="//apple_ref/doc/uid/TP40003278-CH7-DontLinkElementID_28" title="Custom Testing of Audio Units"></a><h4>Custom Testing of Audio Units</h4><p>As an audio unit developer, you'll want to stay up to date with the host applications your target market is using. Apple recommends that you test your audio units with, at least, Apple’s suite of professional host applications:</p><ul class="spaceabove"><li class="li"><p>GarageBand</p></li><li class="li"><p>Logic Pro</p></li><li class="li"><p>Soundtrack Pro</p></li><li class="li"><p>Final Cut Pro</p></li></ul><p>There are many third-party and open source applications that support audio units, among them Ableton Live, Amadeus, Audacity, Cubase, Digital Performer, DSP-Quattro, Peak, Rax, and Metro. <!-- Refer to the “See Also” section in the <xName DestinationChapterID="CH1" Id="SW2" targetElementType="Chapter">“Introduction”</xName> for more information on host applications. --></p>

        <br /><br /> 
        
        <div class="mini_nav_text" align="left">
        <span class="navButtons">
        <a href="../Introduction/Introduction.html">&lt; Previous Page</a><span style="margin-left: 8px"><a href="../TheAudioUnit/TheAudioUnit.html">Next Page &gt;</a></span>
        </span>
        <span id="showHideTOCLowerSpan">
        <a href="#" onclick="showHideTOC();"><img src="../../../../Resources/Images/show_toc_icon.gif" width="15" height="14" border="0" style="margin-bottom: -2px;" alt="" /></a> <a href="#" onclick="showHideTOC();">Hide TOC</a>
        </span>
        </div>

        <br/><hr /><div align="center"><p class="content_text" lang="en" dir="ltr"> <!--#if expr="0=1" -->&#x00a9; 2007 Apple Inc. All Rights Reserved. &#40;<!--#endif -->Last updated: 2007-10-31<!--#if expr="0=1" -->&#041;<!--#endif --></p></div>

        
        <div class="hideOnPrint hideInXcode">
        <!-- start of footer -->
        	<table width="100%" border="0" cellpadding="0" cellspacing="0">
		<tr>
			<td><div style="width: 100%; height: 1px; background-color: #919699; margin-top: 5px; margin-bottom: 15px"></div></td>
		</tr>
		<tr>
			<td align="center"><br/>
				<table border="0" cellpadding="0" cellspacing="0" class="graybox">
					<tr>
						<th>Did this document help you?</th>
					</tr>
					<tr>
						<td>
						    <div style="margin-bottom: 8px"><a href="http://developer.apple.com/feedback/?v=1&url=/documentation/MusicAudio/Conceptual/AudioUnitProgrammingGuide/AudioUnitDevelopmentFundamentals/AudioUnitDevelopmentFundamentals.html%3Fid%3DTP40003278-1.2&media=dvd" target=_new>Yes</a>:  Tell us what works for you.</div>
							<div style="margin-bottom: 8px"><a href="http://developer.apple.com/feedback/?v=2&url=/documentation/MusicAudio/Conceptual/AudioUnitProgrammingGuide/AudioUnitDevelopmentFundamentals/AudioUnitDevelopmentFundamentals.html%3Fid%3DTP40003278-1.2&media=dvd" target=_new>It&#8217;s good, but:</a> Report typos, inaccuracies, and so forth.</div>
							<div><a href="http://developer.apple.com/feedback/?v=3&url=/documentation/MusicAudio/Conceptual/AudioUnitProgrammingGuide/AudioUnitDevelopmentFundamentals/AudioUnitDevelopmentFundamentals.html%3Fid%3DTP40003278-1.2&media=dvd" target=_new>It wasn&#8217;t helpful</a>: Tell us what would have helped.</div>
						</td>
					</tr>
				</table>
			</td>
		</tr>
	</table>

        <!--#include virtual="/includes/framesetfooter" -->
        <!-- end of footer -->
        </div>
    </div>
</body>
</html>