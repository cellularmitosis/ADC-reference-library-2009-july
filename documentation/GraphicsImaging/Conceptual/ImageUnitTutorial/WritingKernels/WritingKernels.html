<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
	<title>Image Unit Tutorial: Writing Kernels</title>
	<meta id="Generator" name="Generator" content="Gutenberg"/>
	<meta id="GeneratorVersion" name="GeneratorVersion" content="v132"/>
	<meta http-equiv="content-type" content="text/html;charset=utf-8"/>
	<meta id="Copyright" name="Copyright" content="Copyright 2009 Apple Inc. All Rights Reserved."/>
	<meta id="IndexTitle" name="IndexTitle" content="Writing Kernels"/>
	<meta id="xcode-display" name="xcode-display" content="render"/>
	<meta id="toc-file" name="toc-file" content="../toc.html"/>
	<meta id="RESOURCES" content="../../../../Resources" />
	
	<link rel="stylesheet" type="text/css" href="../../../../Resources/CSS/frameset_styles.css"/>
	<script language="JavaScript" type="text/javascript" src="../../../../Resources/JavaScript/lib/prototype.js"></script>
	<script language="JavaScript" type="text/javascript" src="../../../../Resources/JavaScript/lib/scriptaculous.js"></script>
	<script language="JavaScript" type="text/javascript" src="../../../../Resources/JavaScript/page.js"></script>
	<script language="JavaScript" type="text/javascript" src="../../../../Resources/JavaScript/pedia.js"></script>
	<!--[if lte IE 6]>
		<style type="text/css">
			/*<![CDATA[*/ 
			html {overflow-x:auto; overflow-y:hidden;  }
			/*]]>*/
		</style>
	<![endif]-->
</head>    
<body bgcolor="#ffffff" onload="initialize_page();"><a name="//apple_ref/doc/uid/TP40004531-CH3" title="Writing Kernels"></a>
    <noscript>
    <div id="tocMenu">
        <iframe id="toc_content" name="toc_content" SRC="../toc.html" width="210" height="100%" align="left" frameborder="0">This document set is best viewed in a browser that supports iFrames.</iframe>
    </div>
    </noscript>
    <div id="bodyText">
        <a name="top"></a>
        <div class="hideOnPrint hideInXcode">
        <!-- start of header -->
        <!--#include virtual="/includes/framesetheader" -->
        <!-- end of header -->
        </div>
        
        <!-- start of path -->
<div class="breadcrumb hideOnPrint hideInXcode"><a href="http://developer.apple.com/" target="_top">ADC Home</a> &gt; <a href="../../../../../referencelibrary/index.html#//apple_ref/doc/uid/TP30000943" target="_top">Reference Library</a> &gt; <a href="../../../../index.html#//apple_ref/doc/uid/TP30000440" target="_top">Guides</a> &gt; <a href="../../../index.html#//apple_ref/doc/uid/TP30000440-TP30000424" target="_top">Graphics &amp; Imaging</a> &gt; <a href="../../../Quartz-date.html#//apple_ref/doc/uid/TP30000440-TP30000424-TP30000559" target="_top">Quartz</a> &gt; <a href="../Introduction/Introduction.html#//apple_ref/doc/uid/TP40004531-CH1-SW1">Image Unit Tutorial</a> &gt; </div><br class="hideInXcode"/><!-- end of path -->
        
        <div class="mini_nav_text" align="left">
        <span class="navButtons">
        <a href="../Overview/Overview.html">&lt; Previous Page</a><span style="margin-left: 8px"><a href="../WritingtheObjective-CPortion/WritingtheObjective-CPortion.html">Next Page &gt;</a></span>
        </span>
        <span id="showHideTOCUpperSpan">
        <a href="#" onclick="showHideTOC();"><img src="../../../../Resources/Images/show_toc_icon.gif" width="15" height="14" border="0" style="margin-bottom: -2px;" alt="" hideText="Hide TOC" showText="Show TOC" /></a> <a href="#" onclick="showHideTOC();">Hide TOC</a>
        </span>
        </div>

        <hr />
        
        
        <a name="//apple_ref/doc/uid/TP40004531-CH3-SW1" title="Writing Kernels"></a><h1>Writing Kernels</h1><p>The heart of any image processing filter is the kernel file. A kernel file contains one or more <code>kernel</code> routines and any required subroutines. A <code>kernel</code> routine gets called once for each pixel for the destination image. The routine must return a <code>vec4</code> data type. Although this four-element vector typically contains pixel data, the vector is not required to represent a pixel. However, the <code>kernel</code> routines in this chapter produce only pixel data because that’s the most common data returned by a <code>kernel</code> routine. </p><p>A <code>kernel</code> routine can:</p><ul class="ul"><li class="li"><p>Fabricate the data. For example, a routine can produce random pixel values, generate a solid color, or produce a gradient. It can generate patterns, such as stripes, a checkerboard, a starburst, or color bars.</p></li><li class="li"><p>Modify a single pixel from a source image. A <code>kernel</code> routine can adjust hue, exposure, white point values, replace colors, and so on.</p></li><li class="li"><p>Sample several pixels from a source image to produce the output pixel. Stylize filters such as edge detection, pixellate, pointillize, gloom, and bloom use this technique.</p></li><li class="li"><p>Use location information from one or more pixels in a source image to produce the output pixel. Distortion effects, such as bump, pinch, and hole distortions are created this way.</p></li><li class="li"><p>Produce the output pixel by using data from a mask, texture, or other source to modify one or more pixels in a source image. The Core Image stylize filters—height field from mask, shaded material, and the disintegrate with mask transition—are examples of filters that use this technique.</p></li><li class="li"><p>Combine the pixels from two images to produce the output pixel. Blend mode, compositing, and transition filters work this way.</p></li></ul><p>This chapter shows how to write a variety of <code>kernel</code> routines. First you’ll see what the programming constraints, or rules, are. Then you’ll learn how to write a simple filter that operates on one input pixel to produce one output pixel. As the chapter progresses, you’ll learn how to write more complex <code>kernel</code> routines, including those used for a multipass filter. </p><p>Although the <code>kernel</code> routine is  where the per-pixel processing occurs, it is only one part of an image unit. You also need to write code that provides input data to the <code>kernel</code> routine and performs a number of other tasks as described in <span class="content_text"><a href="../WritingtheObjective-CPortion/WritingtheObjective-CPortion.html#//apple_ref/doc/uid/TP40004531-CH4-SW1">“Writing the Objective-C Portion.”</a></span> Then you’ll need to bundle all the code by following the instructions in <span class="content_text"><a href="../PackagingtheFilter/PackagingtheFilter.html#//apple_ref/doc/uid/TP40004531-CH5-SW1">“Preparing an Image Unit for Distribution.”</a></span></p><p>Before continuing in this chapter, see <em><a href="../../../Reference/CIKernelLangRef/index.html#//apple_ref/doc/uid/TP40004397" target="_top">Core Image Kernel Language Reference</a></em> for a description of the language you use to write <code>kernel</code> routines. Make sure you are familiar with the constraints discussed in <span class="content_text"><a href="../Overview/Overview.html#//apple_ref/doc/uid/TP40004531-CH6-SW4">“Kernel Routine Rules.”</a></span></p>
<!-- This template is being used for both PDF and HTML. -->

    
    <h4>In this section:</h4>
    
    
    <p class="blockquote">
    
        
			
			
				<a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW5">Writing Simple Kernel Routines</a>
				
			<br/>
			
        
			
			
				<a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW52">Testing Kernel Routines in Quartz Composer</a>
				
			<br/>
			
        
			
			
				<a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW54">Writing Advanced Kernel Routines</a>
				
			<br/>
			
        
			
			
				<a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW25">Writing Kernel Routines for a Detective Lens</a>
				
			<br/>
			
        
			
			
				<a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW33">Solution to the Kernel Challenge</a>
				
			<br/>
			
        
			
			
				<a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW64">Next Steps</a>
				
			<br/>
			
        

    </p><br/>

<a name="//apple_ref/doc/uid/TP40004531-CH3-SW5" title="Writing Simple Kernel Routines"></a><h2>Writing Simple Kernel Routines</h2><p>A <code>kernel</code> routine that operates on the color of a source pixel at location (<em>x</em>, <em>y</em>) to produce a pixel at the same location in the destination image is fairly straightforward to write. In general, a <code>kernel</code> routine that operates on color follows these steps:</p><ol class="ol"><li class="li"><p>Gets the pixel from the source image that is at the same location as the pixel you want to produce in the output image. The Core Image Kernel Language function <code>sample</code> returns the pixel value produced by the specified <code>sampler</code> at a specified point. To get the specified point, use the function <code>samplerCoord</code>, which returns the position, in sampler space, that is associated with the current output pixel after any transformation matrix associated with the sampler is applied. That means if the image is transformed in some way (for example, rotation or scaling), the <code>sampler</code> ensures that the transformation is reflected in the sample it fetches.</p></li><li class="li"><p>Operates on the color values.</p><div class="notebox"><a name="//apple_ref/doc/uid/TP40004531-CH3-SW46" title="Note"></a><p><strong>Note:</strong>&nbsp;Pixels in Core Image are comprised of red, green, blue, and alpha components whose floating-point values can range from <code>0.0</code> (component absent) to <code>1.0</code> (component present at 100%).</p></div></li><li class="li"><p>Returns the modified pixel.</p></li></ol><p>Equations for this sort of filter take the following form:</p><br/><div><img src = "../ART/pixel_expression.gif" alt = "A general equation" width="309" height="23"></div><br/><p>Depending on the operation, you may need to unpremultiply the color values prior to operating on them and the premultiply the color values before returning the modified pixel. The Core Image Kernel Language provides the <code>unpremultiply</code> and <code>premultiply</code> functions for this purpose.</p><a name="//apple_ref/doc/uid/TP40004531-CH3-SW18" title="Color Inversion"></a><h3>Color Inversion</h3><p>Color component values for pixels in Core Image are floating-point values that range from <code>0.0</code> (color component absent) to <code>1.0</code> (color component present at 100%). Inverting a color is accomplished by reassigning each color component of value of <code>1.0 – component_value</code>, such that:</p><div class="codesample"><table><tr><td scope="row"><pre>red_value = 1.0 - red_value<span></span></pre></td></tr><tr><td scope="row"><pre>blue_value = 1.0 - blue_value<span></span></pre></td></tr><tr><td scope="row"><pre>green_value = 1.0 - green_value<span></span></pre></td></tr></table></div><p><span class="content_text"><a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW31">Figure 2-1</a></span> shows a grid of pixels. If you invert the color of each pixel by applying these equations, you get the resulting grid of pixels shown in <span class="content_text"><a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW32">Figure 2-2</a></span>.</p><br/><div><a name="//apple_ref/doc/uid/TP40004531-CH3-SW31" title="Figure 2-1A grid of pixels"></a><p><strong>Figure 2-1&nbsp;&nbsp;</strong>A grid of pixels</p><img src = "../ART/source_color.gif" alt = "A grid of pixels" width="332" height="271"></div><br/><br/><div><a name="//apple_ref/doc/uid/TP40004531-CH3-SW32" title="Figure 2-2A grid of pixels after inverting the color"></a><p><strong>Figure 2-2&nbsp;&nbsp;</strong>A grid of pixels after inverting the color</p><img src = "../ART/invert_color.gif" alt = "A grid of pixels after inverting the color" width="331" height="270"></div><br/><p>Take a look at the <code>kernel</code> routine in <span class="content_text">Listing 2-1</span> to see how to implement color inversion. A detailed explanation for each numbered line of code appears following the listing.  You’ll see how to write the Objective-C portion that packages this routine as an image unit by reading <span class="content_text"><a href="../WritingtheObjective-CPortion/WritingtheObjective-CPortion.html#//apple_ref/doc/uid/TP40004531-CH4-SW7">“Creating a Color Inversion Image Unit.”</a></span> </p><a name="//apple_ref/doc/uid/TP40004531-CH3-SW2" title="Listing 2-1A kernel routine that inverts color"></a><p class="codesample"><strong>Listing 2-1&nbsp;&nbsp;</strong>A <code>kernel</code> routine that inverts color</p><div class="codesample"><table><tr><td scope="row"><pre>kernel vec4 _invertColor(sampler source_image) <span>// 1</span></pre></td></tr><tr><td scope="row"><pre>{<span></span></pre></td></tr><tr><td scope="row"><pre>    vec4 pixValue; <span>// 2</span></pre></td></tr><tr><td scope="row"><pre>    pixValue = sample(source_image, samplerCoord(source_image)); <span>// 3</span></pre></td></tr><tr><td scope="row"><pre>    unpremultiply(pixValue); <span>// 4</span></pre></td></tr><tr><td scope="row"><pre>    pixValue.r = 1.0 - pixValue.r; <span>// 5</span></pre></td></tr><tr><td scope="row"><pre>    pixValue.g = 1.0 - pixValue.g;<span></span></pre></td></tr><tr><td scope="row"><pre>    pixValue.b = 1.0 - pixValue.b;<span></span></pre></td></tr><tr><td scope="row"><pre>    return premultiply(pixValue); <span>// 6</span></pre></td></tr><tr><td scope="row"><pre>}<span></span></pre></td></tr></table></div><p>Here’s what the code does:</p><ol class="ol"><li class="li"><p>Takes a <code>sampler</code> object as an input parameter. Recall (see <span class="content_text"><a href="../Overview/Overview.html#//apple_ref/doc/uid/TP40004531-CH6-SW4">“Kernel Routine Rules”</a></span>)  that <code>kernel</code> routines do not take images as input parameters. Instead, the <code>sampler</code> object is in charge of accessing image data and providing it to the <code>kernel</code> routine. </p><p>A routine that modifies a single pixel value will always have a <code>sampler</code> object as an input parameter. The <code>sampler</code> object for this category of <code>kernel</code> routine is passed in from a <code>CISampler</code> object created in the Objective-C portion of an image unit. (See <span class="content_text"><a href="../Overview/Overview.html#//apple_ref/doc/uid/TP40004531-CH6-SW3">“Division of Labor.”</a></span>) The <code>sampler</code> object simply retrieves pixel values from the a source image. You can think of the <code>sampler</code> object as a data source.</p></li><li class="li"><p>Declares a <code>vec4</code> data type to hold the red, green, blue, and alpha component values of a pixel. A four-element vector provides a convenient way to store pixel data. </p></li><li class="li"><p>Fetches a pixel value from the source image. Let’s take a closer look at this statement, particularly the <code>sample</code> and <code>samplerCoord</code> functions provided by the Core Image Kernel Language. The <code>samplerCoord</code> function returns the position, in sampler space, associated with the current destination pixel after any transformations associated with the image source or the sampler are applied to the image data. As the <code>kernel</code> routine has no way of knowing whether any transformations have been applied, it’s best to use the <code>samplerCoord</code> function to retrieve the the position. When you read <span class="content_text"><a href="../WritingtheObjective-CPortion/WritingtheObjective-CPortion.html#//apple_ref/doc/uid/TP40004531-CH4-SW1">“Writing the Objective-C Portion”</a></span> you’ll see that it is possible, and often necessary, to apply transformations in the Objective-C portion of an image unit. </p><p>The <code>sample</code> function returns the pixel value obtained by the <code>sampler</code> object from the specified position. This function assumes the position is in sampler space, which is why you need to nest the call to <code>samplerCoord</code> to retrieve the position.</p></li><li class="li"><p>Unpremultiplies the pixel data. If your routine operates on color data that could have an alpha value other then <code>1.0</code> (fully opaque), you need to call the Core Image Kernel Language function <code>unpremultiply</code> (or take similar steps—see the advanced tip below) prior to operating on the color values</p><div class="notebox"><a name="//apple_ref/doc/uid/TP40004531-CH3-SW47" title="Note"></a><p><strong>Note:</strong>&nbsp;Core Image always works in an RGB colorspace. Data, such as YUV, must first be converted to RGB. Such conversion is at a much higher level than the <code>kernel</code> routine. However, Core Image performs the conversion of YUV texture data automatically for you. Keep in mind that the data provided to your <code>kernel</code> routine by Core Image is always RGB based.</p></div></li><li class="li"><p>Inverts the red color component. The next two lines invert the green and blue color components.  Note that you can access the individual components of a pixel by using <code>.r</code>, <code>.g</code>, <code>.b</code>, and <code>.a</code> instead of a numerical index. That way, you never need to concern yourself with the order of the components. (You can also use <code>.x</code>, <code>.y</code>, <code>.z</code>, and <code>.w</code> as field accessors.)</p></li><li class="li"><p>Premultiplies the data and returns a <code>vec4</code> data type that contains inverted values for the color components of the destination pixel. The function <code>premultiply</code> is defined by the Core Image Kernel Language.</p></li></ol><div class="notebox"><a name="//apple_ref/doc/uid/TP40004531-CH3-SW48" title="Advanced Tip"></a><p><strong>Advanced Tip:</strong>&nbsp;The following is a more efficient way to write  <span class="content_text"><a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW2">Listing 2-1</a></span>, and faster to execute.  The code simply subtracts the red, green, and blue values from the alpha value. For any value of alpha, this has the same result as <span class="content_text">Listing 2-1</span>. The <code>pixValue.aaa</code> notation might look a bit odd, but it is valid, and represents a three-element vector made up of three identical values (the alpha value). </p><div class="codesample"><table><tr><td scope="row"><pre>kernel vec4 _invertColor(sampler source_image)<span></span></pre></td></tr><tr><td scope="row"><pre>{<span></span></pre></td></tr><tr><td scope="row"><pre>    vec4 pixValue; 2<span></span></pre></td></tr><tr><td scope="row"><pre>    pixValue = sample(source_image, samplerCoord(source_image));<span></span></pre></td></tr><tr><td scope="row"><pre>    pixValue.rgb = pixValue.aaa - pixValue.rgb;<span></span></pre></td></tr><tr><td scope="row"><pre>    return pixValue;<span></span></pre></td></tr><tr><td scope="row"><pre>}<span></span></pre></td></tr></table></div></div><p>When you apply a color inversion <code>kernel</code> routine to the image shown in <span class="content_text">Figure 2-3</span> you get the result shown in <span class="content_text"><a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW27">Figure 2-4</a></span>. </p><br/><div><a name="//apple_ref/doc/uid/TP40004531-CH3-SW13" title="Figure 2-3An image of a gazelle"></a><p><strong>Figure 2-3&nbsp;&nbsp;</strong>An image of a gazelle</p><img src = "../ART/gazelle_before.jpg" alt = "An image of a gazelle" ></div><br/><br/><div><a name="//apple_ref/doc/uid/TP40004531-CH3-SW27" title="Figure 2-4A gazelle image after inverting the colors"></a><p><strong>Figure 2-4&nbsp;&nbsp;</strong>A gazelle image after inverting the colors</p><img src = "../ART/gazelle_invert.jpg" alt = "A gazelle image after inverting the colors" ></div><br/><a name="//apple_ref/doc/uid/TP40004531-CH3-SW49" title="Color Component Rearrangement"></a><h3>Color Component Rearrangement</h3><p><span class="content_text"><a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW3">Listing 2-2</a></span> shows another simple <code>kernel</code> routine that modifies the color values of a pixel by rearranging the color component values. The red channel is assigned the green values. The green channel is assigned the blue values. The blue channel is assigned the red values. Applying this filter to the image shown in <span class="content_text">Figure 2-5</span> results in the image shown in <span class="content_text"><a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW7">Figure 2-6</a></span>.</p><br/><div><a name="//apple_ref/doc/uid/TP40004531-CH3-SW6" title="Figure 2-5An image of a ladybug"></a><p><strong>Figure 2-5&nbsp;&nbsp;</strong>An image of a ladybug</p><img src = "../ART/ladybug_pre.jpg" alt = "An image of a ladybug" ></div><br/><br/><div><a name="//apple_ref/doc/uid/TP40004531-CH3-SW7" title="Figure 2-6A ladybug image after rearranging pixel components "></a><p><strong>Figure 2-6&nbsp;&nbsp;</strong>A ladybug image after rearranging pixel components </p><img src = "../ART/ladybug_post.jpg" alt = "A ladybug image after rearranging pixel components" ></div><br/><p>As you can see, the routine in <span class="content_text"><a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW3">Listing 2-2</a></span> is very similar to <span class="content_text"><a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW2">Listing 2-1</a></span>. This <code>kernel</code> routine, however, uses two vectors, one for the pixel provided from the source image and the other to hold the modified values. The alpha value remains unchanged, but the red, green, and blue values are shifted.</p><a name="//apple_ref/doc/uid/TP40004531-CH3-SW3" title="Listing 2-2A kernel routine that places RGB values in the GBR channels"></a><p class="codesample"><strong>Listing 2-2&nbsp;&nbsp;</strong>A <code>kernel</code> routine that places RGB values in the GBR channels</p><div class="codesample"><table><tr><td scope="row"><pre>kernel vec4 RGB_to_GBR(sampler source_image)<span></span></pre></td></tr><tr><td scope="row"><pre>{<span></span></pre></td></tr><tr><td scope="row"><pre>    vec4 originalColor, twistedColor;<span></span></pre></td></tr><tr><td scope="row"><pre> <span></span></pre></td></tr><tr><td scope="row"><pre>    originalColor = sample(image, samplerCoord(source_image));<span></span></pre></td></tr><tr><td scope="row"><pre>    twistedColor.r = originalColor.g;<span></span></pre></td></tr><tr><td scope="row"><pre>    twistedColor.g = originalColor.b;<span></span></pre></td></tr><tr><td scope="row"><pre>    twistedColor.b = originalColor.r ;<span></span></pre></td></tr><tr><td scope="row"><pre>    twistedColor.a = originalColor.a;<span></span></pre></td></tr><tr><td scope="row"><pre>    return twistedColor;<span></span></pre></td></tr><tr><td scope="row"><pre>}<span></span></pre></td></tr></table></div><a name="//apple_ref/doc/uid/TP40004531-CH3-SW50" title="Color Multiplication"></a><h3>Color Multiplication</h3><p>Color multiplication is true to its name; it multiplies each pixel in a source image by a specified color. <span class="content_text">Figure 2-7</span> shows the effect of applying a color multiply filter to the image shown in <span class="content_text"><a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW6">Figure 2-5</a></span>.</p><br/><div><a name="//apple_ref/doc/uid/TP40004531-CH3-SW8" title="Figure 2-7A ladybug image after applying a multiply filter"></a><p><strong>Figure 2-7&nbsp;&nbsp;</strong>A ladybug image after applying a multiply filter</p><img src = "../ART/peachy_multiply.jpg" alt = "A ladybug image after applying a multiply filter" ></div><br/><p> <span class="content_text"><a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW4">Listing 2-3</a></span> shows the <code>kernel</code> routine used to produce this effect. So that you don’t get the idea that a kernel can take only one input parameter, note that this routine takes two input parameters—one a  <code>sampler</code> object and the other a <code>__color</code> data type. The <code>__color</code> data type is one of two data types defined by the Core Image kernel language; the other data type is <code>sampler</code>, which you already know about. These two data types are not the only ones that you can use input parameters to a <code>kernel</code> routine. You can also use these data types which are defined by the Open GL Shading Language (glslang)—<code>float</code>, <code>vec2</code>, <code>vec3</code>, <code>vec4</code>.</p><div class="importantbox"><a name="//apple_ref/doc/uid/TP40004531-CH3-DontLinkElementID_4" title="Important:"></a><p><strong>Important:</strong>&nbsp;Keep in mind, however, that in the Objective-C portion of the filter, all data types passed to the <code>kernel</code> routine must be packaged Objective-C objects. See <span class="content_text"><a href="../Overview/Overview.html#//apple_ref/doc/uid/TP40004531-CH6-SW4">“Kernel Routine Rules”</a></span> for details.</p><p></p></div><p>The color supplied to a <code>kernel</code> routine will be matched, by Core Image, to the working color space of the Core Image context associated with the kernel. There is nothing that you need to do regarding color in the kernel. Just keep in mind that, to the <code>kernel</code> routine, <code>__color</code> is a <code>vec4</code> data type in premultiplied RGBA format, just as the pixel values fetched by the <code>sampler</code> are.</p><p><span class="content_text">Listing 2-3</span> points out an important aspect of kernel calculations—the use of vector math. The sample fetched by the Core Image Kernel Language function <code>sample</code> is a four-element vector. Because it is a vector, you can multiply it directly by <code>multiplyColor</code>; there is no need to access each component separately.</p><p>By now you should be used to seeing the <code>samplerCoord</code> function nested with the <code>sample</code> function!</p><a name="//apple_ref/doc/uid/TP40004531-CH3-SW4" title="Listing 2-3A kernel routine that produces a multiply effect"></a><p class="codesample"><strong>Listing 2-3&nbsp;&nbsp;</strong>A kernel routine that produces a multiply effect</p><div class="codesample"><table><tr><td scope="row"><pre>kernel vec4 multiplyEffect (sampler image_source, __color multiplyColor)<span></span></pre></td></tr><tr><td scope="row"><pre>{<span></span></pre></td></tr><tr><td scope="row"><pre>  return sample (image_source, samplerCoord (image_source)) * multiplyColor;<span></span></pre></td></tr><tr><td scope="row"><pre>}<span></span></pre></td></tr></table></div><a name="//apple_ref/doc/uid/TP40004531-CH3-SW51" title="A Kernel Challenge"></a><h3>A Kernel Challenge</h3><p>Now that you’ve seen how to write <code>kernel</code> routines that operate on a single pixel, it’s time to challenge yourself. Write a <code>kernel</code> routine that produces a monochrome image similar to what’s shown in <span class="content_text">Figure 2-8</span>. The filter should take two input parameters, a  <code>sampler</code> and a <code>__color</code>. Use Quartz Composer to test and debug your routine. You can find a solution in <span class="content_text"><a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW33">“Solution to the Kernel Challenge.”</a></span></p><br/><div><a name="//apple_ref/doc/uid/TP40004531-CH3-SW9" title="Figure 2-8A monochrome image of a ladybug"></a><p><strong>Figure 2-8&nbsp;&nbsp;</strong>A monochrome image of a ladybug</p><img src = "../ART/ladybug_monochrome.jpg" alt = "A monochrome image of a ladybug" ></div><br/><a name="//apple_ref/doc/uid/TP40004531-CH3-SW52" title="Testing Kernel Routines in Quartz Composer"></a><h2>Testing Kernel Routines in Quartz Composer</h2><p>The <code>kernel</code> routine, as you know, is one part of a Core Image filter. There is still a good deal of code that you need to write at a level higher than the <code>kernel</code> routine and a bit more work beyond that to package your image processing code as an image unit. Fortunately, you don’t need to write this additional code to test simple <code>kernel</code> routines. You can instead use Quartz Composer. </p><p>Quartz Composer is a development tool for processing and rendering graphical data. It’s available on any computer that has the Developer tools installed. You can find it in <code>/Developer/Applications</code>. The following steps will show you how to use Quartz Composer to test each of the <code>kernel</code> routines you’ve read about so far.</p><div class="notebox"><a name="//apple_ref/doc/uid/TP40004531-CH3-SW53" title="Note"></a><p><strong>Note:</strong>&nbsp;Before you follow these steps, you’ll need to familiarize yourself with Quartz Composer by reading <em><a href="../../QuartzComposerUserGuide/index.html#//apple_ref/doc/uid/TP40005381" target="_top">Quartz Composer User Guide</a></em>. The time that you spend learning to use Quartz Composer will save you a lot of time debugging and testing many of the <code>kernel</code> routines that your write.</p></div><ol class="ol"><li class="li"><p>Launch Quartz Composer by double-clicking its icon in <code>/Developer/Applications</code>. </p></li><li class="li"><p>In the sheet that appears, choose Blank Composition.</p></li><li class="li"><p>Open the Patch Creator and search for Core Image Filter.</p></li><li class="li"><p>Add the Core Image Filter patch to the workspace.</p></li><li class="li"><p>Use the search field to locate the Billboard patch, then add that patch to the workspace.</p></li><li class="li"><p>In a similar manner, locate the Image Importer patch and add it to the workspace.</p></li><li class="li"><p>Connect the Image output port on the Image Importer patch to the Image input port on the Core Image Filter patch. </p></li><li class="li"><p>Connect the Image output port on the Core Image Filter patch to the Image input port on the Billboard.</p></li><li class="li"><p>Click the Image Importer patch and then click the Inspector button on the toolbar.</p></li><li class="li"><p>Choose Settings from the inspector pop-up menu. Then click Import From File and choose an image.</p></li><li class="li"><p>Click Viewer in the toolbar to make sure that the Viewer window is visible.</p><p>You should see the image that you imported rendered to the Viewer window.</p></li><li class="li"><p>Click the Core Image Filter patch and open the inspector to the Settings pane.</p><p>Note that there is already a <code>kernel</code> routine entered for a multiply effect. If you want to see how that works, choose Input Parameters from the inspector pop-up menu and then click the color well to set a color. You immediately see the results in the Viewer window.</p><div class="item_figure"><img src = "../ART/cikp_settings.jpg" alt = "The default kernel routine in the Core Image Kernel patch" ></div></li><li class="li"><p>Copy the <code>kernel</code> routine shown in <span class="content_text"><a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW2">Listing 2-1</a></span> and replace the multiply effect routine that’s in the Settings pane of the Core Image Kernel patch.</p><p>Notice that not only does the image on the Viewer window change (its color should be inverted), but the Core Image Kernel patch automatically changes to match the input parameters of the kernel routine. The invert color kernel has only one input parameter, whereas the multiply effect supplied as a default had two parameters.</p></li><li class="li"><p>Follow the same procedure to test the <code>kernel</code> routine shown in <span class="content_text"><a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW3">Listing 2-2</a></span>.</p></li></ol><a name="//apple_ref/doc/uid/TP40004531-CH3-SW54" title="Writing Advanced Kernel Routines"></a><h2>Writing Advanced Kernel Routines</h2><p>Up to now you’ve seen how to write several simple <code>kernel</code> routines that operate on a pixel from a source image to produce a pixel in a destination image that’s at the same working-space coordinate as the pixel from the source image. Some of the most interesting and useful filters, however, do not use this one-to-one mapping. These more advanced <code>kernel</code> routines are what you’ll learn about in this section.</p><p>Recall from <span class="content_text"><a href="../Overview/Overview.html#//apple_ref/doc/uid/TP40004531-CH6-SW1">“An Image Unit and Its Parts”</a></span> that <code>kernel</code> routines that do not use a one-to-one mapping require  a region-of-interest method that defines the area from which a <code>sampler</code> object can fetch pixels. The <code>kernel</code> routine knows nothing about this method. The routine simply takes the data that is passed to it, operates on it, and computes the <code>vec4</code> data type that the <code>kernel</code> routine returns. As a result, this section doesn’t show you how to set up the ROI method. You’ll see how to accomplish that task in <span class="content_text"><a href="../WritingtheObjective-CPortion/WritingtheObjective-CPortion.html#//apple_ref/doc/uid/TP40004531-CH4-SW1">“Writing the Objective-C Portion.”</a></span> For now, assume that each <code>sampler</code> passed to a <code>kernel</code> routine supplies data from the appropriate region of interest.</p><p>An example of an image produced by an advanced <code>kernel</code> routine is shown in <span class="content_text">Figure 2-9</span>. The figure shows a grid of pixels produced by a “color block” <code>kernel</code> routine. You’ll notice that the blocks of color are 4 pixels wide and 4 pixels high. The pixels marked with “S” denote the location of the pixel in a source image from which the 4 by 4 block  inherits its color. As you can see, the <code>kernel</code> routine must perform a one-to-many mapping. This is just the sort of operation that the the pixellate <code>kernel</code> routine discussed in detail in <span class="content_text"><a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW15">“Pixellate”</a></span> performs. </p><br/><div><a name="//apple_ref/doc/uid/TP40004531-CH3-SW23" title="Figure 2-9Colored blocks of pixels"></a><p><strong>Figure 2-9&nbsp;&nbsp;</strong>Colored blocks of pixels</p><img src = "../ART/color_pixelate.gif" alt = "Colored blocks of pixels" width="332" height="271"></div><br/><p>You’ll see how to write two other advanced kernel routines in  <span class="content_text"><a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW24">“Edge Enhancement”</a></span> and  <span class="content_text"><a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW34">“Fun House Mirror Distortion.”</a></span></p><a name="//apple_ref/doc/uid/TP40004531-CH3-SW15" title="Pixellate"></a><h3>Pixellate</h3><p>A pixellate filter uses a limited number of pixels from a source image to produce the destination image, as described in previously. Compare <span class="content_text"><a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW13">Figure 2-3</a></span> with <span class="content_text"><a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW26">Figure 2-10</a></span>. Notice that the processed image looks blocky; it is made up of dots of a solid color. The size of the dots are determined by a scaling factor that’s passed to the <code>kernel</code> routine as an input parameter.</p><br/><div><a name="//apple_ref/doc/uid/TP40004531-CH3-SW26" title="Figure 2-10A gazelle image after pixellation"></a><p><strong>Figure 2-10&nbsp;&nbsp;</strong>A gazelle image after pixellation</p><img src = "../ART/gazelle_pixelate.jpg" alt = "A gazelle image after pixellation" ></div><br/><p>The trick to any pixellate routine is to use a modulus operator on the coordinates to divide  the coordinates into discrete steps. This causes your code to read the same source pixel until your output coordinate has incremented beyond the threshold of the scale, producing an effect similar to that shown in <span class="content_text"><a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW26">Figure 2-10</a></span>. The code shown in <span class="content_text"><a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW28">Listing 2-4</a></span> creates round dots instead of squares by creating an anti-aliased edge that produces the dot effect shown in <span class="content_text">Figure 2-11</span>. Notice that each 4-by-4 block represents a single color, but the alpha component varies from 0.0 to 1.0. Anti-aliasing effects are used in a number of filters, so it is worthwhile to study the code, shown in <span class="content_text"><a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW28">Listing 2-4</a></span>, that accomplishes this.</p><br/><div><a name="//apple_ref/doc/uid/TP40004531-CH3-SW29" title="Figure 2-11Colored blocks of pixels with opacity added"></a><p><strong>Figure 2-11&nbsp;&nbsp;</strong>Colored blocks of pixels with opacity added</p><img src = "../ART/color_opacity_pixelate.gif" alt = "Colored blocks of pixels with opacity added" width="332" height="271"></div><br/><p>The pixellate <code>kernel</code> routine  takes two input parameters: a <code>sampler</code> object for fetching samples from the source image and a floating-point value that specifies the diameter of the dots. A detailed explanation for each numbered line of code appears following the listing. </p><a name="//apple_ref/doc/uid/TP40004531-CH3-SW28" title="Listing 2-4A kernel routine that pixellates"></a><p class="codesample"><strong>Listing 2-4&nbsp;&nbsp;</strong>A <code>kernel</code> routine that pixellates</p><div class="codesample"><table><tr><td scope="row"><pre>kernel vec4 roundPixellate(sampler src, float scale)<span>// 1</span></pre></td></tr><tr><td scope="row"><pre>{<span></span></pre></td></tr><tr><td scope="row"><pre>    vec2    positionOfDestPixel, centerPoint; <span>// 2</span></pre></td></tr><tr><td scope="row"><pre>    float   d, radius;<span></span></pre></td></tr><tr><td scope="row"><pre>    vec4    outValue;<span></span></pre></td></tr><tr><td scope="row"><pre>    float   smooth = 0.5;<span></span></pre></td></tr><tr><td scope="row"><pre> <span></span></pre></td></tr><tr><td scope="row"><pre>    radius = scale / 2.0;<span></span></pre></td></tr><tr><td scope="row"><pre>    positionOfDestPixel = destCoord();<span>// 3</span></pre></td></tr><tr><td scope="row"><pre>    centerPoint = positionOfDestPixel;<span></span></pre></td></tr><tr><td scope="row"><pre>    centerPoint.x = centerPoint.x - mod(positionOfDestPixel.x, scale) + radius; <span>// 4</span></pre></td></tr><tr><td scope="row"><pre>    centerPoint.y = centerPoint.y - mod(positionOfDestPixel.y, scale) + radius; <span>// 5</span></pre></td></tr><tr><td scope="row"><pre>    d = distance(centerPoint, positionOfDestPixel); <span>// 6</span></pre></td></tr><tr><td scope="row"><pre> <span></span></pre></td></tr><tr><td scope="row"><pre>    outValue = sample(src, samplerTransform(src, centerPoint)); <span>// 7</span></pre></td></tr><tr><td scope="row"><pre>    outValue.a = outValue.a * smoothstep((d - smooth), d, radius); <span>// 8</span></pre></td></tr><tr><td scope="row"><pre> <span></span></pre></td></tr><tr><td scope="row"><pre>    return premultiply(outValue);  <span>// 9</span></pre></td></tr><tr><td scope="row"><pre> <span></span></pre></td></tr><tr><td scope="row"><pre>}<span></span></pre></td></tr></table></div><p>Here’s what the code does: </p><ol class="ol"><li class="li"><p>Takes a <code>sampler</code> and a scaling value. Note the scaling value is declared as a <code>float</code> data type here, but when you write the Objective-C portion of the filter, you must pass the <code>float</code> as an <code>NSNumber</code> object. Otherwise, the filter will not work. See <span class="content_text"><a href="../Overview/Overview.html#//apple_ref/doc/uid/TP40004531-CH6-SW4">“Kernel Routine Rules.”</a></span></p></li><li class="li"><p>Declares two <code>vec2</code> data types. The <code>centerPoint</code> variable holds the coordinate of the pixel that determines the color of a block; it is the “S” shown in <span class="content_text"><a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW29">Figure 2-11</a></span>. The <code>positionOfDestPixel</code> variable holds the position of the destination pixel.</p></li><li class="li"><p>Gets the position, in working-space coordinates, of the pixel currently being computed. The function <code>destCoord</code> is defined by the Core Image Kernel Language. (See <em><a href="../../../Reference/CIKernelLangRef/index.html#//apple_ref/doc/uid/TP40004397" target="_top">Core Image Kernel Language Reference</a></em>.)</p></li><li class="li"><p>Calculates the x-coordinate for the pixel that determines the color of the destination pixel.</p></li><li class="li"><p>Calculates the y-coordinate for the pixel that determines the color of the destination pixel.</p></li><li class="li"><p>Calculates how far the destination pixel is from the center point (“S”). This distance determines the value of the alpha component.</p><div class="notebox"><a name="//apple_ref/doc/uid/TP40004531-CH3-SW55" title="Note"></a><p><strong>Note:</strong>&nbsp;The <code>distance</code> function is defined in the <span class="content_text"><a href="http://www.opengl.org/documentation/glsl/" target="_blank">OpenGL Shading Language specification</a></span>. It returns the distance between two points.</p><code>float distance (genType p0, genType p1);</code></p></div></li><li class="li"><p>Fetches a pixel value from the source image, at the location specified by the <code>centerPoint</code> vector. </p><p>Recall that the <code>sample</code> function returns the pixel value produced by the <code>sampler</code> at the specified position. This function assumes the position is in sampler space, which is why you need to nest the call to <code>samplerCoord</code> to retrieve the position.</p></li><li class="li"><p>Creates an anti-aliased edge by multiplying the alpha component of the destination pixel by the <code>smoothstep</code> function defined by glslang. (See the <span class="content_text"><a href="http://www.opengl.org/documentation/glsl/" target="_blank">OpenGL Shading Language specification</a></span>.)</p><div class="notebox"><a name="//apple_ref/doc/uid/TP40004531-CH3-SW56" title="Note"></a><p><strong>Note:</strong>&nbsp;The <code>smoothstep</code> function returns <code>0.0</code> if <code>x &lt;= edge0</code> and if <code>x >= edge1</code>. Otherwise, it interpolates between <code>0</code> and <code>1</code>.</p><code>genType smoothstep (float edge0, float edge1, genType x);</code></p></div></li><li class="li"><p>Premultiplies the result before returning the value.</p></li></ol><p>You’ll see how to write the Objective-C portion that packages this routine as an image unit by reading  <span class="content_text"><a href="../WritingtheObjective-CPortion/WritingtheObjective-CPortion.html#//apple_ref/doc/uid/TP40004531-CH4-SW3">“Creating a Pixellate Image Unit.”</a></span> </p><a name="//apple_ref/doc/uid/TP40004531-CH3-SW24" title="Edge Enhancement"></a><h3>Edge Enhancement</h3><p>The edge enhancement <code>kernel</code> routine discussed in this section performs two tasks. It detects the edges in an image using a Sobel template. It also enhances the edges. Although the <code>kernel</code> routine operates on all color components, you can get an idea of what is does by comparing <span class="content_text"><a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW10">Figure 2-12</a></span> with <span class="content_text"><a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW11">Figure 2-13</a></span>.</p><div class="notebox"><a name="//apple_ref/doc/uid/TP40004531-CH3-SW57" title="Note"></a><p><strong>Note:</strong>&nbsp;There are many ways to computationally detect edges in an image. One approach is to use a template as a model of the ideal edge. An <strong>edge-detection template</strong> approximates the gradient at the pixel that corresponds to the center of the template. A <strong>Sobel template</strong> is a commonly used template because it provides a very good edge model while remaining small and thus not too costly from a computational standpoint. The template used here is implemented as a set of convolution masks whose weights on the diagonal elements are less than those on the horizontal and vertical elements. For details, see one of the image processing books suggested in <span class="content_text"><a href="../PackagingtheFilter/PackagingtheFilter.html#//apple_ref/doc/uid/TP40004531-CH5-SW35">“Further Reading.”</a></span></p></div><br/><div><a name="//apple_ref/doc/uid/TP40004531-CH3-SW10" title="Figure 2-12A checkerboard pattern before edge enhancement"></a><p><strong>Figure 2-12&nbsp;&nbsp;</strong>A checkerboard pattern before edge enhancement</p><img src = "../ART/before_edgy.jpg" alt = "A checkerboard pattern before edge enhancement" ></div><br/><br/><div><a name="//apple_ref/doc/uid/TP40004531-CH3-SW11" title="Figure 2-13A checkerboard pattern after edge enhancement"></a><p><strong>Figure 2-13&nbsp;&nbsp;</strong>A checkerboard pattern after edge enhancement</p><img src = "../ART/after_edgy.jpg" alt = "A checkerboard pattern after edge enhancement" ></div><br/><p>The <code>_EdgyFilter</code> kernel is shown in <span class="content_text">Listing 2-5</span>. It takes two parameters, a <code>sampler</code> for fetching image data from a source image and a <code>power</code> parameter that’s used to brighten or darken the image. A detailed explanation for each numbered line of code appears following the listing.</p><a name="//apple_ref/doc/uid/TP40004531-CH3-SW12" title="Listing 2-5A kernel routine that enhances edges"></a><p class="codesample"><strong>Listing 2-5&nbsp;&nbsp;</strong>A <code>kernel</code> routine that enhances edges</p><div class="codesample"><table><tr><td scope="row"><pre>kernel vec4 _EdgyFilter(sampler image, float power) <span>// 1</span></pre></td></tr><tr><td scope="row"><pre>{<span></span></pre></td></tr><tr><td scope="row"><pre>    const vec2 xy = destCoord(); <span>// 2</span></pre></td></tr><tr><td scope="row"><pre>    vec4  p00,p01,p02, p10,p12, p20,p21,p22; <span>// 3</span></pre></td></tr><tr><td scope="row"><pre>    vec4  sumX, sumY, computedPixel;  <span>// 4</span></pre></td></tr><tr><td scope="row"><pre>    float edgeValue;<span></span></pre></td></tr><tr><td scope="row"><pre> <span></span></pre></td></tr><tr><td scope="row"><pre>    p00 = sample(image, samplerTransform(image, xy+vec2(-1.0, -1.0))); <span>// 5</span></pre></td></tr><tr><td scope="row"><pre>    p01 = sample(image, samplerTransform(image, xy+vec2( 0.0, -1.0)));<span></span></pre></td></tr><tr><td scope="row"><pre>    p02 = sample(image, samplerTransform(image, xy+vec2(+1.0, -1.0)));<span></span></pre></td></tr><tr><td scope="row"><pre>    p10 = sample(image, samplerTransform(image, xy+vec2(-1.0,  0.0)));<span></span></pre></td></tr><tr><td scope="row"><pre>    p12 = sample(image, samplerTransform(image, xy+vec2(+1.0,  0.0)));<span></span></pre></td></tr><tr><td scope="row"><pre>    p20 = sample(image, samplerTransform(image, xy+vec2(-1.0, +1.0)));<span></span></pre></td></tr><tr><td scope="row"><pre>    p21 = sample(image, samplerTransform(image, xy+vec2( 0.0, +1.0)));<span></span></pre></td></tr><tr><td scope="row"><pre>    p22 = sample(image, samplerTransform(image, xy+vec2(+1.0, +1.0)));<span></span></pre></td></tr><tr><td scope="row"><pre> <span></span></pre></td></tr><tr><td scope="row"><pre>    sumX = (p22+p02-p20-p00) + 2.0*(p12-p10); <span>// 6</span></pre></td></tr><tr><td scope="row"><pre>    sumY = (p20+p22-p00-p02) + 2.0*(p21-p01); <span>// 7</span></pre></td></tr><tr><td scope="row"><pre> <span></span></pre></td></tr><tr><td scope="row"><pre>    edgeValue = sqrt(dot(sumX,sumX) + dot(sumY,sumY)) * power; <span>// 8</span></pre></td></tr><tr><td scope="row"><pre> <span></span></pre></td></tr><tr><td scope="row"><pre>    computedPixel = sample(image, samplerCoord(image)); <span>// 9</span></pre></td></tr><tr><td scope="row"><pre>    computedPixel.rgb = computedPixel.rgb * edgeValue; <span>// 10</span></pre></td></tr><tr><td scope="row"><pre>    return computedPixel; <span>// 11</span></pre></td></tr><tr><td scope="row"><pre>}<span></span></pre></td></tr></table></div><p>Here’s what the code does:</p><ol class="ol"><li class="li"><p>Takes a <code>sampler</code> and a <code>power</code> value. Note that the <code>power</code> value is declared as a <code>float</code> data type here, but when you write the Objective-C portion of the filter, you must pass the <code>float</code> as an <code>NSNumber</code> object. Otherwise, the filter will not work. See <span class="content_text"><a href="../Overview/Overview.html#//apple_ref/doc/uid/TP40004531-CH6-SW4">“Kernel Routine Rules.”</a></span></p></li><li class="li"><p>Gets the position, in working-space coordinates, of the pixel currently being computed. The function <code>destCoord</code> is defined by the Core Image Kernel Language. (See <em><a href="../../../Reference/CIKernelLangRef/index.html#//apple_ref/doc/uid/TP40004397" target="_top">Core Image Kernel Language Reference</a></em>.)</p></li><li class="li"><p>Declares 8 four-element vectors. These vectors will hold the values of the 8 pixels that are neighbors to the destination pixel that the <code>kernel</code> routine is computing.</p></li><li class="li"><p>Declares vectors to hold the intermediate results and the final computed pixel.</p></li><li class="li"><p>This and the following seven lines of code fetch  8 neighboring pixels.</p></li><li class="li"><p>Computes the sum of the <em>x</em> values of the neighboring pixels, weighted by the Sobel template.</p></li><li class="li"><p>Computes the sum of the <em>y</em> values of the neighboring pixels, weighted by the Sobel template.</p></li><li class="li"><p>Computes the magnitude, then scales by the <code>power</code>  parameter. The magnitude provides the edge detection/enhancement portion of the filter, and the power has a brightening (or darkening) effect.</p><div class="notebox"><a name="//apple_ref/doc/uid/TP40004531-CH3-SW58" title="Note"></a><p><strong>Note:</strong>&nbsp;The <code>dot</code> function provided by OpenGL Shading Language returns the <strong>dot product</strong> of two vectors. If <em>x</em> and <em>y</em> are two four-element vectors that represent pixels, the result returned is:</p><code>result = x.r * y.r + x.g * y.g + x.b * y.b + x.a * y.a</code></p></div></li><li class="li"><p>Gets the pixel whose destination value needs to be computed.</p></li><li class="li"><p>Modifies the color of the destination pixel by the edge value.</p></li><li class="li"><p>Returns the computed pixel.</p></li></ol><p>When you apply the <code>_EdgyFilter</code> kernel  to the image shown in <span class="content_text"><a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW13">Figure 2-3</a></span>, you get the resulting image shown in <span class="content_text">Figure 2-14</span>.</p><br/><div><a name="//apple_ref/doc/uid/TP40004531-CH3-SW14" title="Figure 2-14A gazelle image after edge enhancement"></a><p><strong>Figure 2-14&nbsp;&nbsp;</strong>A gazelle image after edge enhancement</p><img src = "../ART/edgy_gazelle.jpg" alt = "A gazelle image after edge enhancement" ></div><br/><a name="//apple_ref/doc/uid/TP40004531-CH3-SW34" title="Fun House Mirror Distortion"></a><h3>Fun House Mirror Distortion</h3><p>The fun house mirror distortion <code>kernel</code> routine is provided as the default <code>kernel</code> routine for the image unit template in Xcode. (You’ll learn how to use the image unit template in <span class="content_text"><a href="../WritingtheObjective-CPortion/WritingtheObjective-CPortion.html#//apple_ref/doc/uid/TP40004531-CH4-SW1">“Writing the Objective-C Portion.”</a></span>) Similar to a mirror in a carnival fun house, this filter distorts an image by stretching and magnifying a vertical strip of the image. Compare <span class="content_text"><a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW37">Figure 2-15</a></span> with <span class="content_text"><a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW13">Figure 2-3</a></span>.</p><br/><div><a name="//apple_ref/doc/uid/TP40004531-CH3-SW37" title="Figure 2-15A  gazelle image distorted by a fun house mirror routine"></a><p><strong>Figure 2-15&nbsp;&nbsp;</strong>A  gazelle image distorted by a fun house mirror routine</p><img src = "../ART/distorted_gazelle.jpg" alt = "A  gazelle image distorted by a fun house mirror routine" ></div><br/><p>The fun house <code>kernel</code> routine shown in <span class="content_text"><a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW35">Listing 2-6</a></span> takes the following parameters:</p><ul class="spaceabove"><li class="li"><p><code>src</code> is the <code>sampler</code> that  fetches image data from a source image.</p></li><li class="li"><p><code>center_x</code> is the x coordinate that defines the center of the vertical strip in which the warping takes place.</p></li><li class="li"><p><code>inverse_radius</code> is the inverse of the radius. You can avoid a division operation in the <code>kernel</code> routine by performing this calculation outside the routine, in the Objective-C portion of the filter.</p></li><li class="li"><p><code>radius</code> is  the extent of the effect.</p></li><li class="li"><p><code>scale</code> specifies the amount of warping.</p></li></ul><p>The <code>mySmoothstep</code> routine in <span class="content_text">Listing 2-6</span> is a custom smoothing function to ensure that the pixels at the edge of the effect blend with the rest of the image.</p><a name="//apple_ref/doc/uid/TP40004531-CH3-SW35" title="Listing 2-6Code that creates a fun house mirror distortion"></a><p class="codesample"><strong>Listing 2-6&nbsp;&nbsp;</strong>Code that creates a fun house mirror distortion</p><div class="codesample"><table><tr><td scope="row"><pre>float mySmoothstep(float x)<span></span></pre></td></tr><tr><td scope="row"><pre>{<span></span></pre></td></tr><tr><td scope="row"><pre>    return (x * -2.0 + 3.0) * x * x;<span></span></pre></td></tr><tr><td scope="row"><pre>}<span></span></pre></td></tr><tr><td scope="row"><pre> <span></span></pre></td></tr><tr><td scope="row"><pre>kernel vec4 funHouse(sampler src, float center_x, float inverse_radius,<span></span></pre></td></tr><tr><td scope="row"><pre>            float radius, float scale) <span>// 1</span></pre></td></tr><tr><td scope="row"><pre>{<span></span></pre></td></tr><tr><td scope="row"><pre>    float distance;<span></span></pre></td></tr><tr><td scope="row"><pre>    vec2 myTransform1, adjRadius;<span></span></pre></td></tr><tr><td scope="row"><pre> <span></span></pre></td></tr><tr><td scope="row"><pre>    myTransform1 = destCoord(); <span>// 2</span></pre></td></tr><tr><td scope="row"><pre>    adjRadius.x = (myTransform1.x - center_x) * inverse_radius; <span>// 3</span></pre></td></tr><tr><td scope="row"><pre>    distance = clamp(abs(adjRadius.x), 0.0, 1.0); <span>// 4</span></pre></td></tr><tr><td scope="row"><pre>    distance = mySmoothstep(1.0 - distance) * (scale - 1.0) + 1.0; <span>// 5</span></pre></td></tr><tr><td scope="row"><pre>    myTransform1.x = adjRadius.x * distance * radius + center_x; <span>// 6</span></pre></td></tr><tr><td scope="row"><pre>    return sample(src, samplerTransform(src, myTransform1)); <span>// 7</span></pre></td></tr><tr><td scope="row"><pre>}<span></span></pre></td></tr></table></div><p>Here’s what the code does:</p><ol class="ol"><li class="li"><p>Takes a <code>sampler</code> and four <code>float</code> values as parameters. Note that when you write the Objective-C portion of the filter, you must pass each <code>float</code> value as an <code>NSNumber</code> object. Otherwise, the filter will not work. See <span class="content_text"><a href="../Overview/Overview.html#//apple_ref/doc/uid/TP40004531-CH6-SW4">“Kernel Routine Rules.”</a></span></p></li><li class="li"><p>Fetches the position, in working-space coordinates, of the pixel currently being computed.</p></li><li class="li"><p>Computes an x coordinate that’s adjusted for it’s distance from the center of the effect. </p></li><li class="li"><p>Computes a distance value based on the adjusted x coordinate and that varies between <code>0</code> and <code>1</code>. Essentially, this normalizes the distance.</p></li><li class="li"><p>Adjusts the normalized distance value so that is varies along a curve. The <code>scale</code> value determines the height of the curve. The radius value used previously to calculate the distance determines the width of the curve. </p></li><li class="li"><p>Computes a transformation vector.</p></li><li class="li"><p>Returns the pixel located at the position in the coordinate space after the coordinate space is transformed by the <code>myTransform1</code> vector.</p></li></ol><p>Take a look at the default image unit in Xcode to see what’s required for the Objective-C portion of the image unit. (See <span class="content_text"><a href="../WritingtheObjective-CPortion/WritingtheObjective-CPortion.html#//apple_ref/doc/uid/TP40004531-CH4-SW9">“The Image Unit Template in Xcode.”</a></span>) You’ll see that a region-of-interest method is required. You’ll also notice that the inverse radius calculation is computed in the <code>outputImage</code> method.</p><a name="//apple_ref/doc/uid/TP40004531-CH3-SW25" title="Writing Kernel Routines for a Detective Lens "></a><h2>Writing Kernel Routines for a Detective Lens </h2><p>This section describes a more sophisticated use of <code>kernel</code> routines. You’ll see how to create two <code>kernel</code> routines that could stand on their own, but later, in <span class="content_text"><a href="../WritingtheObjective-CPortion/WritingtheObjective-CPortion.html#//apple_ref/doc/uid/TP40004531-CH4-SW4">“Creating a Detective Lens Image Unit,”</a></span> you’ll see how to combine them to create a filter that, to the user, will look similar to a physical magnification lens, as shown in <span class="content_text">Figure 2-16</span>. </p><br/><div><a name="//apple_ref/doc/uid/TP40004531-CH3-SW38" title="Figure 2-16The ideal detective lens "></a><p><strong>Figure 2-16&nbsp;&nbsp;</strong>The ideal detective lens </p><img src = "../ART/detective_lens.jpg" alt = "The ideal detective lens" ></div><br/><p>To create this effect, it’s necessary to perform tasks outside the <code>kernel</code> routine. You’ll need Objective-C code to set up region-of-interest routines, to set up input parameters for each <code>kernel</code> routine, and to pass the output image produced by each <code>kernel</code> routine to a compositing filter. You’ll see how to accomplish these tasks later. After a discussion of the problem and the components of a detective lens, you’ll see how to write each of the <code>kernel</code> routines.</p><a name="//apple_ref/doc/uid/TP40004531-CH3-SW59" title="The Problem: Why a Detective Lens?"></a><h3>The Problem: Why a Detective Lens?</h3><p>The resolution of images taken by today’s digital cameras have outpaced the resolution of computer displays. Images are typically downsampled by image editing applications to allow the user to see the entire image onscreen. Unfortunately, the downsampling hides the details in the image. One solution is to show the image using a 1:1 ratio between the screen resolution and the image resolution. This solution is not ideal, because only a portion of the image is displayed onscreen, causing the user to scroll in order to view other parts of the image.</p><p>This is where the detective lens filter comes in. The filter allows the user to inspect the details of part of a high resolution image, similar to what’s shown in <span class="content_text"><a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW30">Figure 2-17</a></span>. The filter does not actually magnify the original image. Instead, it displays a downsampled image for the pixels that are not underneath the lens and fetches pixels from the unscaled original image for the pixels that are underneath the lens.</p><br/><div><a name="//apple_ref/doc/uid/TP40004531-CH3-SW30" title="Figure 2-17A detective lens that enlarges a portion of a high-resolution image"></a><p><strong>Figure 2-17&nbsp;&nbsp;</strong>A detective lens that enlarges a portion of a high-resolution image</p><img src = "../ART/gazelle_lens_ring.jpg" alt = "A detective lens that enlarges a portion of a high resolution image" ></div><br/><div class="notebox"><a name="//apple_ref/doc/uid/TP40004531-CH3-SW60" title="Note"></a><p><strong>Note:</strong>&nbsp;Why a detective lens? Although the filter could just as easily be called a loupe filter, “detective” more readily calls to mind the act of searching for details.</p></div><a name="//apple_ref/doc/uid/TP40004531-CH3-SW45" title="Detective Lens Anatomy"></a><h3>Detective Lens Anatomy</h3><p>Before writing any <code>kernel</code> routine, it’s helpful to understand the parameters that control the image processing effect you want to achieve. <span class="content_text">Figure 2-18</span> shows a diagram of the top view of the lens. The lens, has a center and a diameter. The lens holder has a width. The lens also has: </p><ul class="spaceabove"><li class="li"><p>An opacity. Compare the colors underneath the lens with those outside the lens in <span class="content_text"><a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW30">Figure 2-17</a></span>.</p></li><li class="li"><p>Reflectivity, which can  cause a shiny spot on the lens if the lens does not have a modern reflection-free coating. </p></li></ul><br/><div><a name="//apple_ref/doc/uid/TP40004531-CH3-SW39" title="Figure 2-18The parameters of a detective lens "></a><p><strong>Figure 2-18&nbsp;&nbsp;</strong>The parameters of a detective lens </p><img src = "../ART/lens_diagram.gif" alt = "The parameters of a detective lens" width="346" height="271"></div><br/><p><span class="content_text">Figure 2-19</span> shows another characteristic of the lens that influences its effect—roundness. This lens is convex, but the height shown in the diagram (along with the lens diameter) controls how curved the lens is.</p><br/><div><a name="//apple_ref/doc/uid/TP40004531-CH3-SW40" title="Figure 2-19A side view of a detective lens"></a><p><strong>Figure 2-19&nbsp;&nbsp;</strong>A side view of a detective lens</p><img src = "../ART/side_view.jpg" alt = "A side view of a detective lens" ></div><br/><p>The lens holder has additional characteristics as you can see in <span class="content_text">Figure 2-20</span>. This particular lens holder has a fillet. A fillet is a strip of material that rounds off an interior angle between two sections. By looking at the cross section, you’ll see that the lens holder can have three parts to it—an inner sloping section, an outer sloping section, and a flat top. The fillet radius determines whether there is a flat top, as shown in the figure. If the fillet radius is half the lens holder width, there is no flat top. If the fillet radius is less than half the lens holder width, there will be a flattened section as shown in the figure.</p><br/><div><a name="//apple_ref/doc/uid/TP40004531-CH3-SW41" title="Figure 2-20A cross section of the lens holder"></a><p><strong>Figure 2-20&nbsp;&nbsp;</strong>A cross section of the lens holder</p><img src = "../ART/holder_cross_section.jpg" alt = "A cross section of the lens holder" ></div><br/><p>Next you’ll take a look at the <code>kernel</code> routines needed to produce the lens and lens holder characteristics.</p><a name="//apple_ref/doc/uid/TP40004531-CH3-SW16" title="The Lens Kernel Routine"></a><h3>The Lens Kernel Routine</h3><p>The lens <code>kernel</code> routine must produce an effect similar to a physical lens. Not only should the routine appear to magnify what’s underneath it, but it should be slightly opaque and reflect some light. <span class="content_text">Figure 2-21</span> shows a lens with those characteristics. </p><br/><div><a name="//apple_ref/doc/uid/TP40004531-CH3-SW42" title="Figure 2-21A checkerboard pattern magnified by a lens filter"></a><p><strong>Figure 2-21&nbsp;&nbsp;</strong>A checkerboard pattern magnified by a lens filter</p><img src = "../ART/lens_only.jpg" alt = "A checkerboard pattern magnified by a lens filter" ></div><br/><p>In previous sections, you’ve seen how to write routines that require only one <code>sampler</code>. The <code>kernel</code> routine that produced the effect shown in <span class="content_text">Figure 2-21</span> requires three <code>sampler</code> objects for fetching pixels from:</p><ul class="spaceabove"><li class="li"><p>The high-resolution image. Recall that the purpose of the lens is to allow the user to inspect details in an image that’s too large to fit onscreen. This <code>sampler</code> object fetches pixels to show underneath the lens—the part of the image that will appear magnified. Depending on the amount of magnification desired by the filter client, the <code>sampler</code> might need to downsample the high resolution image.</p></li><li class="li"><p>A downsampled version of the high-resolution image. This <code>sampler</code> object fetches pixels to show outside the lens—the part of the image that will not appear to be magnified. </p></li><li class="li"><p>The highlight image. These samples are used to generate highlights in the lens to give the appearance of the lens being reflective. <span class="content_text">Figure 2-22</span> shows the highlight image. The highlights are so subtle, that to reproduce the image for this document, transparent pixels are represented as black. White pixels are opaque. The highlight shown in the figure is exaggerated so that it can be seen here. </p></li></ul><p>Recall that the setup work for <code>sampler</code> objects is done in the Objective-C portion of an image unit. See <span class="content_text"><a href="../Overview/Overview.html#//apple_ref/doc/uid/TP40004531-CH6-SW3">“Division of Labor.”</a></span> You’ll see how to set up the <code>CISampler</code> objects in <span class="content_text"><a href="../WritingtheObjective-CPortion/WritingtheObjective-CPortion.html#//apple_ref/doc/uid/TP40004531-CH4-SW4">“Creating a Detective Lens Image Unit.”</a></span></p><br/><div><a name="//apple_ref/doc/uid/TP40004531-CH3-SW21" title="Figure 2-22An image used to generate lens highlights"></a><p><strong>Figure 2-22&nbsp;&nbsp;</strong>An image used to generate lens highlights</p><img src = "../ART/lensshine.jpg" alt = "An image used to generate lens highlights" ></div><br/><p>The lens <code>kernel</code> routine (see <span class="content_text"><a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW19">Listing 2-7</a></span>) takes nine input parameters:</p><ul class="spaceabove"><li class="li"><p><code>downsampled_src</code> is the <code>sampler</code> associated with the  downsampled version of the image. </p></li><li class="li"><p><code>hires_src</code> is the <code>sampler</code> associated with the high resolution image.</p></li><li class="li"><p><code>highlights</code> is the <code>sampler</code> associated with the highlight image.</p></li><li class="li"><p><code>center</code> is a two-element vector that specifies the center of the magnifying lens.</p></li><li class="li"><p><code>radius</code> is the radius of the magnifying lens.</p></li><li class="li"><p><code>roundness</code> is a value that specifies how convex the lens is.</p></li><li class="li"><p><code>opacity</code> specifies how opaque the glass of the magnifying lens is. If the lens has a reflection-free coating, this value is <code>0.0</code>. If it is as reflective as possible, the value is <code>1.0</code>. </p></li><li class="li"><p><code>highlight size</code> is a two-element vector that specifies the height and width of the highlight image.</p></li></ul><p>Now that you know a bit about the lens characteristics and the input parameters needed for the <code>kernel</code> routine, take a look at <span class="content_text">Listing 2-7</span>. After the necessary declarations, the routine starts out by calculating normalized distance. The three lines of code that perform this calculation are typical of routines that operate within a portion of an image. Part of the routine is devoted to calculating and applying a transform that determines which pixel from the highlight image to fetch, and then modifying the magnified pixel by the highlight pixel. You’ll find more information in the detailed explanation for each numbered line of code that follows the listing.</p><a name="//apple_ref/doc/uid/TP40004531-CH3-SW19" title="Listing 2-7A kernel routine for a lens"></a><p class="codesample"><strong>Listing 2-7&nbsp;&nbsp;</strong>A <code>kernel</code> routine for a lens</p><div class="codesample"><table><tr><td scope="row"><pre>kernel vec4 lens(sampler downsampled_src, sampler highres_src, sampler highlights,<span></span></pre></td></tr><tr><td scope="row"><pre>            vec2 center, float radius, float magnification,<span></span></pre></td></tr><tr><td scope="row"><pre>            float roundness, float opacity, vec2 highlightsSize) <span>// 1</span></pre></td></tr><tr><td scope="row"><pre>{<span></span></pre></td></tr><tr><td scope="row"><pre>    float dist, mapdist; <span>// 2</span></pre></td></tr><tr><td scope="row"><pre>    vec2 v0; <span>// 3</span></pre></td></tr><tr><td scope="row"><pre>    vec4 pix, pix2, mappix; <span>// 4</span></pre></td></tr><tr><td scope="row"><pre> <span></span></pre></td></tr><tr><td scope="row"><pre>    v0 = destCoord() - center; <span>// 5</span></pre></td></tr><tr><td scope="row"><pre>    dist = length(v0); <span>// 6</span></pre></td></tr><tr><td scope="row"><pre>    v0 = normalize(v0); <span>// 7</span></pre></td></tr><tr><td scope="row"><pre> <span></span></pre></td></tr><tr><td scope="row"><pre>    pix = sample(downsampled_src, samplerCoord(downsampled_src)); <span>// 8</span></pre></td></tr><tr><td scope="row"><pre>    mapdist = (dist / radius) * roundness; <span>// 9</span></pre></td></tr><tr><td scope="row"><pre>    mappix = sample(highlights, samplerTransform(highlights,<span></span></pre></td></tr><tr><td scope="row"><pre>                highlightsSize * (v0 * mapdist + 1.0) * 0.5)); <span>// 10</span></pre></td></tr><tr><td scope="row"><pre>    mappix *= opacity; <span>// 11</span></pre></td></tr><tr><td scope="row"><pre>    pix2 = sample(highres_src, samplerCoord(highres_src)); <span>// 12</span></pre></td></tr><tr><td scope="row"><pre>    pix2 = mappix + (1.0 - mappix.a) * pix2; <span>// 13</span></pre></td></tr><tr><td scope="row"><pre> <span></span></pre></td></tr><tr><td scope="row"><pre>    return mix(pix, pix2, clamp(radius - dist, 0.0, 1.0)); <span>// 14</span></pre></td></tr><tr><td scope="row"><pre>}<span></span></pre></td></tr></table></div><p>Here’s what the code does: </p><ol class="ol"><li class="li"><p>Takes three <code>sampler</code> objects, four <code>float</code> values, and two <code>vec2</code> data types as parameters. Note that when you write the Objective-C portion of the filter, you must pass each <code>float</code> and <code>vec2</code> values as <code>NSNumber</code> objects. Otherwise, the filter will not work. See <span class="content_text"><a href="../Overview/Overview.html#//apple_ref/doc/uid/TP40004531-CH6-SW4">“Kernel Routine Rules.”</a></span></p></li><li class="li"><p>Declares two variables: <code>dist</code> provides intermediate storage for calculating a <code>mapdist</code> distance. <code>mapdist</code> is used to determine which pixel to fetch from the highlight image.</p></li><li class="li"><p>Declares a two-element vector for storing normalized distance.</p></li><li class="li"><p>Declares three four-element vectors for storing pixel values associated with the three <code>sampler</code> sources.</p></li><li class="li"><p>Subtracts the vector that represents the center point of the lens from the vector that represents the destination coordinate.</p></li><li class="li"><p>Calculates the length of the difference vector.</p><div class="notebox"><a name="//apple_ref/doc/uid/TP40004531-CH3-SW61" title="Note"></a><p><strong>Note:</strong>&nbsp;The <code>length</code> function is defined in the <span class="content_text"><a href="http://www.opengl.org/documentation/glsl/" target="_blank">OpenGL Shading Language specification</a></span>: </p><code>float length(genType x);</code></p>It returns the length of a vector, calculated by this formula:</p><code>sqrt (x1*x2 + y1*y2)</code> </p></div></li><li class="li"><p>Normalizes the distance vector.</p><div class="notebox"><a name="//apple_ref/doc/uid/TP40004531-CH3-SW62" title="Note"></a><p><strong>Note:</strong>&nbsp;The <code>normalize</code> function is defined in the <span class="content_text"><a href="http://www.opengl.org/documentation/glsl/" target="_blank">OpenGL Shading Language specification</a></span>: </p><code>genType normalize(genType x);</code></p>It returns a vector with a length of <code>1</code> that lies in the same direction as <code>x</code>.</p></div></li><li class="li"><p>Fetches a pixel from the downsampled image. Recall that this image represents the pixels that appear outside the lens—the “unmagnified” pixels.</p></li><li class="li"><p>Calculates the distance value that is needed to determine which pixel to fetch from the highlight image. This calculation is needed because the size of the highlight image is independent of the diameter of the lens. The calculation ensures that the highlight image stretches or shrinks to fit the area of the lens.</p></li><li class="li"><p>Fetches a pixel from the highlight image by applying a transform  based on the distance function and the size of the highlight.</p></li><li class="li"><p>Modifies the pixel fetched from the highlight image to account for the opacity of the lens.</p></li><li class="li"><p>Fetches a pixel from the high resolution image. You’ll see later (<span class="content_text"><a href="../WritingtheObjective-CPortion/WritingtheObjective-CPortion.html#//apple_ref/doc/uid/TP40004531-CH4-SW4">“Creating a Detective Lens Image Unit”</a></span>) that the magnification is applied in the Objective-C portion of the image unit.</p></li><li class="li"><p>Modifies the pixel from the high resolution image by the opacity-adjusted highlight pixel.</p></li><li class="li"><p>Softens the edge between the magnified (high resolution image) and unmagnified (downsampled image) pixels. </p><p>The <code>mix</code> and <code>clamp</code> functions provided by OpenGL Shading Language have hardware support and, as a result, are much more efficient for you to use than to implement your own.</p><p>The <code>clamp</code> function</p><p><code>genType clamp (genType x, float minValue, float maxValue)</code> </p><p>returns:</p><p> <code>min(max(x, minValue), maxValue)</code></p><p>If the destination coordinate falls within the area of the lens, the value of <code>x</code> is returned; otherwise <code>clamp</code> returns <code>0.0</code>.</p><p>The <code>mix</code> function </p><p><code>genType mix (genType x, genType y,float a)</code></p><p>returns the linear blend between the first two arguments (x, y) passed to the function:</p><p><code>x * (1 - a) + y * a</code></p><p>If the destination coordinate falls outside of the area of the lens (<code>a = 0.0</code>), <code>mix</code> returns an unmagnified pixel. If the destination coordinate falls on the edges of the lens (<code>a = 1.0</code>), <code>mix</code> returns a linear blend of the magnified and unmagnified pixels.  If the destination coordinate inside the area of the lens, <code>mix</code> returns  magnified pixel.</p></li></ol><a name="//apple_ref/doc/uid/TP40004531-CH3-SW17" title="The Lens Holder Kernel Routine"></a><h3>The Lens Holder Kernel Routine</h3><p>The lens holder <code>kernel</code> routine is generator routine in that is does not operate on any pixels from the source image. The <code>kernel</code> routine generates an image from a material map, and that image sits on top of the source image. See <span class="content_text">Figure 2-23</span>. </p><br/><div><a name="//apple_ref/doc/uid/TP40004531-CH3-SW43" title="Figure 2-23A magnifying lens holder placed over a checkerboard pattern"></a><p><strong>Figure 2-23&nbsp;&nbsp;</strong>A magnifying lens holder placed over a checkerboard pattern</p><img src = "../ART/ring_only.jpg" alt = "A magnifying lens holder placed over a checkerboard pattern." ></div><br/><p>The material map for the lens holder is shown in <span class="content_text">Figure 2-24</span>. It is a digital photograph of a highly reflective ball. You can just as easily use another image of reflective material. </p><br/><div><a name="//apple_ref/doc/uid/TP40004531-CH3-SW22" title="Figure 2-24A material map image"></a><p><strong>Figure 2-24&nbsp;&nbsp;</strong>A material map image</p><img src = "../ART/emap1.jpg" alt = "A material map image" ></div><br/><p>The <code>kernel</code> routine performs several calculations to determine which pixel to fetch from the material map for each location on the lens holder. The routine warps the material map to fit the inner part of the lens holder and warps it in reverse so that it fits the outer part of the lens holder. If the fillet radius is less than one-half the lens holder radius, the routine colors the flat portion of the lens holder by using pixels from the center portion of the material map. It may be a bit easier to see the results of the warping by looking at the lens holder in <span class="content_text">Figure 2-25</span>, which was created from a checkerboard pattern. The figure also demonstrates that if you don’t use an image that has reflections in it, the lens holder won’t look realistic. </p><br/><div><a name="//apple_ref/doc/uid/TP40004531-CH3-SW44" title="Figure 2-25A lens holder generated from a checkerboard image"></a><p><strong>Figure 2-25&nbsp;&nbsp;</strong>A lens holder generated from a checkerboard image</p><img src = "../ART/warp.jpg" alt = "A lens holder generated from a checkerboard image" ></div><br/><p>The lens holder <code>kernel</code> routine (see <span class="content_text"><a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW20">Listing 2-8</a></span>) takes six input parameters:</p><ul class="spaceabove"><li class="li"><p><code>material</code> is a <code>sampler</code> object that fetches samples from the material map shown in <span class="content_text"><a href="WritingKernels.html#//apple_ref/doc/uid/TP40004531-CH3-SW22">Figure 2-24</a></span>.</p></li><li class="li"><p><code>center</code> is a two-element vector that specifies the center of the lens that the lens holder is designed to hold.</p></li><li class="li"><p><code>innerRadius</code> is the distance from the center of the lens to the inner edge of the lens holder.</p></li><li class="li"><p><code>outerRadius</code> is the distance from the center of the lens to the outer edge of the lens holder.</p></li><li class="li"><p><code>filletRadius</code> is the distance from the lens holder edge towards the lens holder center. This value should be less than half the lens holder radius.</p></li><li class="li"><p><code>materialSize</code> is a two-element vector that specifies the height and width of the material map.</p></li></ul><p>The routine shown in <span class="content_text">Listing 2-8</span> starts with the necessary declarations. Similar to the lens <code>kernel</code> routine, the lens holder <code>kernel</code> routine  calculates normalized distance. Its effect is limited to a ring shape, so the normalized distance is needed to determine where to apply the effect. The routine also calculated whether the destination coordinate is on the inner or outer portion of the ring (that is, lens holder). Part of the routine constructs a transform that is then used to fetch a pixel from the material map. The material map size is independent of the inner and outer lens holder diameter, so a transform is necessary to perform the warping required to map the material onto the lens holder. You’ll find more information in the detailed explanation for each numbered line of code that follows the listing.</p><a name="//apple_ref/doc/uid/TP40004531-CH3-SW20" title="Listing 2-8A kernel routine that generates a magnifying lens holder"></a><p class="codesample"><strong>Listing 2-8&nbsp;&nbsp;</strong>A <code>kernel</code> routine that generates a magnifying lens holder</p><div class="codesample"><table><tr><td scope="row"><pre>kernel vec4 ring(sampler material, vec2 center, float innerRadius,<span></span></pre></td></tr><tr><td scope="row"><pre>                    float outerRadius, float filletRadius, vec2 materialSize) <span>// 1</span></pre></td></tr><tr><td scope="row"><pre>{<span></span></pre></td></tr><tr><td scope="row"><pre>    float dist, f, d0, d1, alpha;<span></span></pre></td></tr><tr><td scope="row"><pre>    vec2 t0, v0;<span></span></pre></td></tr><tr><td scope="row"><pre>    vec4 pix;<span></span></pre></td></tr><tr><td scope="row"><pre> <span></span></pre></td></tr><tr><td scope="row"><pre>    t0 = destCoord() - center; <span>// 2</span></pre></td></tr><tr><td scope="row"><pre>    dist = length(t0);<span>// 3</span></pre></td></tr><tr><td scope="row"><pre>    v0 = normalize(t0);<span>// 4</span></pre></td></tr><tr><td scope="row"><pre> <span></span></pre></td></tr><tr><td scope="row"><pre>    d0 = dist - (innerRadius + filletRadius); <span>// 5</span></pre></td></tr><tr><td scope="row"><pre>    d1 = dist - (outerRadius - filletRadius); <span>// 6</span></pre></td></tr><tr><td scope="row"><pre>    f = (d1 > 0.0) ? (d1 / filletRadius) : min(d0 / filletRadius, 0.0); <span>// 7</span></pre></td></tr><tr><td scope="row"><pre>    v0 = v0 * f; <span>// 8</span></pre></td></tr><tr><td scope="row"><pre> <span></span></pre></td></tr><tr><td scope="row"><pre>    alpha = clamp(dist - innerRadius, 0.0, 1.0) * clamp(outerRadius - dist, 0.0, 1.0); <span>// 9</span></pre></td></tr><tr><td scope="row"><pre> <span></span></pre></td></tr><tr><td scope="row"><pre>    v0 = materialSize * (v0 + 1.0) * 0.5; <span>// 10</span></pre></td></tr><tr><td scope="row"><pre>    pix = sample(material, samplerTransform(material, v0)); <span>// 11</span></pre></td></tr><tr><td scope="row"><pre> <span></span></pre></td></tr><tr><td scope="row"><pre>  return pix * alpha; <span>// 11</span></pre></td></tr><tr><td scope="row"><pre>}<span></span></pre></td></tr></table></div><p>Here’s what the code does:</p><ol class="ol"><li class="li"><p>Takes one <code>sampler</code> object, three <code>float</code> values, and two <code>vec2</code> data types as parameters. Note that when you write the Objective-C portion of the filter, you must pass each <code>float</code> and <code>vec2</code> value  as <code>NSNumber</code> objects. Otherwise, the filter will not work. See <span class="content_text"><a href="../Overview/Overview.html#//apple_ref/doc/uid/TP40004531-CH6-SW4">“Kernel Routine Rules.”</a></span></p></li><li class="li"><p>Subtracts the vector that represents the center point of the lens from the vector that represents the destination coordinate. </p></li><li class="li"><p>Calculates the length of the difference vector.</p></li><li class="li"><p>Normalizes the length.</p></li><li class="li"><p>Calculates whether the destination coordinate is on the inner portion of the lens holder.</p></li><li class="li"><p>Calculates whether the destination coordinate is on the outer portion of the lens holder.</p></li><li class="li"><p>Calculates a shaping value that depends on the location of the destination coordinate: <code>[-1...0]</code> in the inner portion of the lens holder, <code>[0...1]</code> in the outer portion of the lens holder, and <code>0</code> otherwise.</p><div class="notebox"><a name="//apple_ref/doc/uid/TP40004531-CH3-SW63" title="Note"></a><p><strong>Note:</strong>&nbsp;The <strong>ternary operator</strong><code>?:</code> is used as follows:</p><code>expression_1 ? expression_2 : expression_3</code></p>If expression 1 is true, then expression 2 is evaluated and expression 3 is ignored.</p>If expression 1 is false, then expression 3 is evaluated and expression 2 is ignored.</p></div></li><li class="li"><p>Modifies the normalized distance to account for the lens holder fillet. This value will shape the lens holder.</p></li><li class="li"><p>Calculates an alpha value for the pixel at the destination coordinate. If the the location falls short of the inner radius, the alpha value is clamped to <code>0.0</code>. Similarly, if the location overshoots the outer radius, the result is clamped to <code>0.0</code>. Alpha values within the lens holder are clamped to <code>1.0</code>. Pixels not on the lens holder are transparent.</p></li><li class="li"><p>Modifies the <code>v0</code> vector by the width and height of the material map. Then scales the vector to account for the size of the material map.</p></li><li class="li"><p>Fetches a pixel from the material map by applying the <code>v0</code> vector as a transform.</p></li><li class="li"><p>Applies alpha to the pixel prior to returning it. </p></li></ol><a name="//apple_ref/doc/uid/TP40004531-CH3-SW33" title="Solution to the Kernel Challenge"></a><h2>Solution to the Kernel Challenge</h2><p>The <code>kernel</code> routine for a monochrome filter should look similar to what’s shown in <span class="content_text">Listing 2-9</span>.</p><a name="//apple_ref/doc/uid/TP40004531-CH3-SW36" title="Listing 2-9A solution to the monochrome filter challenge"></a><p class="codesample"><strong>Listing 2-9&nbsp;&nbsp;</strong>A solution to the monochrome filter challenge</p><div class="codesample"><table><tr><td scope="row"><pre>kernel vec4 monochrome(sampler image, __color color)<span></span></pre></td></tr><tr><td scope="row"><pre>{<span></span></pre></td></tr><tr><td scope="row"><pre>    vec4 pixel;<span></span></pre></td></tr><tr><td scope="row"><pre>    pixel = sample(image, samplerCoord(image));<span></span></pre></td></tr><tr><td scope="row"><pre>    pixel.g = pixel.b = pixel.r;<span></span></pre></td></tr><tr><td scope="row"><pre>    return pixel * color;<span></span></pre></td></tr><tr><td scope="row"><pre>}<span></span></pre></td></tr></table></div><a name="//apple_ref/doc/uid/TP40004531-CH3-SW64" title="Next Steps"></a><h2>Next Steps</h2><p>Now that you’ve seen how to write a variety of <code>kernel</code> routines, you are ready to move on to writing the Objective-C portion of an image unit. The next chapter shows how to create a project from the Xcode image unit template. You’ll see how to create an image unit for several of the <code>kernel</code> routines described in this chapter.</p>

        <br /><br /> 
        
        <div class="mini_nav_text" align="left">
        <span class="navButtons">
        <a href="../Overview/Overview.html">&lt; Previous Page</a><span style="margin-left: 8px"><a href="../WritingtheObjective-CPortion/WritingtheObjective-CPortion.html">Next Page &gt;</a></span>
        </span>
        <span id="showHideTOCLowerSpan">
        <a href="#" onclick="showHideTOC();"><img src="../../../../Resources/Images/show_toc_icon.gif" width="15" height="14" border="0" style="margin-bottom: -2px;" alt="" /></a> <a href="#" onclick="showHideTOC();">Hide TOC</a>
        </span>
        </div>

        <br/><hr /><div align="center"><p class="content_text" lang="en" dir="ltr"> <!--#if expr="0=1" -->&#x00a9; 2009 Apple Inc. All Rights Reserved. &#40;<!--#endif -->Last updated: 2009-05-06<!--#if expr="0=1" -->&#041;<!--#endif --></p></div>

        
        <div class="hideOnPrint hideInXcode">
        <!-- start of footer -->
        	<table width="100%" border="0" cellpadding="0" cellspacing="0">
		<tr>
			<td><div style="width: 100%; height: 1px; background-color: #919699; margin-top: 5px; margin-bottom: 15px"></div></td>
		</tr>
		<tr>
			<td align="center"><br/>
				<table border="0" cellpadding="0" cellspacing="0" class="graybox">
					<tr>
						<th>Did this document help you?</th>
					</tr>
					<tr>
						<td>
						    <div style="margin-bottom: 8px"><a href="http://developer.apple.com/feedback/?v=1&url=/documentation/GraphicsImaging/Conceptual/ImageUnitTutorial/WritingKernels/WritingKernels.html%3Fid%3DTP40004531-2.1&media=dvd" target=_new>Yes</a>:  Tell us what works for you.</div>
							<div style="margin-bottom: 8px"><a href="http://developer.apple.com/feedback/?v=2&url=/documentation/GraphicsImaging/Conceptual/ImageUnitTutorial/WritingKernels/WritingKernels.html%3Fid%3DTP40004531-2.1&media=dvd" target=_new>It&#8217;s good, but:</a> Report typos, inaccuracies, and so forth.</div>
							<div><a href="http://developer.apple.com/feedback/?v=3&url=/documentation/GraphicsImaging/Conceptual/ImageUnitTutorial/WritingKernels/WritingKernels.html%3Fid%3DTP40004531-2.1&media=dvd" target=_new>It wasn&#8217;t helpful</a>: Tell us what would have helped.</div>
						</td>
					</tr>
				</table>
			</td>
		</tr>
	</table>

        <!--#include virtual="/includes/framesetfooter" -->
        <!-- end of footer -->
        </div>
    </div>
</body>
</html>