<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
	<title>Core Audio Overview: Core Audio Programming Interfaces</title>
	<meta id="Generator" name="Generator" content="Gutenberg"/>
	<meta id="GeneratorVersion" name="GeneratorVersion" content="v132"/>
	<meta http-equiv="content-type" content="text/html;charset=utf-8"/>
	<meta id="Copyright" name="Copyright" content="Copyright 2009 Apple Inc. All Rights Reserved."/>
	<meta id="IndexTitle" name="IndexTitle" content="Core Audio Programming Interfaces"/>
	<meta id="xcode-display" name="xcode-display" content="render"/>
	<meta id="toc-file" name="toc-file" content="../toc.html"/>
	<meta id="RESOURCES" content="../../../../Resources" />
	
	<link rel="stylesheet" type="text/css" href="../../../../Resources/CSS/frameset_styles.css"/>
	<script language="JavaScript" type="text/javascript" src="../../../../Resources/JavaScript/lib/prototype.js"></script>
	<script language="JavaScript" type="text/javascript" src="../../../../Resources/JavaScript/lib/scriptaculous.js"></script>
	<script language="JavaScript" type="text/javascript" src="../../../../Resources/JavaScript/page.js"></script>
	<script language="JavaScript" type="text/javascript" src="../../../../Resources/JavaScript/pedia.js"></script>
	<!--[if lte IE 6]>
		<style type="text/css">
			/*<![CDATA[*/ 
			html {overflow-x:auto; overflow-y:hidden;  }
			/*]]>*/
		</style>
	<![endif]-->
</head>    
<body bgcolor="#ffffff" onload="initialize_page();"><a name="//apple_ref/doc/uid/TP40003577-CH4" title="Core Audio Programming Interfaces"></a>
    <noscript>
    <div id="tocMenu">
        <iframe id="toc_content" name="toc_content" SRC="../toc.html" width="210" height="100%" align="left" frameborder="0">This document set is best viewed in a browser that supports iFrames.</iframe>
    </div>
    </noscript>
    <div id="bodyText">
        <a name="top"></a>
        <div class="hideOnPrint hideInXcode">
        <!-- start of header -->
        <!--#include virtual="/includes/framesetheader" -->
        <!-- end of header -->
        </div>
        
        <!-- start of path -->
<div class="breadcrumb hideOnPrint hideInXcode"><a href="http://developer.apple.com/" target="_top">ADC Home</a> &gt; <a href="../../../../../referencelibrary/index.html#//apple_ref/doc/uid/TP30000943" target="_top">Reference Library</a> &gt; <a href="../../../../index.html#//apple_ref/doc/uid/TP30000440" target="_top">Guides</a> &gt; <a href="../../../index.html#//apple_ref/doc/uid/TP30000440-TP30000428" target="_top">Audio</a> &gt; <a href="../../../CoreAudio-date.html#//apple_ref/doc/uid/TP30000440-TP30000428-TP30000500" target="_top">Core Audio</a> &gt; <a href="../Introduction/Introduction.html#//apple_ref/doc/uid/TP40003577-CH1-SW1">Core Audio Overview</a> &gt; </div><br class="hideInXcode"/><!-- end of path -->
        
        <div class="mini_nav_text" align="left">
        <span class="navButtons">
        <a href="../WhatisCoreAudio/WhatisCoreAudio.html">&lt; Previous Page</a><span style="margin-left: 8px"><a href="../ARoadmaptoCommonTasks/ARoadmaptoCommonTasks.html">Next Page &gt;</a></span>
        </span>
        <span id="showHideTOCUpperSpan">
        <a href="#" onclick="showHideTOC();"><img src="../../../../Resources/Images/show_toc_icon.gif" width="15" height="14" border="0" style="margin-bottom: -2px;" alt="" hideText="Hide TOC" showText="Show TOC" /></a> <a href="#" onclick="showHideTOC();">Hide TOC</a>
        </span>
        </div>

        <hr />
        
        
        <a name="//apple_ref/doc/uid/TP40003577-CH4-SW4" title="Core Audio Programming Interfaces"></a><h1>Core Audio Programming Interfaces</h1><p>Core Audio is a comprehensive set of services for handling all audio tasks in Mac OS X, and as such it contains many constituent parts. This chapter describes the various programming interfaces of Core Audio.</p><p>For the purposes of this document, an <strong>API</strong> refers to a programming interface defined by a single header file, while a <strong>service</strong> is an interface defined by several header files. </p><p>For a complete list of Core Audio frameworks and the headers they contain, see <span class="content_text"><a href="../CoreAudioFrameworks/CoreAudioFrameworks.html#//apple_ref/doc/uid/TP40003577-CH9-SW1">“Core Audio Frameworks.”</a></span> </p>
<!-- This template is being used for both PDF and HTML. -->

    
    <h4>In this section:</h4>
    
    
    <p class="blockquote">
    
        
			
			
				<a href="WhatsinCoreAudio.html#//apple_ref/doc/uid/TP40003577-CH4-DontLinkElementID_23">Audio Unit Services</a>
				
			<br/>
			
        
			
			
				<a href="WhatsinCoreAudio.html#//apple_ref/doc/uid/TP40003577-CH4-SW6">Audio Processing Graph API</a>
				
			<br/>
			
        
			
			
				<a href="WhatsinCoreAudio.html#//apple_ref/doc/uid/TP40003577-CH4-DontLinkElementID_24">Audio File and Converter Services</a>
				
			<br/>
			
        
			
			
				<a href="WhatsinCoreAudio.html#//apple_ref/doc/uid/TP40003577-CH4-SW2">Hardware Abstraction Layer (HAL) Services</a>
				
			<br/>
			
        
			
			
				<a href="WhatsinCoreAudio.html#//apple_ref/doc/uid/TP40003577-CH4-DontLinkElementID_28">Music Player API</a>
				
			<br/>
			
        
			
			
				<a href="WhatsinCoreAudio.html#//apple_ref/doc/uid/TP40003577-CH4-SW11">Core MIDI Services and MIDI Server Services</a>
				
			<br/>
			
        
			
			
				<a href="WhatsinCoreAudio.html#//apple_ref/doc/uid/TP40003577-CH4-DontLinkElementID_29">Core Audio Clock API</a>
				
			<br/>
			
        
			
			
				<a href="WhatsinCoreAudio.html#//apple_ref/doc/uid/TP40003577-CH4-DontLinkElementID_30">OpenAL (Open Audio Library)</a>
				
			<br/>
			
        
			
			
				<a href="WhatsinCoreAudio.html#//apple_ref/doc/uid/TP40003577-CH4-DontLinkElementID_31">System Sound API</a>
				
			<br/>
			
        

    </p><br/>

<a name="//apple_ref/doc/uid/TP40003577-CH4-DontLinkElementID_23" title="Audio Unit Services"></a><h2>Audio Unit Services</h2><p>Audio Unit Services allows you to create and manipulate audio units. This interface consists of the functions, data types, and constants found in the following header files in <code>AudioUnit.framework</code> and <code>AudioToolbox.framework</code>:  </p><ul class="ul"><li class="li"><p><code>AudioUnit.h</code></p></li><li class="li"><p><code>AUComponent.h</code></p></li><li class="li"><p><code>AudioOutputUnit.h</code></p></li><li class="li"><p><code>AudioUnitParameters.h</code></p></li><li class="li"><p><code>AudioUnitProperties.h</code></p></li><li class="li"><p><code>AudioUnitCarbonView.h</code></p></li><li class="li"><p><code>AUCocoaUIView.h</code></p></li><li class="li"><p><code>MusicDevice.h</code></p></li><li class="li"><p><code>AudioUnitUtilities.h </code> (in <code>AudioToolbox.framework</code>)</p></li></ul><p>Audio units are plug-ins, specifically  Component Manager components, for handling or generating audio signals. Multiple instances of the same audio unit can appear in the same host application. They can appear virtually anywhere in an audio signal chain.</p><p>An audio unit must support the noninterleaved 32-bit floating-point linear PCM format to ensure compatibility with units from other vendors. It may also support other linear PCM variants. Currently audio units do not support audio formats other than linear PCM. To convert audio data of a different format to linear PCM, you can use an audio converter (see <span class="content_text"><a href="WhatsinCoreAudio.html#//apple_ref/doc/uid/TP40003577-CH4-SW1">“Audio Converters and Codecs.”</a></span> </p><div class="notebox"><a name="//apple_ref/doc/uid/TP40003577-CH4-DontLinkElementID_45" title="Note"></a><p><strong>Note:</strong>&nbsp;Audio File and Converter Services uses Component Manager components to handle custom file formats or data conversions. However, these components are not audio units. </p></div><p>Host applications must use Component Manager calls to discover and load audio units. Each audio unit is uniquely identified by a combination of the Component Manager type, subtype, and manufacturer’s code. The type indicates the general purpose of the unit (effect unit, generator unit, and so on). The subtype is an arbitrary value that uniquely identifies an audio unit of a given type by a particular manufacturer. For example, if your company supplies several different effect units, each must have a distinct subtype to distinguish them from each other. Apple defines the standard audio unit types, but you are free to create any subtypes you wish.  </p><p>Audio units describe their capabilities and configuration information using <strong>properties</strong>. Properties are key-value pairs that describe non-time varying characteristics, such as the number of channels in an audio unit, the audio data stream format it supports, the sampling rate it accepts, and whether or not the unit supports a custom Cocoa view. Each audio unit type has several required properties, as defined by Apple, but you are free to define additional properties based on your unit’s needs. Host applications can use property information to create user interfaces for a unit, but in many cases, more sophisticated audio units supply their own custom user interfaces. </p><p>Audio units also contain various <strong>parameters</strong>, the types of which depend on the capabilities of the audio unit. Parameters typically represent settings that are adjustable in real time, often by the end user. For example, a parametric filter audio unit may have parameters determining the center frequency and the width of the filter response, which may be set using the user interface. An instrument unit, on the other hand, uses parameters to represent the current state of MIDI or event data.  </p><p>A signal chain composed of audio units typically ends in an output unit. An output unit often interfaces with hardware (the AUHAL is such an output unit, for example), but this is not a requirement. Output units differ from other audio units in that they are the only units that can start and stop data flow independently. Standard audio units rely on a "pull" mechanism to obtain data. Each audio unit registers a callback with its successor in the audio chain. When an output unit starts the data flow (triggered by the host application), its render function calls back to the previous unit in the chain to ask for data, which in turn calls its predecessor, and so on. </p><p>Host applications can combine audio units in an audio processing graph to create larger signal processing modules. Combining units in a processing graph automatically creates the callback links to allow data flow through the chain. See <span class="content_text"><a href="WhatsinCoreAudio.html#//apple_ref/doc/uid/TP40003577-CH4-SW6">“Audio Processing Graph API”</a></span> for more information. </p><p>To monitor changes in the state of an audio unit, applications can register callbacks ("listeners") that are invoked when particular audio unit events occur. For example, an application might want to know when a parameter changes value or when the data flow is interrupted. See <span class="content_text"><a href="../../../../../technotes/tn2002/tn2104.html" target="_top">Technical Note TN2104: Handling Audio Unit Events</a></span> for more details. </p><p>The Core Audio SDK (in its  <code>AudioUnits</code> folder) provides templates for common audio unit types (for example, effect units and instrument units) along with a C++ framework that implements most of the Component Manager plug-in interface for you. </p><p>For more detailed information about building audio units using the SDK, see the <em>Audio Unit Programming Guide</em>. </p><a name="//apple_ref/doc/uid/TP40003577-CH4-SW6" title="Audio Processing Graph API"></a><h2>Audio Processing Graph API</h2><p>The Audio Processing Graph API lets audio unit host application developers create and manipulate audio processing graphs. The Audio Processing Graph API consists of the functions, data types, and constants defined in the header file <code>AUGraph.h</code> in <code>AudioToolbox.framework</code>.</p><p>An audio processing graph (sometimes called an AUGraph) defines a collection of audio units strung together to perform a particular task. This arrangement lets you create modules of common processing tasks that you can easily add to and remove from your signal chain. For example, a graph could connect several audio units together to distort a signal, compress it, and then pan it to a particular location in the soundstage. You can end a graph with the AUHAL to transmit the sound to a hardware device (such as an amplifier/speaker). Audio processing graphs are useful for applications that primarily handle signal processing by connecting audio units rather than implementing the processing themselves.  <span class="content_text">Figure 2-1</span> shows a simple audio processing graph.</p><br/><div><a name="//apple_ref/doc/uid/TP40003577-CH4-SW10" title="Figure 2-1A simple audio processing graph"></a><p><strong>Figure 2-1&nbsp;&nbsp;</strong>A simple audio processing graph</p><img src = "../Art/audio_proc_graph_simple.jpg" alt = "" ></div><br/><p>Each audio unit in an audio processing graph is called a <strong>node</strong>. You make a connection by attaching an output from one node to the input of another. You can't connect an output from one audio unit to more than one audio unit input unless you use a splitter unit, as shown in <span class="content_text">Figure 2-2</span>. However, an audio unit may contain multiple outputs or inputs, depending on its type.   </p><br/><div><a name="//apple_ref/doc/uid/TP40003577-CH4-SW9" title="Figure 2-2Incorrect and correct ways to fan out a connection"></a><p><strong>Figure 2-2&nbsp;&nbsp;</strong>Incorrect and correct ways to fan out a connection</p><img src = "../Art/correct_incorrect_connections.jpg" alt = "" ></div><br/><p>You can use the Audio Processing Graph API to combine subgraphs into a larger graph, where a subgraph appears as a single node in the larger graph, as shown in <span class="content_text">Figure 2-3</span>.</p><br/><div><a name="//apple_ref/doc/uid/TP40003577-CH4-SW5" title="Figure 2-3A subgraph within a larger audio processing graph"></a><p><strong>Figure 2-3&nbsp;&nbsp;</strong>A subgraph within a larger audio processing graph</p><img src = "../Art/audio_processing_graph.jpg" alt = "" ></div><br/><p>Each graph or subgraph must end in an output audio unit. In the case of a subgraph, the signal path should end with the generic output unit, which does not connect to any hardware. </p><p>While it is possible to link audio units programmatically without using an audio processing graph, you can modify a graph dynamically, allowing you to change the signal path while processing audio data. In addition, because a graph represents simply an interconnection of audio units, you can create and modify a graph without having to instantiate the audio units it references. </p><a name="//apple_ref/doc/uid/TP40003577-CH4-DontLinkElementID_24" title="Audio File and Converter Services"></a><h2>Audio File and Converter Services</h2><p>Audio File and Converter Services lets you read or write audio data, either to a file or to a buffer, and allows you to convert the data between any number of different formats. This service consists of the functions, data types, and constants defined in the following header files in <code>AudioToolbox.framework</code> and <code>AudioUnit.framework</code>: </p><ul class="ul"><li class="li"><p><code>ExtendedAudioFile.h</code></p></li><li class="li"><p><code>AudioFile.h</code></p></li><li class="li"><p><code>AudioFormat.h</code></p></li><li class="li"><p><code>AudioConverter.h</code></p></li><li class="li"><p><code>AudioCodec.h</code> (located in <code>AudioUnit.framework</code>). </p></li><li class="li"><p><code>CAFFile.h</code></p></li></ul><p>In many cases, you use the Extended Audio File API, which provides the simplest interface for reading and writing audio data. Files read using this API are automatically uncompressed and/or converted into linear PCM format, which is the native format for audio units. Similarly, you can use one function call to write linear PCM audio data to a file in a compressed or converted format. <span class="content_text"><a href="../SupportedAudioConverterFormats/SupportedAudioConverterFormats.html#//apple_ref/doc/uid/TP40003577-CH7-SW1">“Supported Audio File and Data Formats”</a></span> lists the file formats that Core Audio supports by default. Some formats have restrictions; for example, by default, Core Audio can read, but not write, MP3 files, and an AC-3 file can be decoded only to a stereo data stream (not 5.1 surround). </p><p>If you need more control over the file reading, file writing, or data conversion process, you can access the Audio File and Audio Converter APIs directly (in <code>AudioFile.h</code> and <code>AudioConverter.h</code>). When using the Audio File API, the audio data source (as represented by an audio file object) can be either an actual file or a buffer in memory. In addition, if your application reads and writes proprietary file formats, you can handle the format translation using custom Component Manager components that the Audio File API can discover and load.  For example, if your file format incorporates DRM, you would want to create a custom component to handle that process. </p><a name="//apple_ref/doc/uid/TP40003577-CH4-SW1" title="Audio Converters and Codecs"></a><h3>Audio Converters and Codecs</h3><p>An audio converter lets you convert audio data from one format to another. For example, you can make simple conversions such as changing the sample rate and interleaving or deinterleaving audio data streams, to more complex operations such as compressing or decompressing audio. Three types of conversions are possible:</p><ul class="spaceabove"><li class="li"><p>Decoding an audio format (such as AAC (Advanced Audio Coding)) to linear PCM format.</p></li><li class="li"><p>Encoding linear PCM data into a different audio format. </p></li><li class="li"><p>Translating between different variants of linear PCM (for example, converting 16-bit signed integer linear PCM to 32-bit floating point linear PCM). </p></li></ul><p>The Audio Converter API lets you create and manipulate audio converters. You can use the API with many built-in converters to handle most common audio formats. You can instantiate more than one converter at a time, and specify the converter to use when calling a conversion function. Each audio converter has properties that describe characteristics of the converter. For example, a channel mapping property also allows you to specify how the input channels should map to the output channels.  </p><p>You convert data by calling a conversion function with a particular converter instance, specifying where to find the input data and where to write the output. Most conversions require a callback function to periodically supply input data to the converter.   </p><p>An audio codec is a Component Manager component loaded by an audio converter to encode or decode a specific audio format. Typically a codec would decode to or encode from linear PCM. The Audio Codec API provides the Component Manager interface necessary for implementing an audio codec. After you create a custom codec, then you can use an audio converter to access it.  <span class="content_text"><a href="../SupportedAudioConverterFormats/SupportedAudioConverterFormats.html#//apple_ref/doc/uid/TP40003577-CH7-SW1">“Supported Audio File and Data Formats”</a></span> lists standard Core Audio codecs for translating between compressed formats and Linear PCM.</p><p>For examples of how to use audio converters, see <code>SimpleSDK/ConvertFile</code> and the <code>AFConvert</code> command-line tool in <code>Services/AudioFileTools</code> in the Core Audio SDK.  </p><a name="//apple_ref/doc/uid/TP40003577-CH4-DontLinkElementID_25" title="File Format Information"></a><h3>File Format Information</h3><p>In addition to reading, writing, and conversion, Audio File and Converter Services can obtain useful information about file types and the audio data a file contains. For example, you can obtain data such as the following using the Audio File API:  </p><ul class="spaceabove"><li class="li"><p>File types that Core Audio can read or write</p></li><li class="li"><p>Data formats that the Audio File API can read or write</p></li><li class="li"><p>The name of a given file type</p></li><li class="li"><p>The file extension(s) for a given file type</p></li></ul><p>The Audio File API also allows you to set or read properties associated with a file. Examples of properties include the data format stored in the file and a CFDictionary containing metadata such as the genre, artist, and copyright information. </p><a name="//apple_ref/doc/uid/TP40003577-CH4-DontLinkElementID_26" title="Audio Metadata"></a><h3>Audio Metadata</h3><p>When handling audio data, you often need specific information about the data so you know how to best process it.  The Audio Format API (in <code>AudioFormat.h</code>) allows you to query information stored in various audio structures. For example, you might want to know some of the following characteristics:</p><ul class="spaceabove"><li class="li"><p>Information associated with a particular channel layout (number of channels, channel names, input to output mapping). </p></li><li class="li"><p>Panning matrix information, which you can use for mapping between channel layouts. </p></li><li class="li"><p>Sampling rate, bit rate, and other basic information. </p></li></ul><p>In addition to this information, you can also use the Audio Format API to obtain specific information about the system related to Core Audio, such as the audio codecs that are currently available.</p><a name="//apple_ref/doc/uid/TP40003577-CH4-DontLinkElementID_27" title="Core Audio File Format"></a><h3>Core Audio File Format</h3><p>Although technically not a part of the Core Audio programming interface, the Core Audio File format (CAF) is a powerful and flexible file format , defined by Apple, for storing audio data. CAF files have no size restrictions (unlike AIFF, AIFF-C, and WAVE files) and can support a wide range of metadata, such as channel information and text annotations. The CAF format is flexible enough to contain any audio data format, even formats that do not exist yet. For detailed information about the Core Audio File format, see <em><a href="../../../Reference/CAFSpec/index.html#//apple_ref/doc/uid/TP40001862" target="_top">Apple Core Audio Format Specification 1.0</a></em>. </p><a name="//apple_ref/doc/uid/TP40003577-CH4-SW2" title="Hardware Abstraction Layer (HAL) Services "></a><h2>Hardware Abstraction Layer (HAL) Services </h2><p>Core Audio uses a hardware abstraction layer (HAL) to provide a consistent and predictable interface for applications to deal with hardware. Each piece of hardware is represented by an audio device object (type  <code>AudioDevice</code>) in the HAL. Applications can query the audio device object to obtain timing information that can be used for synchronization or to adjust for latency.  </p><p>HAL Services consists of the functions, data types, and constants defined in the following header files in <code>CoreAudio.framework</code>:</p><ul class="ul"><li class="li"><p><code>AudioDriverPlugin.h</code></p></li><li class="li"><p><code>AudioHardware.h</code></p></li><li class="li"><p><code>AudioHardwarePlugin.h</code></p></li><li class="li"><p><code>CoreAudioTypes.h</code> (Contains data types and constants used by all Core Audio interfaces)</p></li><li class="li"><p><code>HostTime.h</code></p></li></ul><p>Most developers will find that Apple’s AUHAL unit serves their hardware interface needs, so they don’t have to interact directly with the HAL Services. The AUHAL is responsible for transmitting audio data, including any required channel mapping, to the specified audio device object. For more information about using the AUHAL and output units, see <span class="content_text"><a href="../ARoadmaptoCommonTasks/ARoadmaptoCommonTasks.html#//apple_ref/doc/uid/TP40003577-CH6-SW12">“Interfacing with Hardware Devices.”</a></span> </p><a name="//apple_ref/doc/uid/TP40003577-CH4-DontLinkElementID_28" title="Music Player API"></a><h2>Music Player API</h2><p>The Music Player API allows you to arrange and play a collection of music tracks. It consists of the functions, data types, and constants defined in the header file <code>MusicPlayer.h</code> in <code>AudioToolbox.framework</code>. </p><p>A particular stream of MIDI or event data is a <strong>track</strong> (represented by the <code>MusicTrack</code> type). Tracks contain a series of time-based events, which can be MIDI data, Core Audio event data, or your own custom event messages. A collection of tracks is a <strong>sequence</strong> (type <code>MusicSequence</code>). A sequence always contains an additional tempo track, which synchronizes the playback of all tracks in the sequence. Your application can add, delete, or edit tracks in a sequence dynamically. Each sequence must be assigned to a corresponding <strong>music player</strong> object (type <code>MusicPlayer</code>), which acts as the overall controller for all the tracks in the sequence. </p><p>A track is analogous to sheet music for an instrument, indicating which notes to play and for how long. A sequence is similar to a musical score, which contains notes for multiple instruments. Instrument units or external MIDI devices represent the musical instruments, while the music player is similar to the conductor who keeps all the musicians coordinated. </p><p>Track data played by a music player can be sent to an audio processing graph, an external MIDI device, or a combination of the two. The audio processing graph receives the track data through one or more instrument units, which convert the event (or MIDI) data into actual audio signals.  The music player automatically communicates with the graph's output audio unit or Core MIDI to ensure that the audio output is properly synchronized.</p><p>Track data does not have to represent musical information. For example, special Core Audio events can represent changes in audio unit parameter values. A track assigned to a panner audio unit might send parameter events to alter the position of a sound source in the soundstage over time. Tracks can also contain proprietary user events that trigger an application-defined callback. </p><p>For more information about using the Music Player API to play MIDI data, see <span class="content_text"><a href="../ARoadmaptoCommonTasks/ARoadmaptoCommonTasks.html#//apple_ref/doc/uid/TP40003577-CH6-SW13">“Handling MIDI Data.”</a></span> </p><a name="//apple_ref/doc/uid/TP40003577-CH4-SW11" title="Core MIDI Services and MIDI Server Services"></a><h2>Core MIDI Services and MIDI Server Services</h2><p>Core Audio uses Core MIDI Services for MIDI support.  These services consist of the functions, data types, and constants defined in the following header files in <code>CoreMIDI.framework</code>: </p><ul class="ul"><li class="li"><p><code>MIDIServices.h</code></p></li><li class="li"><p><code>MIDISetup.h</code></p></li><li class="li"><p><code>MIDIThruConnection.h</code></p></li><li class="li"><p><code>MIDIDriver.h</code></p></li></ul><p>Core MIDI Services defines an interface that applications and audio units can use to communicate with MIDI devices. It uses a number of abstractions that allow an application to interact with a MIDI network. </p><p>A <strong>MIDI endpoint</strong> (defined by an opaque type <code>MIDIEndpointRef</code>) represents a source or destination for a standard 16-channel MIDI data stream, and it is the primary conduit for interacting with Core Audio services. For example, you can associate endpoints with tracks used by the Music Player API, allowing you to record or play back MIDI data. A MIDI endpoint is a logical representation of a standard MIDI cable connection. MIDI endpoints do not necessarily have to correspond to a physical device, however; an application can set itself up as a virtual source or destination to send or receive MIDI data.</p><p>MIDI drivers often combine multiple endpoints into logical groups, called <strong>MIDI entities</strong> (<code>MIDIEntityRef</code>). For example, it would be reasonable to group a MIDI-in endpoint and a MIDI-out endpoint as a MIDI entity, which can then be easily referenced for bidirectional communication with a device or application. </p><p>Each physical MIDI device (not a single MIDI connection) is represented by a Core MIDI device object (<code>MIDIDeviceRef</code>). Each device object may contain one or more MIDI entities. </p><p>Core MIDI communicates with the MIDI Server, which does the actual job of passing MIDI data between applications and devices. The MIDI Server runs in its own process, independent of any application.  <span class="content_text">Figure 2-4</span> shows the relationship between Core MIDI and MIDI Server. </p><br/><div><a name="//apple_ref/doc/uid/TP40003577-CH4-SW8" title="Figure 2-4Core MIDI and Core MIDI Server"></a><p><strong>Figure 2-4&nbsp;&nbsp;</strong>Core MIDI and Core MIDI Server</p><img src = "../Art/core_midi_core_server.jpg" alt = "" ></div><br/><p>In addition to providing an application-agnostic base for MIDI communications, MIDI Server also handles any MIDI thru connections, which allows device-to device chaining without involving the host application.  </p><p>If you are a MIDI device manufacturer, you may need to supply a CFPlugin plug-in for the MIDI Server packaged in a CFBundle to interact with the kernel-level I/O Kit drivers.  <span class="content_text">Figure 2-5</span> shows how Core MIDI and Core MIDI Server interact with the underlying hardware. </p><div class="notebox"><a name="//apple_ref/doc/uid/TP40003577-CH4-DontLinkElementID_46" title="Note"></a><p><strong>Note:</strong>&nbsp;If you create a USB MIDI class-compliant device, you do not have to write your own driver, because Apple’s supplied USB driver will support your hardware. </p></div><br/><div><a name="//apple_ref/doc/uid/TP40003577-CH4-SW3" title="Figure 2-5MIDI Server interface with I/O Kit"></a><p><strong>Figure 2-5&nbsp;&nbsp;</strong>MIDI Server interface with I/O Kit</p><img src = "../Art/midi_io.jpg" alt = "" ></div><br/><p>The drivers for each MIDI device generally exist outside the kernel, running in the MIDI Server process. These drivers interact with the default I/O Kit drivers for the underlying protocols (such as USB and FireWire). The MIDI drivers are responsible for presenting the raw device data to Core MIDI in a usable format. Core MIDI then passes the MIDI information to your application through the designated MIDI endpoints, which are the abstract representations of the MIDI ports on the external devices.</p><p>MIDI devices on PCI cards, however, cannot be controlled entirely through a user-space driver. For PCI cards, you must create a kernel extension to provide a custom user client. This client must either control the PCI device itself (providing a simple message queue for the user-space driver) or map the address range of the PCI device into the address of the MIDI server when requested to do so by the user-space driver. Doing so allows the user-space driver to control the PCI device directly.  </p><p>For an example of implementing a user-space MIDI driver, see <code>MIDI/SampleUSBDriver</code> in the Core Audio SDK.</p><a name="//apple_ref/doc/uid/TP40003577-CH4-DontLinkElementID_29" title="Core Audio Clock API"></a><h2>Core Audio Clock API</h2><p>The Core Audio Clock API, as defined in the header file <code>CoreAudioClock.h</code> in <code>AudioToolbox.framework</code>, provides a reference clock that you can use to synchronize applications or devices. This clock may be a standalone timing source, or it can be synchronized with an external trigger, such as a MIDI beat clock or MIDI time code. You can start and stop the clock yourself, or you can set the clock to activate or deactivate in response to certain events. </p><p>You can obtain the generated clock time in a number of formats, including seconds, beats, SMPTE time, audio sample time, and bar-beat time. The latter describes the time in a manner that is easy to display onscreen in terms of musical bars, beats, and subbeats. The Core Audio Clock API also contains utility functions that convert one time format to another and that display bar-beat or SMPTE times. <span class="content_text">Figure 2-6</span> shows the interrelationship between various Core Audio Clock formats.  </p><br/><div><a name="//apple_ref/doc/uid/TP40003577-CH4-SW7" title="Figure 2-6Some Core Audio Clock formats"></a><p><strong>Figure 2-6&nbsp;&nbsp;</strong>Some Core Audio Clock formats</p><img src = "../Art/core_audio_clock_formats.jpg" alt = "" ></div><br/><p>The hardware times represent absolute time values from either the host time (the system clock) or an audio time obtained from an external audio device (represented by an <code>AudioDevice</code> object in the HAL). You determine the current host time by calling <code>mach_absolute_time</code> or <code>UpTime</code>. The audio time is the audio device’s current time represented by a sample number. The sample number’s rate of change depends on the audio device’s sampling rate.  </p><p>The media times represent common timing methods for audio data. The canonical representation is in seconds, expressed as a double-precision floating point value. However, you can use a tempo map to translate seconds into musical bar-beat time, or apply a SMPTE offset to convert seconds to SMPTE seconds.</p><p>Media times do not have to correspond to real time. For example, an audio file that is 10 seconds long will take only 5 seconds to play if you double the playback rate. The knob in <span class="content_text"><a href="WhatsinCoreAudio.html#//apple_ref/doc/uid/TP40003577-CH4-SW7">Figure 2-6</a></span> indicates that you can adjust the correlation between the absolute (“real”) times and the media-based times.  For example, bar-beat notation indicates the rhythm of a musical line and what notes to play when, but does not indicate how long it takes to play. To determine that, you need to know the playback rate (say, in beats per second). Similarly, the correspondence of SMPTE time to actual time depends on such factors as the frame rate and whether frames are dropped or not.  </p><a name="//apple_ref/doc/uid/TP40003577-CH4-DontLinkElementID_30" title="OpenAL (Open Audio Library)"></a><h2>OpenAL (Open Audio Library)</h2><p>Core Audio includes a Mac OS X implementation of the open-source OpenAL specification. OpenAL is a cross-platform API used to position and manipulate sounds in a simulated three-dimensional space. For example, you can use OpenAL for positioning and moving sound effects in a game, or creating a sound space for multichannel audio. In addition to simple positioning sound around a listener, you can also add distancing effects through a medium (such as fog or water), doppler effects, and so on.  </p><p>The OpenAL coding conventions and syntax were designed to mimic OpenGL (only controlling sound rather than light), so OpenGL programmers should find many concepts familiar. </p><p>For an example of using OpenAL in Core Audio, see <code>Services/OpenALExample</code> in the Core Audio SDK. For more details about OpenAL, including programming information and API references, see <span class="content_text"><a href="http:/openal.org" target="_blank">openal.org</a></span>.</p><a name="//apple_ref/doc/uid/TP40003577-CH4-DontLinkElementID_31" title="System Sound API"></a><h2>System Sound API</h2><p>The System Sound API provides a simple way to play standard system sounds in your application. Its header, <code>SystemSound.h</code> is the only Core Audio header located in a non-Core Audio framework. It is located in the  <code>CoreServices/OSServices</code> framework. </p><p>For more details about using the System Sound API, see <span class="content_text"><a href="../../../../../technotes/tn2002/tn2102.html" target="_top">Technical Note 2102: The System Sound APIs for Mac OS X v10.2, 10.3, and Later</a></span>. </p>

        <br /><br /> 
        
        <div class="mini_nav_text" align="left">
        <span class="navButtons">
        <a href="../WhatisCoreAudio/WhatisCoreAudio.html">&lt; Previous Page</a><span style="margin-left: 8px"><a href="../ARoadmaptoCommonTasks/ARoadmaptoCommonTasks.html">Next Page &gt;</a></span>
        </span>
        <span id="showHideTOCLowerSpan">
        <a href="#" onclick="showHideTOC();"><img src="../../../../Resources/Images/show_toc_icon.gif" width="15" height="14" border="0" style="margin-bottom: -2px;" alt="" /></a> <a href="#" onclick="showHideTOC();">Hide TOC</a>
        </span>
        </div>

        <br/><hr /><div align="center"><p class="content_text" lang="en" dir="ltr"> <!--#if expr="0=1" -->&#x00a9; 2007 Apple Inc. All Rights Reserved. &#40;<!--#endif -->Last updated: 2007-01-08<!--#if expr="0=1" -->&#041;<!--#endif --></p></div>

        
        <div class="hideOnPrint hideInXcode">
        <!-- start of footer -->
        	<table width="100%" border="0" cellpadding="0" cellspacing="0">
		<tr>
			<td><div style="width: 100%; height: 1px; background-color: #919699; margin-top: 5px; margin-bottom: 15px"></div></td>
		</tr>
		<tr>
			<td align="center"><br/>
				<table border="0" cellpadding="0" cellspacing="0" class="graybox">
					<tr>
						<th>Did this document help you?</th>
					</tr>
					<tr>
						<td>
						    <div style="margin-bottom: 8px"><a href="http://developer.apple.com/feedback/?v=1&url=/documentation/MusicAudio/Conceptual/CoreAudioOverview/WhatsinCoreAudio/WhatsinCoreAudio.html%3Fid%3DTP40003577-1.2&media=dvd" target=_new>Yes</a>:  Tell us what works for you.</div>
							<div style="margin-bottom: 8px"><a href="http://developer.apple.com/feedback/?v=2&url=/documentation/MusicAudio/Conceptual/CoreAudioOverview/WhatsinCoreAudio/WhatsinCoreAudio.html%3Fid%3DTP40003577-1.2&media=dvd" target=_new>It&#8217;s good, but:</a> Report typos, inaccuracies, and so forth.</div>
							<div><a href="http://developer.apple.com/feedback/?v=3&url=/documentation/MusicAudio/Conceptual/CoreAudioOverview/WhatsinCoreAudio/WhatsinCoreAudio.html%3Fid%3DTP40003577-1.2&media=dvd" target=_new>It wasn&#8217;t helpful</a>: Tell us what would have helped.</div>
						</td>
					</tr>
				</table>
			</td>
		</tr>
	</table>

        <!--#include virtual="/includes/framesetfooter" -->
        <!-- end of footer -->
        </div>
    </div>
</body>
</html>