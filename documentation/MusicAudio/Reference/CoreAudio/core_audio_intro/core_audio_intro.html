<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
	<title>Core Audio (Legacy): Core Audio Overview</title>
	<meta id="Generator" name="Generator" content="Gutenberg"/>
	<meta id="GeneratorVersion" name="GeneratorVersion" content="v132"/>
	<meta http-equiv="content-type" content="text/html;charset=utf-8"/>
	<meta id="Copyright" name="Copyright" content="Copyright 2009 Apple Inc. All Rights Reserved."/>
	<meta id="IndexTitle" name="IndexTitle" content="Core Audio Overview"/>
	<meta id="xcode-display" name="xcode-display" content="render"/>
	<meta id="toc-file" name="toc-file" content="../toc.html"/>
	<meta id="RESOURCES" content="../../../../Resources" />
	<meta name="ROBOTS" content="NOINDEX"/>
	<link rel="stylesheet" type="text/css" href="../../../../Resources/CSS/frameset_styles.css"/>
	<script language="JavaScript" type="text/javascript" src="../../../../Resources/JavaScript/lib/prototype.js"></script>
	<script language="JavaScript" type="text/javascript" src="../../../../Resources/JavaScript/lib/scriptaculous.js"></script>
	<script language="JavaScript" type="text/javascript" src="../../../../Resources/JavaScript/page.js"></script>
	<script language="JavaScript" type="text/javascript" src="../../../../Resources/JavaScript/pedia.js"></script>
	<!--[if lte IE 6]>
		<style type="text/css">
			/*<![CDATA[*/ 
			html {overflow-x:auto; overflow-y:hidden;  }
			/*]]>*/
		</style>
	<![endif]-->
</head>    
<body bgcolor="#ffffff" onload="initialize_page();"><a name="//apple_ref/doc/uid/TP30001108-CH209" title="Core Audio Overview"></a>
    <noscript>
    <div id="tocMenu">
        <iframe id="toc_content" name="toc_content" SRC="../toc.html" width="210" height="100%" align="left" frameborder="0">This document set is best viewed in a browser that supports iFrames.</iframe>
    </div>
    </noscript>
    <div id="bodyText">
        <a name="top"></a>
        <div class="hideOnPrint hideInXcode">
        <!-- start of header -->
        <!--#include virtual="/includes/framesetheader" -->
        <!-- end of header -->
        </div>
        
        <!-- start of path -->
<div class="breadcrumb hideOnPrint hideInXcode"><a href="http://developer.apple.com/" target="_top">ADC Home</a> &gt; <a href="../../../../../referencelibrary/index.html#//apple_ref/doc/uid/TP30000943" target="_top">Reference Library</a> &gt; <a href="../../../../../reference/index.html#//apple_ref/doc/uid/TP30001281" target="_top">Reference</a> &gt; <a href="../../../../../reference/LegacyTechnologies/index.html#//apple_ref/doc/uid/TP30001281-TP30000470" target="_top">Legacy Documents</a> &gt; <a href="../../../../../reference/LegacyTechnologies/idxMusicAudio-date.html#//apple_ref/doc/uid/TP30001281-TP30000470-TP30000452" target="_top">Audio</a> &gt; <a href="../book_intro/book_intro.html#//apple_ref/doc/uid/TP30001108-CH201-DontLinkElementID_85">Core Audio (Legacy)</a> &gt; </div><br class="hideInXcode"/><!-- end of path -->
        
        <div class="mini_nav_text" align="left">
        <span class="navButtons">
        <a href="../book_intro/book_intro.html">&lt; Previous Page</a><span style="margin-left: 8px"><a href="../audiocodec/audiocodec.html">Next Page &gt;</a></span>
        </span>
        <span id="showHideTOCUpperSpan">
        <a href="#" onclick="showHideTOC();"><img src="../../../../Resources/Images/show_toc_icon.gif" width="15" height="14" border="0" style="margin-bottom: -2px;" alt="" hideText="Hide TOC" showText="Show TOC" /></a> <a href="#" onclick="showHideTOC();">Hide TOC</a>
        </span>
        </div>

        <hr />
        
        <script type="text/javascript" language="JavaScript">placeWatermark()</script>
<div id="legacyOuterWrapper"><div align="center" id="watermark">
<div class="legacybox">
<h1>Legacy Document<span class="closebutton"><a href="javascript:closeWatermark()"><img src="../../../../Resources/Images/closebutton.png" width="14" height="14" border="0" align="top" alt="close button"></a></span></h1><p><b>Important:</b>
The information in this document is obsolete and should not be used for new development.</p></div></div></div>
        <a name="//apple_ref/doc/uid/TP30001108-CH209-DontLinkElementID_87" title="Core Audio Overview"></a><h1><a name="//apple_ref/doc/uid/TP30001108-CH209-TPXREF101" title="Core Audio Overview"></a>Core Audio Overview</h1><p>This chapter will provide you with an understanding of the architecture of Core Audio, and how the various pieces fit together functionally.</p>
<!-- This template is being used for both PDF and HTML. -->

    
    <h4>In this section:</h4>
    
    
    <p class="blockquote">
    
        
			
			
				<a href="core_audio_intro.html#//apple_ref/doc/uid/TP30001108-CH209-TPXREF102">Apple&#8217;s Objectives</a>
				
			<br/>
			
        
			
			
				<a href="core_audio_intro.html#//apple_ref/doc/uid/TP30001108-CH209-TPXREF104">Introduction to Core Audio</a>
				
			<br/>
			
        
			
			
				<a href="core_audio_intro.html#//apple_ref/doc/uid/TP30001108-CH209-85579">Using Core Audio</a>
				
			<br/>
			
        

    </p><br/>

<a name="//apple_ref/doc/uid/TP30001108-CH209-101164" title="Apple&amp;#8217;s Objectives"></a><a name="//apple_ref/doc/uid/TP30001108-CH209-TPXREF102" title="Apple&amp;#8217;s Objectives"></a><h2>Apple&#8217;s Objectives</h2><p>In creating Core Audio, Apple&#8217;s objective in the audio space has been twofold. The primary goal is to deliver a high-quality, superior audio experience for Macintosh users. The second objective reflects a shift in emphasis from developers having to establish their own audio and MIDI protocols in their applications to Apple moving ahead to assume responsibility for these services on the Macintosh platform. </p><p>Some of the key features of the Core Audio architecture available in Mac OS X include:</p><ul class="ul"><li class="li"><p>A flexible audio format</p></li><li class="li"><p>Multichannel audio I/O</p></li><li class="li"><p>Support for both PCM and non-PCM formats</p></li><li class="li"><p>32-bit floating point native-endian PCM as the canonical format</p></li><li class="li"><p>Fully specifiable sample rates</p></li><li class="li"><p>Multiple application usage of audio devices</p></li><li class="li"><p>Application determined latency</p></li><li class="li"><p>Ubiquity of timing information</p></li><li class="li"><p>Both C and Java APIs</p></li></ul><p>Figure 2-1 illustrates the Core Audio architecture in Mac OS X and its various building blocks. </p><br/><div><a name="//apple_ref/doc/uid/TP30001108-CH209-101417" title="Figure 2-1The Core Audio Architecture"></a><p><strong>Figure 2-1&nbsp;&nbsp;</strong>The Core Audio Architecture</p><img src = "../Art/audio_midi_01.gif" alt = "" width="204" height="244"></div><br/><p>The theory of operation behind the Core Audio architecture is discussed in subsequent chapters of this document. </p><a name="//apple_ref/doc/uid/TP30001108-CH209-101462" title="Introduction to Core Audio "></a><a name="//apple_ref/doc/uid/TP30001108-CH209-TPXREF104" title="Introduction to Core Audio "></a><h2>Introduction to Core Audio </h2><a name="//apple_ref/doc/uid/TP30001108-CH209-101508" title="Hardware Abstraction Layer (HAL)"></a><a name="//apple_ref/doc/uid/TP30001108-CH209-TPXREF107" title="Hardware Abstraction Layer (HAL)"></a><h3>Hardware Abstraction Layer (HAL)</h3><div class="notebox"><a name="//apple_ref/doc/uid/TP30001108-CH209-DontLinkElementID_88" title="Note"></a><p><strong>Note:</strong>&nbsp;In its preliminary form, this document does not yet contain documentation for the Hardware Abstraction Layer. The final document will contain information on this technology.</p></div><p>The Hardware Abstraction Layer (HAL) is presented in the Core Audio framework and defines the lowest level of audio hardware access to the application. It presents the global properties of the system, such as the list of available audio devices. It also contains an <code>Audio Device</code> object that allows the application to read input data and write output data to an audio device that is represented by this object. It also provides the means to manipulate and control the device through a property mechanism.</p><p>The service allows for devices that use PCM encoded data. For PCM devices, the generic format is 32-bit floating point, maintaining a high resolution of the audio data regardless of the actual physical format of the device. This is also the generic format of PCM data streams throughout the Core Audio API.</p><p>An audio stream object represents n-channels of interleaved samples that correspond to a particular I/O end-point of the device itself. Some devices (for example, a card that has both digital and analog I/O) may present more than one audio stream. </p><p>The service provides the scheduling and user/kernel transitions required to both deliver and produce audio data to and from the audio device. Timing information is an essential component of this service; time stamps are ubiquitous throughout both the audio and MIDI system. This provides the capability to know the state of any particular sample (that is, &#8220;sample accurate timing&#8221;) of the device.</p><a name="//apple_ref/doc/uid/TP30001108-CH209-101553" title="Audio Unit"></a><a name="//apple_ref/doc/uid/TP30001108-CH209-TPXREF108" title="Audio Unit"></a><h3>Audio Unit</h3><p>An audio unit is a single processing unit that either is a source of audio data (for example, a software synthesizer), a destination of audio data (for example an audio unit that wraps an audio device), or both a source and destination (for example a DSP unit, such as a reverb, that takes audio data and processes or transforms this data).</p><p>The Audio Unit API uses a similar property mechanism as the Core Audio framework and use the same structures for both the buffers of audio data and timing information. Audio unit also provides real-time control capabilities, called parameters, that can be scheduled, allowing for changes in the audio rendering to be scheduled to a particular sample offset within any given &#8220;slice&#8221; of an audio unit&#8217;s rendering process.</p><p>An application can use an <code>AudioOutputUnit</code> to interface to a device. The <code>DefaultOutputAudioUnit</code> tracks the selection of a device by the user as the &#8220;default&#8221; output for audio, and provides additional services such as sample rate conversion, to provide a simpler means of interfacing to an output device.</p><a name="//apple_ref/doc/uid/TP30001108-CH209-58345" title="Audio Codec"></a><a name="//apple_ref/doc/uid/TP30001108-CH209-TPXREF109" title="Audio Codec"></a><h3>Audio Codec</h3><p>Audio codecs are the encoders and decoders available to the system for audio compression and decompression. Using the Audio Codec API for conversion between audio formats is deprecated in favor of using the Audio Converter API, described in the <span class="content_text"><a href="../audio_toolbox/audio_toolbox.html#//apple_ref/doc/uid/TP30001108-CH205-TPXREF101">“Audio Toolbox”</a></span> section.</p><p>Deploying an audio codec is performed by subclassing either the <code><!--a  -->ACBaseCodec<!--/a--></code> class or the <code><!--a  -->ACSimpleCodec<!--/a--></code> class, both provided in the Core Audio SDK. Once subclassed, the abstract methods (those set equal to zero) need to be provided, and the methods designated as virtual may be overridden as needed. </p><a name="//apple_ref/doc/uid/TP30001108-CH209-58366" title="Audio Toolbox"></a><a name="//apple_ref/doc/uid/TP30001108-CH209-BCIEHAHJ" title="Audio Toolbox"></a><h3>Audio Toolbox</h3><p>This framework currently provides five primary services: </p><ol class="ol"><li class="li"><p>Audio Converter provides format conversion services. When encoding or decoding audio data, Audio Converter should be utilized, as it allows for many different type and format conversions. It also allows for conversions between linear PCM data and compressed audio data.</p></li><li class="li"><p>Audio Format is provided to help handle information about different audio formats. It is able to inspect <code>AudioStreamBasicDescription</code> instances to provide information about various aspects of an audio stream. Also, the Audio Format API can provide information about the encoders and decoders available on the system.</p></li><li class="li"><p>Audio File provides file services for dealing with creating, opening, modifying, and saving audio files. It features file-creation and format-specification capabilities, as well as reading and writing mechanisms and the ability to open files in the file system. Audio File uses a property system to keeps track of a file&#8217;s file format, data format, channel layout, and more.</p></li><li class="li"><p>AUGraph allows for the construction and management of a signal processing graph of Audio Units, managing the connections and run-time state of the units that comprise a particular graph, including run-time management of inserting or removing nodes. The ubiquitous timing information in the signal chain deals with both feedback and fanning.</p></li><li class="li"><p>Music Sequence services provide a sequence object made up of one or more tracks of music events (both system-provided and user-defined). Track data can be edited while a sequence is playing, and its data can be iterated over. A music sequence typically addresses a graph of audio units, where tracks can be addressed to different nodes (audio units) of its graph, or a MIDI endpoint. A music player is responsible for the playing of a sequence.</p></li></ol><a name="//apple_ref/doc/uid/TP30001108-CH209-50466" title="MIDI Services"></a><a name="//apple_ref/doc/uid/TP30001108-CH209-TPXREF110" title="MIDI Services"></a><h3>MIDI Services</h3><div class="notebox"><a name="//apple_ref/doc/uid/TP30001108-CH209-DontLinkElementID_89" title="Note"></a><p><strong>Note:</strong>&nbsp;In its preliminary form, this document does not yet contain documentation for MIDI Services. Please consult the Core Audio SDK, available from <span class="content_text"><a href="http://developer.apple.com/audio" target="_top">http://developer.apple.com/audio</a></span>, for more information on developing MIDI Services.</p></div><p>This framework provides the representation of MIDI hardware and the interapplication communication of MIDI data to an application. The <code>MIDIDevice</code> object presents a MIDI-capable piece of hardware. A discrete MIDI source or destination (16 channels of MIDI data) is represented by the <code>MIDIEndpoint</code> object. This may be a real device or another application that is presented to your application as a virtual <code>MIDIEndpoint</code>, thus providing the interapplication communication of MIDI data.</p><p>The framework provides the I/O service and hosts the drivers that are supplied by both Apple and third-party companies to represent that hardware within the system.</p><a name="//apple_ref/doc/uid/TP30001108-CH209-85159" title="Core Audio Types"></a><h3>Core Audio Types</h3><p>Core Audio utilizes a series of structures and constants to encapsulate various pieces of information. <code>CoreAudioTypes.h</code> includes these structures, used consistently throughout Core Audio:</p><ul class="spaceabove"><li class="li"><p><code>AudioBufferList</code> encapsulates buffer data.</p></li><li class="li"><p><code>AudioStreamBasicDescription</code> encapsulates formatting information.</p></li><li class="li"><p><code>AudioTimeStamp</code> holds time stamp information.</p></li><li class="li"><p><code>AudioChannelLayout</code> specifies the layout of an audio sample&#8217;s layout.</p></li></ul><p>In addition, many constants are declared, including channel layout constants, used in identifying the layout of audio sources, and format ID constants, useful when specifying the format of the audio data.</p><a name="//apple_ref/doc/uid/TP30001108-CH209-85579" title="Using Core Audio"></a><h2>Using Core Audio</h2><p>There are many tasks that you can accomplish with Core Audio. This section will outline the architecture of Core Audio, highlighting the various uses of Mac OS X&#8217;s audio technology.</p><a name="//apple_ref/doc/uid/TP30001108-CH209-85619" title="Audio Data Operations"></a><h3>Audio Data Operations</h3><p>One of the main functions of Core Audio is to work with and manipulate audio data, that is either stored on disk or already in memory. Effects can be applied to the data, and data sources mixed. Beyond that, Core Audio is also responsible for pulling data from input devices, and outputting data back out. Finally, data can be put back out to disk as a file, and may be converted to another format.</p><br/><div><a name="//apple_ref/doc/uid/TP30001108-CH209-85638" title="Figure 2-2Reading in an audio file"></a><p><strong>Figure 2-2&nbsp;&nbsp;</strong>Reading in an audio file</p><img src = "../Art/reading_audio_file.gif" alt = "" width="452" height="181"></div><br/><p>In order to use audio data from a file, it first must be read in. The Audio File API is provided for this purpose. An audio file instance can be created to act as a proxy for the file on disk, or for a buffer in memory (using callbacks).</p><p>Once the audio file has been created and bound to a file or memory, its data can be read in. If the data in the file is encoded, an audio converter is needed to convert the data into 32 bit floating-point Pulse Code Modulated (PCM) native-endian data, also known as the canonical format. Once the data is in this format, it is ready to be used in an audio unit or by another portion of Core Audio.</p><p>It is worth noting that an audio converter instance inherently uses the audio codecs available on the system. Using an audio codec directly for this kind of data conversion is discouraged, since the Audio Converter API takes care of the actual buffering and other considerations that need to be considered during a conversion.</p><p>To write a file back out to disk, simply reverse this process. Data output in the canonical format can be converted to an encoded format with an audio converter, and then saved to disk or memory via the Audio File API.</p><br/><div><a name="//apple_ref/doc/uid/TP30001108-CH209-85707" title="Figure 2-3Converting audio files"></a><p><strong>Figure 2-3&nbsp;&nbsp;</strong>Converting audio files</p><img src = "../Art/audio_file_conversion.gif" alt = "" width="506" height="86"></div><br/><p>Converting a file uses a process similar to the previous example. The Audio File API is used to open the file off of disk, an audio converter takes the incoming data and converts it to the desired format, and another audio file instance is used to save the data out to disk. Again, the codecs needed to decode the incoming data and encode the outgoing data are used automatically by the converter; for instance, it is not necessary for you to read in the encoded data, convert it to the canonical format, and then encode it in the resulting format before writing it out. This service provided by the Audio Converter API.</p><br/><div><a name="//apple_ref/doc/uid/TP30001108-CH209-85768" title="Figure 2-4Playing back audio files"></a><p><strong>Figure 2-4&nbsp;&nbsp;</strong>Playing back audio files</p><img src = "../Art/audio_file_playback.gif" alt = "" width="510" height="190"></div><br/><p>Playing back the contents of an audio file is one of the most common tasks that developers perform. In Core Audio, this is accomplished by reading in the data using an audio file instance. Once the instance is set up, an I/O unit instance can pull on the file, extracting the audio data and outputting it to the assigned audio device. If the data is encoded in the canonical format, no further decoding is needed to output the sound. If the data is encoded, it will need to be converted into the canonical format before it can be played back.</p><p>An I/O unit is a type of audio unit that acts as a proxy for an audio device. When data is sent to it, it will be relayed to the device that it represents. The most common use of this is to send data to the default output, as specified by the user. The unit to use in this case is the <code>Default Output</code> unit. A <code>System Output</code> unit is also provided, which is discussed is the next example.</p><br/><div><a name="//apple_ref/doc/uid/TP30001108-CH209-85840" title="Figure 2-5I/O unit hierarchy"></a><p><strong>Figure 2-5&nbsp;&nbsp;</strong>I/O unit hierarchy</p><img src = "../Art/io_unit_hierarchy.gif" alt = "" width="240" height="258"></div><br/><p>Each I/O unit inherits from <code>AUConverter</code>, an audio unit which owns an audio converter instance; this unit can be used in a graph to convert data between formats, sample rates, and the like. A <code>GenericOutput</code> unit implements adds the ability to start and stop the pulling of data to the output device. </p><p>When playing out to any piece of hardware, an <code>AUHAL</code> unit is needed. An instance of <code>AUHAL</code> can be attached to any audio device, making the instance a proxy for getting input and providing output to that device.</p><p>The <code>Default Output</code> unit is provided to play audio out to the user&#8217;s prefered output, as designated in the System Preferences. Likewise, the <code>System Output</code> unit is provided to play back to the current system out device.</p><br/><div><a name="//apple_ref/doc/uid/TP30001108-CH209-85884" title="Figure 2-6Using an I/O unit for input and output"></a><p><strong>Figure 2-6&nbsp;&nbsp;</strong>Using an I/O unit for input and output</p><img src = "../Art/audio_playthrough.gif" alt = "" width="130" height="219"></div><br/><p>Any of these I/O units may be used to pull input data from its associated audio device, through any number or combination of audio units or audio unit graphs, and output back through the I/O unit. The I/O unit itself has two busses: 0 and 1, where the 0 bus is designated as the output, and the 1 bus is the input bus. The connections between the output of the 0 bus and the audio device and the input of the 1 bus and the audio device are made when the unit is associated with the device. </p><p>To process data from a device and play it back through, simply associate the device with the unit, connect the output of the 1 bus with whatever audio units or graphs are being used to process the data, and connect the output of those units to the input of the 0 bus on the I/O unit. To start the render, tell the I/O unit to render. This, in turn, will cause the unit to ask the units attached to it to render, eventually leading back to the I/O unit&#8217;s input bus, which will pull from the audio device. The data will pass through the input bus and will work its way through all of the attached units until it reaches the I/O units output bus, where it will automatically be output to the audio device.</p><br/><div><a name="//apple_ref/doc/uid/TP30001108-CH209-85953" title="Figure 2-7Audio Format Services"></a><p><strong>Figure 2-7&nbsp;&nbsp;</strong>Audio Format Services</p><img src = "../Art/audio_formats.gif" alt = "" width="513" height="160"></div><br/><p>When working with streams of audio data, information about the data and the formats that the system has available become important. The Audio Format API provides a mechanism to get information about audio data, like the available codecs for encoding and decoding information, the encoding information for channel layouts, and panning information for use with the Matrix Mixer audio unit. Also, the Audio File API provides a function useful for determining the available file formats on the system.</p><a name="//apple_ref/doc/uid/TP30001108-CH209-86078" title="MIDI Data Operations"></a><h3>MIDI Data Operations</h3><p>MIDI stands for Music Instrument Digital Interface. Established as the standard method of communication between music devices, Core Audio features full-fledged MIDI support, including provisions for communication with MIDI devices and reading-in and playback of standard MIDI files.</p><br/><div><a name="//apple_ref/doc/uid/TP30001108-CH209-86091" title="Figure 2-8Reading in a standard MIDI file"></a><p><strong>Figure 2-8&nbsp;&nbsp;</strong>Reading in a standard MIDI file</p><img src = "../Art/midi_file_read_in.gif" alt = "" width="182" height="141"></div><br/><p>The Music Sequence API is provided to sequence events for MIDI endpoints and audio units. One of its functions, though, is the ability to read in MIDI files and parse their contents into its tracks. Normally, each channel of MIDI data in the file can be made into one track in the sequence, allowing each track, and therefore, each channel of data, to be targeted at a different MIDI endpoint.</p><br/><div><a name="//apple_ref/doc/uid/TP30001108-CH209-86189" title="Figure 2-9Music Sequence play through"></a><p><strong>Figure 2-9&nbsp;&nbsp;</strong>Music Sequence play through</p><img src = "../Art/midi_file_audio_playback.gif" alt = "" width="299" height="244"></div><br/><p>To playback the MIDI file as audio data, a music player is assigned to a sequence, and the sequence&#8217;s tracks are assigned to a music device. A music device is a particular type of audio unit that generates audio data by having its parameters altered; in this case, the event track is assigned to a music device which is part of a graph, and the events in that track contain the parameter changes needed to affect the output of the music device. The graph itself is assigned to a sequence, so that the sequence knows which instances its tracks are assigned to. Beyond that, the music player assigned to the sequence communicates with the I/O unit at the head of the graph, to ensure that all timing issues for outputting sound to the unit&#8217;s assigned device are taken care of. This is done inherently when the sequence is assigned to the graph, and so no extra steps need to be taken in order for this synchronization to happen. The compressor is included in order to make sure a constant stream of data is being supplied to the I/O unit.</p><br/><div><a name="//apple_ref/doc/uid/TP30001108-CH209-86258" title="Figure 2-10Music Sequence play through"></a><p><strong>Figure 2-10&nbsp;&nbsp;</strong>Music Sequence play through</p><img src = "../Art/midi_device_playthrough.gif" alt = "" width="248" height="158"></div><br/><p>To play MIDI data back through an attached MIDI device, an event track needs to be assigned to a MIDI endpoint, a proxy for a MIDI device. As with the previous example, the music player will inherently communicate with Core MIDI to ensure all timing issues are solved and that a constant amount of data is being fed to the MIDI sever, and therefore, the MIDI device. </p><br/><div><a name="//apple_ref/doc/uid/TP30001108-CH209-86304" title="Figure 2-11MIDI device input"></a><p><strong>Figure 2-11&nbsp;&nbsp;</strong>MIDI device input</p><img src = "../Art/midi_input.gif" alt = "" width="485" height="105"></div><br/><p>When a MIDI control surface is being used to control the properties of a software component, like an audio unit, it will be assigned to an endpoint, which in turn, is assigned to an <code>AUMIDIController</code>, which will parse the incoming MIDI signals into parameter changes for use with an audio unit.</p><p>To playback the signals generated by a MIDI keyboard, a similar scheme is used. An endpoint is assigned to the keyboard, and the signals coming from the keyboard are assigned to an <code>AUMIDIController</code>, which, in turn, will issue parameter changes to a music device. The music device will synthesize the audio data, based on the parameters given to it via the <code>AUMIDIController</code>.</p><br/><div><a name="//apple_ref/doc/uid/TP30001108-CH209-86356" title="Figure 2-12MIDI input parsing"></a><p><strong>Figure 2-12&nbsp;&nbsp;</strong>MIDI input parsing</p><img src = "../Art/midi_file_recording.gif" alt = "" width="426" height="244"></div><br/><p>To take in MIDI data for saving, it is common to have already-existing data playing, while new data comes in and is recorded. The playback of existing data is handled as before, with the track being assigned to a music device, which outputs its data, via a graph, to an I/O unit. Beyond that, however, the data coming in from any MIDI device needs to be parsed and placed in another event track within the sequence. The Music Player API provides functions for determining when to place the events, based on the time the event happens.</p><a name="//apple_ref/doc/uid/TP30001108-CH209-86401" title="Higher Level Audio Operations"></a><h3>Higher Level Audio Operations</h3><p>Often, elements from the audio data operations and the MIDI data operations come together to provide a complete audio experience. These examples look at some cases where MIDI data is synthesized and also output to a MIDI device concurrently, or when events control a music device&#8217;s synthesis of audio data and the parameters of an audio unit, all while mixing in data from an encoded file being read off of disk.</p><br/><div><a name="//apple_ref/doc/uid/TP30001108-CH209-86418" title="Figure 2-13MIDI synthesis and output"></a><p><strong>Figure 2-13&nbsp;&nbsp;</strong>MIDI synthesis and output</p><img src = "../Art/midi_file_big_picture.gif" alt = "" width="513" height="262"></div><br/><p>In this example, you can see a music sequence being used to control the synthesis of sound, via an audio unit graph containing a music device, while additional events are sent to a MIDI endpoint, which, in turn, are assigned to MIDI devices. This is common when using the Mac as another MIDI device, generating synthesized data to accompany an external MIDI device. Note that the music player automatically takes care of all timing issues between the different outputs, ensuring that the output remains in sync.</p><br/><div><a name="//apple_ref/doc/uid/TP30001108-CH209-86460" title="Figure 2-14Mixing MIDI and audio data"></a><p><strong>Figure 2-14&nbsp;&nbsp;</strong>Mixing MIDI and audio data</p><img src = "../Art/mixing_audio_sources.gif" alt = "" width="440" height="399"></div><br/><p>This example focuses on an audio unit graph, which is used to mix synthesized MIDI data, via a music device, and audio data coming in from a file. This scenario is common in gaming situations, where ambient noises are saved as MIDI data, and the sound track is an encoded file on disk. Note that the sequence controls a 3D Mixer audio unit, often used to mix various audio sources and to provide a spacial orientation for the sources and the output. As with the previous example, the music player will ensure that the output is in sync with the sequence.</p><a name="//apple_ref/doc/uid/TP30001108-CH209-86505" title="Interfacing with Hardware"></a><h3>Interfacing with Hardware</h3><p>Most of the processing done with audio and MIDI data in Core Audio will eventually be played back via audio or MIDI hardware. As a developer, it will be helpful to you if you understand the architecture behind the hardware interfaces, even if an abstraction is used when developing an application.</p><br/><div><a name="//apple_ref/doc/uid/TP30001108-CH209-86526" title="Figure 2-15Audio hardware architecture"></a><p><strong>Figure 2-15&nbsp;&nbsp;</strong>Audio hardware architecture</p><img src = "../Art/hardware_abstraction_layer.gif" alt = "" width="448" height="263"></div><br/><p>When accessing audio hardware, whether it be via on-board audio inputs and outputs, USB, or other means, a driver must exist to handle the exchange of data between the hardware and the Mac. In order for the driver to be used by Core Audio, it must conform to the <code>IO Audio Family</code> of IOKit drivers; this means that the driver music implement <code>IO Audio Device</code> functionality within the driver, in order for proper communication to exist between itself and the Hardware Abstraction Layer.</p><p>The Hardware Abstraction Layer, or HAL, is provided to make discovery and access to audio hardware simpler. Each driver in the <code>IO Audio Family</code> is represented as an audio device in the HAL. To make communication with audio devices easier, and I/O unit may created and bound to an audio device, allowing a device to be used as a source, destination, or both in connections with audio units and audio unit graphs. This is common and encouraged when working with audio hardware.</p><br/><div><a name="//apple_ref/doc/uid/TP30001108-CH209-86581" title="Figure 2-16MIDI hardware architecture"></a><p><strong>Figure 2-16&nbsp;&nbsp;</strong>MIDI hardware architecture</p><img src = "../Art/midi_io.gif" alt = "" width="441" height="314"></div><br/><p>The MIDI hardware architecture is different than that of audio hardware, in that MIDI drivers are in user space, usually working with default drivers provided by the operating system. This means that raw incoming and outgoing data is passed between the hardware and the MIDI driver, and the MIDI driver takes care of the formatting and preparation of the data. The MIDI Server than works with Core MIDI, routing MIDI data via endpoints, the abstraction provided to allow for easy access to MIDI devices.</p>

        <br /><br /> 
        
        <div class="mini_nav_text" align="left">
        <span class="navButtons">
        <a href="../book_intro/book_intro.html">&lt; Previous Page</a><span style="margin-left: 8px"><a href="../audiocodec/audiocodec.html">Next Page &gt;</a></span>
        </span>
        <span id="showHideTOCLowerSpan">
        <a href="#" onclick="showHideTOC();"><img src="../../../../Resources/Images/show_toc_icon.gif" width="15" height="14" border="0" style="margin-bottom: -2px;" alt="" /></a> <a href="#" onclick="showHideTOC();">Hide TOC</a>
        </span>
        </div>

        <br/><hr /><div align="center"><p class="content_text" lang="en" dir="ltr"> <!--#if expr="0=1" -->&#x00a9; 2008 Apple Inc. All Rights Reserved. &#40;<!--#endif -->Last updated: 2008-10-15<!--#if expr="0=1" -->&#041;<!--#endif --></p></div>

        
        <div class="hideOnPrint hideInXcode">
        <!-- start of footer -->
        	<table width="100%" border="0" cellpadding="0" cellspacing="0">
		<tr>
			<td><div style="width: 100%; height: 1px; background-color: #919699; margin-top: 5px; margin-bottom: 15px"></div></td>
		</tr>
		<tr>
			<td align="center"><br/>
				<table border="0" cellpadding="0" cellspacing="0" class="graybox">
					<tr>
						<th>Did this document help you?</th>
					</tr>
					<tr>
						<td>
						    <div style="margin-bottom: 8px"><a href="http://developer.apple.com/feedback/?v=1&url=/documentation/MusicAudio/Reference/CoreAudio/core_audio_intro/core_audio_intro.html%3Fid%3DTP30001108-1.1&media=dvd" target=_new>Yes</a>:  Tell us what works for you.</div>
							<div style="margin-bottom: 8px"><a href="http://developer.apple.com/feedback/?v=2&url=/documentation/MusicAudio/Reference/CoreAudio/core_audio_intro/core_audio_intro.html%3Fid%3DTP30001108-1.1&media=dvd" target=_new>It&#8217;s good, but:</a> Report typos, inaccuracies, and so forth.</div>
							<div><a href="http://developer.apple.com/feedback/?v=3&url=/documentation/MusicAudio/Reference/CoreAudio/core_audio_intro/core_audio_intro.html%3Fid%3DTP30001108-1.1&media=dvd" target=_new>It wasn&#8217;t helpful</a>: Tell us what would have helped.</div>
						</td>
					</tr>
				</table>
			</td>
		</tr>
	</table>

        <!--#include virtual="/includes/framesetfooter" -->
        <!-- end of footer -->
        </div>
    </div>
</body>
</html>