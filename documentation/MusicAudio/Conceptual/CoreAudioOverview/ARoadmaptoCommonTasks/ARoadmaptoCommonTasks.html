<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
	<title>Core Audio Overview: An Overview of Common Tasks</title>
	<meta id="Generator" name="Generator" content="Gutenberg"/>
	<meta id="GeneratorVersion" name="GeneratorVersion" content="v132"/>
	<meta http-equiv="content-type" content="text/html;charset=utf-8"/>
	<meta id="Copyright" name="Copyright" content="Copyright 2009 Apple Inc. All Rights Reserved."/>
	<meta id="IndexTitle" name="IndexTitle" content="An Overview of Common Tasks"/>
	<meta id="xcode-display" name="xcode-display" content="render"/>
	<meta id="toc-file" name="toc-file" content="../toc.html"/>
	<meta id="RESOURCES" content="../../../../Resources" />
	
	<link rel="stylesheet" type="text/css" href="../../../../Resources/CSS/frameset_styles.css"/>
	<script language="JavaScript" type="text/javascript" src="../../../../Resources/JavaScript/lib/prototype.js"></script>
	<script language="JavaScript" type="text/javascript" src="../../../../Resources/JavaScript/lib/scriptaculous.js"></script>
	<script language="JavaScript" type="text/javascript" src="../../../../Resources/JavaScript/page.js"></script>
	<script language="JavaScript" type="text/javascript" src="../../../../Resources/JavaScript/pedia.js"></script>
	<!--[if lte IE 6]>
		<style type="text/css">
			/*<![CDATA[*/ 
			html {overflow-x:auto; overflow-y:hidden;  }
			/*]]>*/
		</style>
	<![endif]-->
</head>    
<body bgcolor="#ffffff" onload="initialize_page();"><a name="//apple_ref/doc/uid/TP40003577-CH6" title="An Overview of Common Tasks"></a>
    <noscript>
    <div id="tocMenu">
        <iframe id="toc_content" name="toc_content" SRC="../toc.html" width="210" height="100%" align="left" frameborder="0">This document set is best viewed in a browser that supports iFrames.</iframe>
    </div>
    </noscript>
    <div id="bodyText">
        <a name="top"></a>
        <div class="hideOnPrint hideInXcode">
        <!-- start of header -->
        <!--#include virtual="/includes/framesetheader" -->
        <!-- end of header -->
        </div>
        
        <!-- start of path -->
<div class="breadcrumb hideOnPrint hideInXcode"><a href="http://developer.apple.com/" target="_top">ADC Home</a> &gt; <a href="../../../../../referencelibrary/index.html#//apple_ref/doc/uid/TP30000943" target="_top">Reference Library</a> &gt; <a href="../../../../index.html#//apple_ref/doc/uid/TP30000440" target="_top">Guides</a> &gt; <a href="../../../index.html#//apple_ref/doc/uid/TP30000440-TP30000428" target="_top">Audio</a> &gt; <a href="../../../CoreAudio-date.html#//apple_ref/doc/uid/TP30000440-TP30000428-TP30000500" target="_top">Core Audio</a> &gt; <a href="../Introduction/Introduction.html#//apple_ref/doc/uid/TP40003577-CH1-SW1">Core Audio Overview</a> &gt; </div><br class="hideInXcode"/><!-- end of path -->
        
        <div class="mini_nav_text" align="left">
        <span class="navButtons">
        <a href="../WhatsinCoreAudio/WhatsinCoreAudio.html">&lt; Previous Page</a><span style="margin-left: 8px"><a href="../CoreAudioFrameworks/CoreAudioFrameworks.html">Next Page &gt;</a></span>
        </span>
        <span id="showHideTOCUpperSpan">
        <a href="#" onclick="showHideTOC();"><img src="../../../../Resources/Images/show_toc_icon.gif" width="15" height="14" border="0" style="margin-bottom: -2px;" alt="" hideText="Hide TOC" showText="Show TOC" /></a> <a href="#" onclick="showHideTOC();">Hide TOC</a>
        </span>
        </div>

        <hr />
        
        
        <a name="//apple_ref/doc/uid/TP40003577-CH6-SW1" title="An Overview of Common Tasks"></a><h1>An Overview of Common Tasks</h1><p>This chapter describes some basic scenarios in which you can use Core Audio. These examples show how you would combine parts of Core Audio to accomplish some common tasks. </p><p>Note that Core Audio is extremely modular, with few restrictions on how to use its various parts, so you may choose to implement certain capabilities in other ways than those shown here. For example, your application can call Audio File and Converter Services functions directly to read data from a file and convert it to linear PCM, or you can choose to encapsulate that capability as a standalone generator audio unit that your application can discover and load.  </p>
<!-- This template is being used for both PDF and HTML. -->

    
    <h4>In this section:</h4>
    
    
    <p class="blockquote">
    
        
			
			
				<a href="ARoadmaptoCommonTasks.html#//apple_ref/doc/uid/TP40003577-CH6-DontLinkElementID_1">Reading and Writing Audio Data</a>
				
			<br/>
			
        
			
			
				<a href="ARoadmaptoCommonTasks.html#//apple_ref/doc/uid/TP40003577-CH6-SW12">Interfacing with Hardware Devices</a>
				
			<br/>
			
        
			
			
				<a href="ARoadmaptoCommonTasks.html#//apple_ref/doc/uid/TP40003577-CH6-SW14">Using Aggregate Devices</a>
				
			<br/>
			
        
			
			
				<a href="ARoadmaptoCommonTasks.html#//apple_ref/doc/uid/TP40003577-CH6-DontLinkElementID_4">Creating Audio Units</a>
				
			<br/>
			
        
			
			
				<a href="ARoadmaptoCommonTasks.html#//apple_ref/doc/uid/TP40003577-CH6-DontLinkElementID_5">Hosting Audio Units</a>
				
			<br/>
			
        
			
			
				<a href="ARoadmaptoCommonTasks.html#//apple_ref/doc/uid/TP40003577-CH6-SW13">Handling MIDI Data</a>
				
			<br/>
			
        
			
			
				<a href="ARoadmaptoCommonTasks.html#//apple_ref/doc/uid/TP40003577-CH6-DontLinkElementID_6">Handling Both Audio and MIDI Data</a>
				
			<br/>
			
        

    </p><br/>

<a name="//apple_ref/doc/uid/TP40003577-CH6-DontLinkElementID_1" title="Reading and Writing Audio Data"></a><h2>Reading and Writing Audio Data</h2><p>Many applications that handle audio need to read and write the data, either to disk or to a buffer. In most cases, you will want to read the file data and convert it to linear PCM (for use in audio units and so on). You can do so in one step using the Extended Audio File API. </p><p>As shown in <span class="content_text">Figure 3-1</span>, the Extended Audio File API makes calls to the Audio File API to read the audio data and then calls the Audio Converter API to convert it to linear PCM (assuming the data is not already in that format). </p><br/><div><a name="//apple_ref/doc/uid/TP40003577-CH6-SW9" title="Figure 3-1Reading audio data"></a><p><strong>Figure 3-1&nbsp;&nbsp;</strong>Reading audio data</p><img src = "../Art/reading_audio_file.jpg" alt = "" ></div><br/><p>If you need more control over the file reading and conversion procedure, you can call Audio File or Audio Converter functions directly. You use the Audio File API to read the file from disk or a buffer. This data may be in a compressed format, in which case it can be converted to linear PCM using an audio converter. You can also use an audio converter to handle changes in bit depth, sampling rate, and so on within the linear PCM format. You handle conversions by using the Audio Converter API to create an audio converter object, specifying the input and output formats you desire. Each format is defined in a <code>AudioStreamBasicDescription</code> structure, which is a fundamental data type in Core Audio for describing audio data. As stated earlier, if you are converting to linear PCM, you can use Extended Audio File API calls to go from audio file data to linear PCM in one step, without having to create your own audio converter. </p><p>Once converted to linear PCM, the data is ready to process by an audio unit. To connect with an audio unit, you must register a callback with a particular audio unit input. When the audio unit needs input, it will invoke your callback, which can then supply the necessary audio data. </p><p>If you want to output the audio data, you send it to an output unit. The output unit can accept only linear PCM format data. The output unit is usually a proxy for a hardware device, but this is not a requirement. </p><p>Linear PCM can act as an intermediate format, which permits many permutations of conversions. To determine whether a particular format conversion is possible, you need to make sure that both a decoder (format A to linear PCM) and an encoder  (linear PCM to format B) are available. For example, if you wanted to convert data from MP3 to AAC, you would need two audio converters: one to convert from MP3 to linear PCM, and another to convert linear PCM to AAC, as shown in <span class="content_text">Figure 3-2</span>.  </p><br/><div><a name="//apple_ref/doc/uid/TP40003577-CH6-SW8" title="Figure 3-2Converting audio data using two converters"></a><p><strong>Figure 3-2&nbsp;&nbsp;</strong>Converting audio data using two converters</p><img src = "../Art/audio_file_conversion.jpg" alt = "" ></div><br/><p>For examples of using the Audio File and Audio Converter APIs, see the <code>SimpleSDK/ConvertFile</code> and  <code>Services/AudioFileTools</code> samples in the Core Audio SDK. If you are interested in writing a custom audio converter codec, see the samples in the <code>AudioCodec</code> folder. </p><a name="//apple_ref/doc/uid/TP40003577-CH6-SW12" title="Interfacing with Hardware Devices"></a><h2>Interfacing with Hardware Devices</h2><p>Most audio applications have to connect with external hardware, either to output audio data (for example, to an amplifier/speaker) or to obtain it (such as through a microphone). These operations must go through the hardware abstraction layer (HAL). Fortunately, in most cases you do not need to write custom code to access the HAL. Apple provides three standard audio units that should address most hardware needs: the default output unit, the system output unit, and the AUHAL. Your application must call the Component Manager to discover and load these units before you can use them. </p><a name="//apple_ref/doc/uid/TP40003577-CH6-DontLinkElementID_2" title="Default and System Output Units"></a><h3>Default and System Output Units</h3><p>The default output unit and system output unit send audio data to the default output (as selected by the user) or system output (where alerts and other system sounds are played) respectively. If you connect an audio unit to one of these output devices (such as in an audio processing graph), your unit's callback function (sometimes called the render callback) is called when the output needs data. The output unit routes the data through the HAL to the appropriate output device, automatically handling the following tasks, as shown in <span class="content_text">Figure 3-3</span>. </p><br/><div><a name="//apple_ref/doc/uid/TP40003577-CH6-SW10" title="Figure 3-3Inside an output unit"></a><p><strong>Figure 3-3&nbsp;&nbsp;</strong>Inside an output unit</p><img src = "../Art/inside_output_unit.jpg" alt = "" ></div><br/><ul class="spaceabove"><li class="li"><p>Any required linear PCM data conversion. The output unit contains an audio converter that can translate your audio data to the linear PCM variant required by the hardware. </p></li><li class="li"><p>Any required channel mapping. For example, if your unit is supplying two-channel data but the output device can handle five, you will probably want to map which channels go to which. You can do so by specifying a channel map using the <code>kAudioOutputUnitProperty_ChannelMap</code> property on the output unit. If you don't supply a channel map, the default is to map the first audio channel to the first device channel, the second to the second, and so on. The actual output heard is then determined by how the user has configured the device speakers in the Audio MIDI Setup application.</p></li><li class="li"><p>Signal Start/Stop. Output units are the only audio units that can control the flow of audio data in the signal chain. </p></li></ul><p>For an example of using the default output unit to play audio, see <code>SimpleSDK/DefaultOutputUnit</code> in the Core Audio SDK. </p><a name="//apple_ref/doc/uid/TP40003577-CH6-DontLinkElementID_3" title="The AUHAL"></a><h3>The AUHAL</h3><p>If you need to connect to an input device, or a hardware device other than the default output device, you need to use the AUHAL. Although designated as an output device, you can configure the AUHAL to accept input as well by setting the <code>kAudioOutputUnitProperty_EnableIO</code> property on the input. For more specifics, see <span class="content_text"><a href="http://developer.apple.com/technotes/tn2002/tn2091.html" target="_top">Technical Note TN2091: Device Input Using the HAL Output Audio Unit</a></span>. When accepting input, the AUHAL supports input channel mapping and uses an audio converter ( if necessary) to translate incoming data to linear PCM format. </p><p>The AUHAL is a more generalized version of the default output unit. In addition to the audio converter and channel mapping capabilities, you can specify the device to connect to by setting the <code>kAudioOutputUnitProperty_CurrentDevice</code> property to the ID of an <code>AudioDevice</code> object in the HAL. Once connected, you can also manipulate properties associated with the <code>AudioDevice</code> object by addressing the AUHAL; the AUHAL automatically passes along any property calls meant for the audio device.</p><p>An AUHAL instance can connect to only one device at a time, so you can enable both input and output only if the device can accept both. For example, the built-in audio for PowerPC-based Macintosh computers is configured as a single device that can both accept input audio data (through the Mic in) and output audio (through the speaker).</p><div class="notebox"><a name="//apple_ref/doc/uid/TP40003577-CH6-DontLinkElementID_32" title="Note"></a><p><strong>Note:</strong>&nbsp;Some audio hardware, including USB audio devices and built-in audio on the current line of Intel-based Macintosh computers, are represented by separate audio devices for input and output. See <span class="content_text"><a href="ARoadmaptoCommonTasks.html#//apple_ref/doc/uid/TP40003577-CH6-SW14">“Using Aggregate Devices”</a></span> for information about how you can combine these separate devices into a single <code>AudioDevice</code> object. </p></div><p>For the purposes of signal flow, the AUHAL configured for both input and output behaves as two audio units. For example, when output is enabled, the AUHAL invokes the previous audio unit's render callback. If an audio unit needs input data from the device, it invokes the AUHAL’s render callback.  <span class="content_text">Figure 3-4</span> shows the AUHAL used for both input and output.  </p><br/><div><a name="//apple_ref/doc/uid/TP40003577-CH6-SW11" title="Figure 3-4The AUHAL used for input and output"></a><p><strong>Figure 3-4&nbsp;&nbsp;</strong>The AUHAL used for input and output</p><img src = "../Art/hal_input_output.jpg" alt = "" ></div><br/><p>An audio signal coming in through the external device is translated into an audio data stream and passed to the AUHAL, which can then send it on to another audio unit. After processing the data (for example, adding effects, or mixing with other audio data), the output is sent back to the AUHAL, which can then output the audio through the same external device. </p><p>For examples of using the AUHAL for input and output, see the <em><a href="../../../../../samplecode/SimplePlayThru/index.html#//apple_ref/doc/uid/DTS10003350" target="_top">SimplePlayThru</a></em> and <em><a href="../../../../../samplecode/CAPlayThrough/index.html#//apple_ref/doc/uid/DTS10004443" target="_top">CAPlayThrough</a></em> code samples in the ADC Reference Library. <code>SimplePlayThru</code> shows how to handle input and output through a single AUHAL instance. <code>CAPlayThrough</code> shows how to implement input and output using an AUHAL for input and the default output unit for output. </p><a name="//apple_ref/doc/uid/TP40003577-CH6-SW14" title="Using Aggregate Devices"></a><h2>Using Aggregate Devices</h2><p>When interfacing with hardware audio devices, Core Audio allows you to add an additional level of abstraction, creating aggregate devices which combine the inputs and outputs of multiple devices to appear as a single device. For example, if you need to accommodate five channels of audio output, you can assign two channels of output to one device and the other three to another device. Core Audio automatically routes the data flow to both devices, while your application can interact with the output as if it were a single device. Core Audio also works on your behalf to ensure proper audio synchronization and to minimize latency, allowing you to focus on details specific to your application or plug-in. </p><p>Users can create aggregate devices in the Audio MIDI Setup application by selecting the Audio > Open Aggregate Device Editor menu item. After selecting the subdevices to combine as an aggregate device, the user can configure the device’s input and output channels like any other hardware device. The user also needs to indicate which subdevice’s clock should act as the master for synchronization purposes.  </p><p>Any aggregate devices the user creates are global to the system. You can create aggregate devices that are local to the application process programmatically using HAL Services function calls. An aggregate device appears as an <code>AudioAggregateDevice</code> object (a subclass of <code>AudioDevice</code>) in the HAL.  </p><div class="notebox"><a name="//apple_ref/doc/uid/TP40003577-CH6-DontLinkElementID_33" title="Note"></a><p><strong>Note:</strong>&nbsp;Aggregate devices can be used to hide implementation details. For example, USB audio devices normally require separate drivers for input and output, which appear as separate <code>AudioDevice</code> objects. However, by creating a global aggregate device, the HAL can represent the drivers as a single <code>AudioDevice</code> object. </p></div><p>An aggregate device retains knowledge of its subdevices. If the user removes a subdevice (or configures it in an incompatible manner), those channels disappear from the aggregate, but those channels will reappear when the subdevice is reattached or reconfigured.  </p><p>Aggregate devices have some limitations:</p><ul class="ul"><li class="li"><p>All the subdevices that make up the aggregate device must be running at the same sampling rate, and their data streams must be mixable. </p></li><li class="li"><p>They don’t provide any configurable controls, such as volume, mute, or input source selection. </p></li><li class="li"><p>You cannot specify an aggregate device to be a default input or output device unless all of its subdevices can be a default device. Otherwise, applications must explicitly select an aggregate device in order to use it. </p></li><li class="li"><p>Currently only devices represented by an <code>IOAudio</code> family (that is, kernel-level) driver can be added to an aggregate device. </p></li></ul><a name="//apple_ref/doc/uid/TP40003577-CH6-DontLinkElementID_4" title="Creating Audio Units"></a><h2>Creating Audio Units</h2><p>For detailed information about creating audio units, see <em>Audio Unit Programming Guide</em>.</p><a name="//apple_ref/doc/uid/TP40003577-CH6-DontLinkElementID_5" title="Hosting Audio Units"></a><h2>Hosting Audio Units</h2><p>Audio units, being plug-ins, require a host application to load and control them. </p><p>Because audio units are Component Manager components, a host application must call the Component Manager to load them. The host application can find and instantiate audio units if they are installed in one of the following folders:</p><ul class="ul"><li class="li"><p><code>~/Library/Audio/Plug-Ins/Components</code>. Audio units installed here may only be used by the owner of the home folder. </p></li><li class="li"><p><code>/Library/Audio/Plug-Ins/Components</code>. Audio units installed here are available to all users. </p></li><li class="li"><p><code>/System/Library/Components</code>. The default location for Apple-supplied audio units. </p></li></ul><p>If you need to obtain a list of available audio units (to display to the user, for example), you need to call the Component Manager function <code>CountComponents</code> to determine how many audio units of a particular type are available, then iterate using <code>FindNextComponent</code> to obtain information about each unit. A <code>ComponentDescription</code> structure contains the identifiers for each audio unit (its type, subtype, and manufacturer codes). See <span class="content_text"><a href="../SystemAudioUnits/SystemAudioUnits.html#//apple_ref/doc/uid/TP40003577-CH8-SW2">“System-Supplied Audio Units”</a></span> for a list of Component Manager types and subtypes for Apple-supplied audio units. The host can also open each unit (by calling <code>OpenComponent</code>) so it can query it for various property information, such as the audio unit’s default input and output data formats, which the host can then cache and present to the user.  </p><p>In most cases, an audio processing graph is the simplest way to connect audio units. One advantage of using a processing graph is that the API takes care of making individual Component Manager calls to instantiate or destroy audio units. To create a graph, call <code>NewAUGraph</code>, which returns a new graph object. Then you can add audio units to the graph by calling <code>AUGraphNewNode</code>.  A graph must end in an output unit, either a hardware interface (such as the default output unit or the AUHAL) or the generic output unit. </p><p>After adding the units that will make up the processing graph, call <code>AUGraphOpen</code>. This function is equivalent to calling <code>OpenComponent</code> on each of the audio units in the graph. At this time, you can set audio unit properties such as the channel layout, sampling rate, or properties specific to a particular unit (such as the number of inputs and outputs it contains).  </p><p>To make individual connections between audio units, call <code>AUGraphConnectNodeInput</code>, specifying the output and input to connect. The audio unit chain must end in an output unit; otherwise the host application has no way to start and stop audio processing.</p><p>If the audio unit has a user interface, the host application is responsible for displaying it. Audio units may supply a Cocoa or a Carbon-based interface (or both). Code for the user interface is typically bundled along with the audio unit. </p><ul class="ul"><li class="li"><p>If the interface is Cocoa-based, the host application must query the unit property <code>kAudioUnitProperty_CocoaUI</code> to find the custom class that implements the interface (a subclass of <code>NSView</code>) and create an instance of that class. </p></li><li class="li"><p>If the interface is Carbon-based, the user interface is stored as one or more Component Manager components. You can obtain the component identifiers (type, subtype, manufacturer) by querying the <code>kAudioUnitProperty_GetUIComponentList</code> property. The host application can then instantiate the user interface by calling <code>AudioUnitCarbonViewCreate</code> on a given component, which displays its interface in a window as an HIView.  </p></li></ul><p>After building the signal chain, you can initialize the audio units by calling <code>AUGraphInitialize</code>. Doing so invokes the initialization function for each audio unit, allowing it to allocate memory for rendering, configure channel information, and so on. Then you can call <code>AUGraphStart</code>, which initiates processing. The output unit then requests audio data from the previous unit in the chain (by means of a callback), which then calls its predecessor, and so on. The source of the audio may be an audio unit (such as a generator unit or AUHAL) or the host application may supply audio data itself by registering a callback with the first audio unit in the signal chain (by setting the unit’s <code>kAudioUnitProperty_SetRenderCallback</code> property). </p><p>While an audio unit is instantiated, the host application may want to know about changes to parameter or property values; it can register a listener object to be notified when changes occur. For details on how to implement such a listener, see <span class="content_text"><a href="http://developer.apple.com/technotes/tn2002/tn2104.html" target="_top">Technical Note TN2104: Handling Audio Unit Events</a></span>.  </p><p>When the host wants to stop signal processing, it calls <code>AUGraphStop</code>.  </p><p>To uninitialize all the audio units in a graph, call <code>AUGraphUninitialize</code>.  When back in the uninitialized state, you can still modify audio unit properties and make or change connections. If you call <code>AUGraphClose</code>, each audio unit in the graph is deallocated by a <code>CloseComponent</code> call. However, the graph still retains the nodal information regarding which units it contains. </p><p>To dispose of a processing graph, call <code>AUGraphDispose</code>. Disposing of a graph automatically disposes of any instantiated audio units it contains.  </p><p>For examples of hosting audio units, see the <code>Services/AudioUnitHosting</code> and <code>Services/CocoaAUHost</code> examples in the Core Audio SDK. </p><p>For an example of implementing an audio unit user interface, see the <code>AudioUnits/CarbonGenericView</code> example in the Core Audio SDK. You can use this example with any audio unit containing user-adjustable parameters.  </p><p>For more information about using the Component Manager, see the following documentation:</p><ul class="ul"><li class="li"><p><em><a href="../../../../Carbon/Reference/Component_Manager/index.html#//apple_ref/doc/uid/TP30000201" target="_top">Component Manager Reference</a></em></p></li><li class="li"><p><em><a href="../../../../QuickTime/Conceptual/ComponentMgr/index.html#//apple_ref/doc/uid/TP40000858" target="_top">Component Manager for QuickTime</a></em></p></li><li class="li"><p>Component Manager documentation in <span class="content_text"><a href="http://developer.apple.com/documentation/mac/MoreToolbox/MoreToolbox-333.html" target="_top">Inside Macintosh: More Macintosh Toolbox</a></span>. Although this is a legacy document, it provides a good conceptual overview of the Component Manager.</p></li></ul><a name="//apple_ref/doc/uid/TP40003577-CH6-SW13" title="Handling MIDI Data"></a><h2>Handling MIDI Data</h2><p>When working with MIDI data, an application might need to load track data from a standard MIDI file (SMF). You can invoke a Music Player function (<code>MusicSequenceLoadSMFWithFlags</code> or <code>MusicSequenceLoadSMFDataWithFlags</code>) to read in data in the Standard MIDI Format, as shown in <span class="content_text">Figure 3-5</span>.</p><br/><div><a name="//apple_ref/doc/uid/TP40003577-CH6-SW7" title="Figure 3-5Reading a standard MIDI file"></a><p><strong>Figure 3-5&nbsp;&nbsp;</strong>Reading a standard MIDI file</p><img src = "../Art/midi_file_read_in.jpg" alt = "" ></div><br/><p>Depending on the type of MIDI file and the flags set when loading a file, you can store all the MIDI data in single track, or store each MIDI channel as a separate track in the sequence. By default, each MIDI channel is mapped sequentially to a new track in the sequence. For example, if the MIDI data contains channels 1, 3, and, 4, three new tracks are added to the sequence, containing data for channels 1, 3, and 4 respectively. These tracks are appended to the sequence at the end of any existing tracks. Each track in a sequence is assigned a zero-based index value. </p><p>Timing information (that is, tempo events) goes to the tempo track. </p><p>Once you have loaded MIDI data into the sequence, you can assign a music player instance to play it, as shown in <span class="content_text">Figure 3-6</span>. </p><br/><div><a name="//apple_ref/doc/uid/TP40003577-CH6-SW6" title="Figure 3-6Playing MIDI data"></a><p><strong>Figure 3-6&nbsp;&nbsp;</strong>Playing MIDI data</p><img src = "../Art/midi_file_audio_playback.jpg" alt = "" ></div><br/><p>The sequence must be associated with a particular audio processing graph, and the tracks in the sequence can be assigned to one or more instrument units in the graph. (If you don't specify a track mapping, the music player sends all the MIDI data to the first instrument unit it finds in the graph.) The music player assigned to the sequence automatically communicates with the graph's output unit to make sure the outgoing audio data is properly synchronized. The compressor unit, while not required, is useful for ensuring that the dynamic range of the instrument unit’s output stays consistent.  </p><p>MIDI data in a sequence can also go to external MIDI hardware (or software configured as a virtual MIDI destination), as shown in <span class="content_text">Figure 3-7</span>.</p><p>Tracks destined for MIDI output must be assigned a MIDI endpoint. The music player communicates with Core MIDI to ensure that the data flow to the MIDI device is properly synchronized. Core MIDI then coordinates with the MIDI Server to transmit the data to the MIDI instrument. </p><br/><div><a name="//apple_ref/doc/uid/TP40003577-CH6-SW5" title="Figure 3-7Sending MIDI data to a MIDI device"></a><p><strong>Figure 3-7&nbsp;&nbsp;</strong>Sending MIDI data to a MIDI device</p><img src = "../Art/midi_device_playthrough.jpg" alt = "" ></div><br/><p>A sequence of tracks can be assigned to a combination of instrument units and MIDI devices. For example, you can assign some of the tracks to play through an instrument unit, while other tracks go through Core MIDI to play through external MIDI devices, as shown in <span class="content_text">Figure 3-8</span>. </p><br/><div><a name="//apple_ref/doc/uid/TP40003577-CH6-SW4" title="Figure 3-8Playing both MIDI devices and a virtual instrument"></a><p><strong>Figure 3-8&nbsp;&nbsp;</strong>Playing both MIDI devices and a virtual instrument</p><img src = "../Art/midi_file_big_picture.jpg" alt = "" ></div><br/><p>The music player automatically coordinates between the audio processing graph's output unit and Core MIDI to ensure that the outputs are synchronized.  </p><p>Another common scenario is to play back already existing track data while accepting new MIDI input, as shown in <span class="content_text">Figure 3-9</span>. </p><br/><div><a name="//apple_ref/doc/uid/TP40003577-CH6-SW3" title="Figure 3-9Accepting new track input "></a><p><strong>Figure 3-9&nbsp;&nbsp;</strong>Accepting new track input </p><img src = "../Art/midi_file_recording.jpg" alt = "" ></div><br/><p>The playback of existing data is handled as usual through the audio processing graph, which sends audio data to the output unit. New data from an external MIDI device enters through Core MIDI and is transferred through the assigned endpoint. Your application must iterate through this incoming data and write the MIDI events to a new or existing track. The Music Player API contains functions to add new tracks to a sequence, and to write time-stamped MIDI events or other messages to a track. </p><p>For examples of handling and playing MIDI data, see the following examples in the Core Audio SDK: </p><ul class="ul"><li class="li"><p><code>MIDI/SampleTools</code>, which shows a simple way to send and receive MIDI data. </p></li><li class="li"><p><code>SimpleSDK/PlaySoftMIDI</code>, which sends MIDI data to a simple processing graph consisting of an instrument unit and an output unit. </p></li><li class="li"><p><code>SimpleSDK/PlaySequence</code>, which reads in a MIDI file into a sequence and uses a music player to play it. </p></li></ul><a name="//apple_ref/doc/uid/TP40003577-CH6-DontLinkElementID_6" title="Handling Both Audio and MIDI Data"></a><h2>Handling Both Audio and MIDI Data</h2><p>Sometimes you want to combine audio data with audio synthesized from MIDI data and play back the result. For example, the audio for many games consists of background music, which is stored as an audio file on disk, along with noises triggered by events (footsteps, gunfire, and so on), which are generated as MIDI data. <span class="content_text">Figure 3-10</span> shows how you can use Core Audio to combine the two. </p><br/><div><a name="//apple_ref/doc/uid/TP40003577-CH6-SW2" title="Figure 3-10Combining audio and MIDI data"></a><p><strong>Figure 3-10&nbsp;&nbsp;</strong>Combining audio and MIDI data</p><img src = "../Art/mixing_audio_sources.jpg" alt = "" ></div><br/><p>The soundtrack audio data is retrieved from disk or memory and converted to linear PCM using the Extended Audio File API. The MIDI data, stored as tracks in a music sequence, is sent to a virtual instrument unit. The output from the virtual instrument unit is in linear PCM format and can then be combined with the soundtrack data. This example uses a 3D mixer unit, which can position audio sources in a three-dimensional space. One of the tracks in the sequence is sending event data to the mixer unit, which alters the positioning parameters, making the sound appear to move over time. The application would have to monitor the player's movements and add events to the special movement track as necessary.</p><p>For an example of loading and playing file-based audio data, see <code>SimpleSDK/PlayFile</code> in the Core Audio SDK.  </p>

        <br /><br /> 
        
        <div class="mini_nav_text" align="left">
        <span class="navButtons">
        <a href="../WhatsinCoreAudio/WhatsinCoreAudio.html">&lt; Previous Page</a><span style="margin-left: 8px"><a href="../CoreAudioFrameworks/CoreAudioFrameworks.html">Next Page &gt;</a></span>
        </span>
        <span id="showHideTOCLowerSpan">
        <a href="#" onclick="showHideTOC();"><img src="../../../../Resources/Images/show_toc_icon.gif" width="15" height="14" border="0" style="margin-bottom: -2px;" alt="" /></a> <a href="#" onclick="showHideTOC();">Hide TOC</a>
        </span>
        </div>

        <br/><hr /><div align="center"><p class="content_text" lang="en" dir="ltr"> <!--#if expr="0=1" -->&#x00a9; 2007 Apple Inc. All Rights Reserved. &#40;<!--#endif -->Last updated: 2007-01-08<!--#if expr="0=1" -->&#041;<!--#endif --></p></div>

        
        <div class="hideOnPrint hideInXcode">
        <!-- start of footer -->
        	<table width="100%" border="0" cellpadding="0" cellspacing="0">
		<tr>
			<td><div style="width: 100%; height: 1px; background-color: #919699; margin-top: 5px; margin-bottom: 15px"></div></td>
		</tr>
		<tr>
			<td align="center"><br/>
				<table border="0" cellpadding="0" cellspacing="0" class="graybox">
					<tr>
						<th>Did this document help you?</th>
					</tr>
					<tr>
						<td>
						    <div style="margin-bottom: 8px"><a href="http://developer.apple.com/feedback/?v=1&url=/documentation/MusicAudio/Conceptual/CoreAudioOverview/ARoadmaptoCommonTasks/ARoadmaptoCommonTasks.html%3Fid%3DTP40003577-1.2&media=dvd" target=_new>Yes</a>:  Tell us what works for you.</div>
							<div style="margin-bottom: 8px"><a href="http://developer.apple.com/feedback/?v=2&url=/documentation/MusicAudio/Conceptual/CoreAudioOverview/ARoadmaptoCommonTasks/ARoadmaptoCommonTasks.html%3Fid%3DTP40003577-1.2&media=dvd" target=_new>It&#8217;s good, but:</a> Report typos, inaccuracies, and so forth.</div>
							<div><a href="http://developer.apple.com/feedback/?v=3&url=/documentation/MusicAudio/Conceptual/CoreAudioOverview/ARoadmaptoCommonTasks/ARoadmaptoCommonTasks.html%3Fid%3DTP40003577-1.2&media=dvd" target=_new>It wasn&#8217;t helpful</a>: Tell us what would have helped.</div>
						</td>
					</tr>
				</table>
			</td>
		</tr>
	</table>

        <!--#include virtual="/includes/framesetfooter" -->
        <!-- end of footer -->
        </div>
    </div>
</body>
</html>